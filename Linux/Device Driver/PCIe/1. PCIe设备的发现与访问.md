
在x86系统中, PCIe 是什么样的一个体系架构. 下图是一个PCIe的拓扑结构示例, PCIe 协议支持 256 个 Bus, 每条 Bus 最多支持 32 个 Device, 每个 Device 最多支持8个Function, 所以由BDF(Bus, device, function)构成了每个PCIe设备节点的身份证号.

![2023-02-09-16-56-15.png](./images/2023-02-09-16-56-15.png)

PCIe体系架构一般由 root complex, switch, endpoint 等类型的 PCIe 设备组成, 在 root complex 和 switch 中通常会有一些 embeded endpoint(这种设备对外不出PCIe接口). 这么多的设备, CPU 启动后要怎么去找到并认出它们呢? Host对PCIe设备扫描是采用了**深度优先算法**, 其过程简要来说是对每一个可能的分支路径深入到不能再深入为止, 而且每个节点只能访问一次. 我们一般称这个过程为 **PCIe 设备枚举**. 枚举过程中 host 通过**配置读事物包**来**获取下游设备的信息**, 通过**配置写事物包**对**下游设备**进行**设置**.

第一步, **PCI Host 主桥**扫描 **Bus 0** 上的设备(在一个处理器系统中, 一般将 Root complex 中与 Host Bridge 相连接的PCI总线命名为PCI Bus 0), 系统首先会忽略 Bus 0 上的 embedded EP 等不会挂接PCI桥的设备, 主桥发现 Bridge 1 后, 将 **Bridge1 下面**的 **PCI Bus** 定为 **Bus 1**, 系统将初始化 **Bridge1** 的**配置空间**, 并将该桥的 `Primary Bus Number` 和 `Secondary Bus Number` 寄存器分别设置成 **0** 和 **1**, 以表明 Bridge1 的**上游总线**是 **0**, **下游总线**是 **1**, 由于还无法确定 Bridge1 下挂载设备的具体情况, 系统先**暂时**将 `Subordinate Bus Number` 设为 **0xFF**.

![2023-02-09-16-57-32.png](./images/2023-02-09-16-57-32.png)

第二步, 系统开始**扫描 Bus 1**, 将会发现 **Bridge 3**, 并发现这是一个 **switch 设备**. 系统将 Bridge 3 下面的 PCI Bus 定为 **Bus 2**, 并将该桥的 `Primary Bus Number` 和 `Secondary Bus Number` 寄存器分别设置成 1 和 2, 和上一步一样**暂时**把 Bridge 3 的 `Subordinate Bus Number` 设为 **0xFF**.

![2023-02-09-16-57-46.png](./images/2023-02-09-16-57-46.png)

第三步, 系统继续扫描 **Bus 2**, 将会发现 **Bridge 4**. 继续扫描, 系统会发现 Bridge 下面挂载的 **NVMe SSD** 设备, 系统将 Bridge 4下面的 PCI Bus 定为 Bus 3, 并将该桥的 `Primary Bus Number` 和 `Secondary Bus Number` 寄存器分别设置成 2 和 3, 因为 **Bus3** 下面挂的是**端点设备**(叶子节点), 下面**不会再有下游总线**了, 因此 Bridge 4 的 `Subordinate Bus Number` 的值可以确定为 3.

![2023-02-09-16-57-59.png](./images/2023-02-09-16-57-59.png)

第四步, 完成 Bus 3 的扫描后, 系统返回到 Bus 2 继续扫描, 会发现Bridge 5. 继续扫描, 系统会发现下面挂载的 NIC 设备, 系统将 Bridge 5 下面的 PCI Bus 设置为 Bus 4, 并将该桥的 Primary Bus Number 和 Secondary Bus Number 寄存器分别设置成 2 和 4, 因为 NIC 同样是端点设备, Bridge 5 的 Subordinate Bus Number 的值可以确定为 4.

![2023-02-09-16-58-15.png](./images/2023-02-09-16-58-15.png)

第五步, 除了 Bridge 4 和 Bridge 5 以外, **Bus2** 下面**没有其他设备**了, 因此返回到 Bridge 3, Bus 4 是找到的挂载在这个 Bridge 下的最后一个 bus 号, 因此将 Bridge 3 的 Subordinate Bus Number 设置为4. Bridge 3 的下游设备都已经扫描完毕, 继续向上返回到 Bridge 1, 同样将 **Bridge 1** 的 `Subordinate Bus Number` 设置为 **4**.

![2023-02-09-16-58-26.png](./images/2023-02-09-16-58-26.png)

第六步, 系统返回到 Bus0 继续扫描, 会发现 Bridge 2, 系统将 Bridge 2下面的 PCI Bus 定为 **Bus 5**. 并将 Bridge 2的Primary Bus Number 和 Secondary Bus Number寄存器分别设置成0和5,  Graphics card也是端点设备, 因此 Bridge 2 的 Subordinate Bus Number 的值可以确定为5.

至此, 挂在PCIe总线上的所有设备都被扫描到, 枚举过程结束, Host通过这一过程获得了一个**完整的 PCIe 设备拓扑结构**.

![2023-02-09-16-58-43.png](./images/2023-02-09-16-58-43.png)

**系统上电**以后, **host** 会自动完成上述的**设备枚举过程**. 除一些专有系统外, 普通系统只会在开机阶段进行进行设备的扫描, 启动成功后(枚举过程结束), 即使插入一个PCIe设备, 系统也不会再去识别它.

在linux操作系统中, 我们可以通过 `lspci –v -t` 命令来查询系统上电阶段扫描到的PCIe设备, 执行结果会以一个树的形式列出系统中所有的pcie设备.

```
# lspci -t -n -v
 \-[0000:00]-+-00.0  8086:09a2
             +-00.1  8086:09a4
             +-00.2  8086:09a3
             +-00.4  8086:0b23
             +-11.0-[04]----00.0  15b7:5011
```

如下所示, 其中 `04:00.0` 是 SSD 设备, `15b7` 是 Sandisk 在 PCI-SIG 组织的注册码, 5011 是设备系列号.

Sandisk5011 设备的 BDF 也可以从上图中找出, 其中 bus 是 0x04, device 是 0x00, function 是 0x0, BDF 表示为 04:00.0, 与之对应的上游端口是 `00:11.0`.

我们可以通过 “`lspci –xxx –s 04:00.0`” 命令来列出该设备的PCIe详细信息. 这些内容存储在 PCIe 设备**配置空间**, 它们描述的是 PCIe 本身的特性. 如下图所示, 可以看到这是一个非易失性存储控制器, 0x00 起始地址是 PCIe 的 Vendor ID 和 Device ID. Class code **0x010802** 表示这是一个 **NVMe 存储设备**. **0x40** 是**第一个 capability 的指针**, 如果你需要查看 PCIe 的特性, 就需要从这个位置开始去查询, 在每组特征的头字段都会给出下一组特性的起始地址. 这儿特别列出了**数据链路功能**(`Data Link Feature`)中的一个 **0x43** 字段, 表示该设备是一个 x4lane 的链接, 支持 PCIe Gen3 速率(8Gbps).




当然也可以使用 `lspci –vvv –s 04:00.0` 命令来查看设备特性.

![2023-02-09-18-58-21.png](./images/2023-02-09-18-58-21.png)

Host 在**枚举设备的同时**也会**对设备进行配置**, 每个 PCIe 设备都会指定一段 CPU memory 访问空间, 从上面的图中我们可以看到这个设备支持两段访问空间, 一段的大小是1M byte, 另一段的大小是256K byte, 系统会分别指定它们的基地址. 基地址配置完成以后, Host就可以通过地址来对PCIe memory空间进行访问了.

PCIe memory空间关联的是PCIe设备物理功能, 对于STAR1000系列芯片而言, 物理功能是NVMe, memory中存放的是NMVe的控制与状态信息, 对于NMVe的控制以及工作状态的获取, 都需要通过memory访问来实现.

下面以NVMe命令下发为例简单描述PCIe设备的memory访问. NVMe命令下发的基本操作是1)Host写doorbell寄存器, 此时使用PCIe memory写请求. 如下图所示, host发出一个memory write(MWr)请求, 该请求经过switch到达要访问的NVMe SSD设备.

这个请求会被端点设备接收并执行2)NVMe读取命令操作. 如下图所示, 此时NVMe SSD作为请求者, 发出一个memory read(MRd)请求, 该请求经过Switch到达Host, Host作为完成者会返回一个完成事物包(CplD), 将访问结果返回给NVMe SSD.

![2023-02-09-16-59-39.png](./images/2023-02-09-16-59-39.png)

这样, 一个NVMe的命令下发过程就完成了. 同样, NVMe的其他操作比如各种队列操作, 命令与完成, 数据传输都是通过PCIe memory访问的方式进行的, 此处不再详述.

通过上面的描述, 相信能够帮助大家了解PCIe的设备枚举和memory空间访问. 以后会继续与大家探讨PCIe的其他内容, 比如PCIe的协议分层, 链路建立, 功耗管理等等.