
# 参考

书籍: 《LINUX设备驱动程序》第三版

初始化参考链接：linux里的nvme驱动代码分析（加载初始化）, https://blog.csdn.net/panzhenjie/article/details/51581063,  nvme_reset_work()函数后的代码大致相同

IO入口点：NVMe的Linux内核驱动分析, https://zhuanlan.zhihu.com/p/72234187

块设备层相关数据结构：Block multi-queue 架构解析（一）数据结构, https://blog.csdn.net/qq_32740107/article/details/106302376

> Linux块设备层已逐步切换到multiqueue , Linux5.0以后单队列代码已被完全移除。multiqueue核心思路是为每个CPU分配一个软件队列，为存储设备的每个硬件队列分配一个硬件派发队列，再将软件队列与硬件派发队列做绑定，减少了IO请求过程中对锁的竞争，从而提高IO性能。

块设备层文档：Block Device Drivers, https://linux-kernel-labs.github.io/refs/heads/master/labs/block_device_drivers.html#block-device-drivers

linux内核源码分析 - nvme设备的初始化, https://www.cnblogs.com/tolimit/p/8779876.html

其他博客:

手把手教Linux驱动: https://blog.csdn.net/daocaokafei/article/details/108071589

linux驱动开发: https://www.cnblogs.com/xiaojiang1025/category/918665.html

nvme kernel driver 阅读笔记: https://www.dazhuanlan.com/karenchan/topics/1006006

nvme协议详解: https://www.zhihu.com/column/c_1338070478725480449

# 源代码阅读顺序

1. `nvme_core` 模块初始化

`nvme_core_init()`:创建工作队列，类; 申请设备号

2. nvme 模块初始化

`nvme_init()`:注册 `pci_driver` 结构体

3. nvme 设备初始化

`nvme_probe()`: 加载 nvme 设备

* `nvme_dev_map()`: 申请 IO 内存并进行映射

* `nvme_setup_prp_pools()`: 创建DMA池

* `nvme_init_ctrl()`: 填充 `nvme_ctrl` 结构体，创建字符设备

* `nvme_reset_ctrl()`: 修改 `nvme_ctrl` 结构体状态，将 `nvme_reset_work` 加入工作队列 `nvme_reset_wq`

4. nvme 设备重启

`nvme_reset_work()`: 重启设备，涉及许多nvme协议相关知识，还有设备各阶段对寄存器的操作

* `nvme_dev_disable(dev, false)`: 如果 controller 是 live 的, disable 掉 nvme

* `nvme_pci_enable()`: 初始化 pci 设备（BAR 寄存器，`MSI-X` 中断，CMB 等)

* `nvme_pci_configure_admin_queue()`: 映射 bar 寄存器，申请并初始化 admin 队列

* `nvme_alloc_admin_tags()`: 初始化 `dev->admin_tagset` 结构体，并创建请求队列

* `nvme_init_identify()`: 向 NVMe 设备发送 identify 命令

* `nvme_setup_io_queues()`: 向 NVMe 设备发送 set feature 命令，再次申请中断号，注册中断函数 申请空间，创建 IO 队列

* `nvme_dev_add()`: 填充 `dev->tagset` 

* `nvme_start_ctrl()` -> `nvme_queue_scan()` -> `ctrl->scan_work->nvme_scan_work()`: 

5. 创建块设备

`nvme_scan_work()`: 发送一系列 identify 命令，创建块设备

* 只需看 `nvme_scan_ns_list()` -> `nvme_validate_ns()` -> `nvme_alloc_ns()`

* nvme_alloc_ns(): 填充 ns，创建块设备

# 模块函数

一个模块 mod1 中定义一个函数 func1; 在另外一个模块 mod2 中定义一个函数 func2，func2 调用 func1。

1. 在模块 mod1 中，EXPORT_SYMBOL(func1);
2. 在模块 mod2 中，extern int func1();

就可以在 mod2 中调用 func1了。

# workqueue

Linux-workqueue讲解: https://www.cnblogs.com/vedic/p/11069249.html

workqueue 是**对内核线程封装**的用于处理各种工作项的一种处理方法，由于处理对象是用**链表拼接一个个工作项**，依次取出来处理，然后**从链表删除**，就像一个队列排好队依次处理一样，所以也称**工作队列**

所谓封装可以简单理解一个**中转站**， **一边**指向“合适”的**内核线程**，**一边**接受你丢过来的**工作项**，用结构体 `workqueue_srtuct` 表示， 而所谓**工作项**也是个**结构体** – `work_struct`，里面有个**成员指针**，指向你最终要**实现的函数**

```cpp
struct workqueue_struct *workqueue_test;

struct work_struct work_test;

void work_test_func(struct work_struct *work)
{
    printk("%s()\n", __func__);

    //mdelay(1000);
    //queue_work(workqueue_test, &work_test);
}


static int test_init(void)
{
    printk("Hello,world!\n");

    /* 1. 自己创建一个workqueue， 中间参数为0，默认配置 */
    workqueue_test = alloc_workqueue("workqueue_test", 0, 0);

    /* 2. 初始化一个工作项，并添加自己实现的函数 */
    INIT_WORK(&work_test, work_test_func);

    /* 3. 将自己的工作项添加到指定的工作队列去， 同时唤醒相应线程处理 */
    queue_work(workqueue_test, &work_test);
    
    return 0;
}
```

# alloc_chrdev_region

alloc_chrdev_region 是让内核**分配**给我们一个**尚未使用的主设备号**，不是由我们自己指定的，该函数的四个参数意义如下：

dev: alloc_chrdev_region 函数向内核申请下来的设备号

baseminor: 次设备号的起始

count: 申请次设备号的个数

name: 执行 cat /proc/devices 显示的名称

# class_create

内核中定义了 struct class 结构体，一个 struct class 结构体类型变量对应一个类，内核同时提供了 `class_create()` 函数 ，可以用它来创建一个类，这个类存放于 sysfs 下面，一旦创建了这个类，再调用 `device_create()` 函数 在 /dev 目录下创建相应的设备节点（/sys/class/类名/设备名）

# struct device

Linux内核中的设备驱动模型，是建立在sysfs设备文件系统和kobject上的，由总线（bus）、设备（device）、驱动（driver）和类（class）所组成的关系结构，在底层，Linux系统中的每个设备都有一个device结构体的实例

dev_to_node：返回 struct device 中的 numa_node 变量，即所属的内存节点

```cpp
struct device {
#ifdef CONFIG_NUMA
	int		numa_node;	/* NUMA node this device is close to */
#endif
}

static inline int dev_to_node(struct device *dev)
{
	return dev->numa_node;
}
```

kzalloc_node：从特定的内存节点分配零内存

```cpp
/**
 * kzalloc_node - allocate zeroed memory from a particular memory node.
 * @size: how many bytes of memory are required.
 * @flags: the type of memory to allocate (see kmalloc).
 * @node: memory node from which to allocate
 */
static inline void *kzalloc_node(size_t size, gfp_t flags, int node)
{
	return kmalloc_node(size, flags | __GFP_ZERO, node);
}
```

驱动中的队列，通过函数 kcalloc_node 创建，可以看到队列数量是和系统中所拥有的cpu数量有关。

```cpp
dev->queues = kcalloc_node(num_possible_cpus() + 1,sizeof(struct nvme_queue), GFP_KERNEL, node);
```

Queue有的概念，那就是**队列深度**, 表示其**能够放多少个成员**。在 NVMe 中，这个队列深度是**由 NVMe SSD 决定**的，存储在 NVMe 设备的 **BAR 空间**里。

**队列**用来存放 `NVMe Command`，NVMe Command 是**主机与 SSD 控制器交流**的基本单元，应用的 **I/O 请求**也要转化成 NVMe Command。

不过需要注意的是，就算有很多 CPU 发送请求，但是**块层**并不能保证都能处理完，**将来**可能要**绕过 IO 栈的块层**，不然瓶颈就是操作系统本身了。

当前 Linux 内核提供了 `blk_queue_make_request` 函数，调用这个函数**注册自定义的队列处理方法**，可以**绕过 io 调度和 io 队列**，从而缩短 io 延时。**Block 层**收到上层发送的 **IO 请求**，就会选择该方法处理.

get_device: https://deepinout.com/linux-kernel-api/device-driver-and-device-management/linux-kernel-api-get_device.html

```cpp
struct device *get_device(struct device *dev)
```

get_device输入参数说明

函数 get_device() 的输入参数是 struct device 结构体类型的指针，代表增加计数的逻辑设备

get_device 返回参数说明

函数 get_device() 的返回结果是 struct device 结构体类型的变量，返回的结果与传入的参数代表的是**同一个变量**，只是此时变量的引用计数器的值增大了1。

pci_set_drvdata

pci_set_drvdata() 为 pci_dev 设置**私有数据指针**, 把**设备指针地址**放入 PCI 设备中的设备指针中，便于后面调用 pci_get_drvdata

```cpp
static inline void pci_set_drvdata(struct pci_dev *pdev, void *data)
{
	dev_set_drvdata(&pdev->dev, data);
}
static inline void dev_set_drvdata(struct device *dev, void *data)
{
	dev->driver_data = data;
}
```

to_pci_dev

```cpp
dev->dev = get_device(&pdev->dev);
struct pci_dev *pdev = to_pci_dev(dev->dev);
```

to_pci_dev 应该是对 container_of 宏的封装，根据成员地址获得结构体地址。

pci_request_mem_regions

内核request_mem_region 和 ioremap的理解: https://blog.csdn.net/skyflying2012/article/details/8672011

pci_request_mem_regions 应该是对 request_mem_region 的封装，

几乎每一种外设都是通过读写设备上的寄存器来进行的，通常包括控制寄存器、状态寄存器和数据寄存器三大类，**外设的寄存器**通常**被连续的编址**。根据CPU体系结构的不同，CPU对IO端口的编址方式有两种：

（1）I/O映射方式（I/O-mapped）

典型地，如X86处理器为外设专门实现了一个单独的地址空间，称为"I/O地址空间"或者"I/O端口空间"，CPU通过专门的I/O指令（如X86的IN和OUT指令）来访问这一空间中的地址单元。

（2）内存映射方式（Memory-mapped）

RISC指令系统的CPU（如MIPS ARM PowerPC等）通常只实现一个物理地址空间，像这种情况,外设的I/O端口的物理地址就被映射到内存地址空间中，外设I/O端口成为内存的一部分。此时，CPU可以象访问一个内存单元那样访问外设I/O端口，而不需要设立专门的外设I/O指令。

但是，这两者在硬件实现上的差异对于软件来说是完全透明的，驱动程序开发人员可以将内存映射方式的I/O端口和外设内存统一看作是"I/O内存"资源。

一般来说，在系统运行时，外设的I/O内存资源的物理地址是已知的，由硬件的设计决定。但是CPU通常并没有为这些已知的外设I/O内存资源的物理地址预定义虚拟地址范围，驱动程序并不能直接通过物理地址访问I/O内存资源，而必须将它们映射到核心虚地址空间内（通过页表），然后才能根据映射所得到的核心虚地址范围，通过访内指令访问这些I/O内存资源。Linux在io.h头文件中声明了函数ioremap（），用来将I/O内存资源的物理地址映射到核心虚地址空间。




# reference


NVMe驱动学习记录-2: https://blog.csdn.net/freedom1523646952/article/details/124734309

