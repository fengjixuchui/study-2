
# 参考

书籍: 《LINUX设备驱动程序》第三版

初始化参考链接：linux里的nvme驱动代码分析（加载初始化）, https://blog.csdn.net/panzhenjie/article/details/51581063,  nvme_reset_work()函数后的代码大致相同

IO入口点：NVMe的Linux内核驱动分析, https://zhuanlan.zhihu.com/p/72234187

块设备层相关数据结构：Block multi-queue 架构解析（一）数据结构, https://blog.csdn.net/qq_32740107/article/details/106302376

> Linux块设备层已逐步切换到multiqueue , Linux5.0以后单队列代码已被完全移除。multiqueue核心思路是为每个CPU分配一个软件队列，为存储设备的每个硬件队列分配一个硬件派发队列，再将软件队列与硬件派发队列做绑定，减少了IO请求过程中对锁的竞争，从而提高IO性能。

块设备层文档：Block Device Drivers, https://linux-kernel-labs.github.io/refs/heads/master/labs/block_device_drivers.html#block-device-drivers

linux内核源码分析 - nvme设备的初始化, https://www.cnblogs.com/tolimit/p/8779876.html

其他博客:

手把手教Linux驱动: https://blog.csdn.net/daocaokafei/article/details/108071589

linux驱动开发: https://www.cnblogs.com/xiaojiang1025/category/918665.html

nvme kernel driver 阅读笔记: https://www.dazhuanlan.com/karenchan/topics/1006006

nvme协议详解: https://www.zhihu.com/column/c_1338070478725480449

# 源代码阅读顺序

1. `nvme_core` 模块初始化

`nvme_core_init()`:创建工作队列，类; 申请设备号

2. nvme 模块初始化

`nvme_init()`:注册 `pci_driver` 结构体

3. nvme 设备初始化

`nvme_probe()`: 加载 nvme 设备

* `nvme_dev_map()`: 申请 IO 内存并进行映射

* `nvme_setup_prp_pools()`: 创建DMA池

* `nvme_init_ctrl()`: 填充 `nvme_ctrl` 结构体，创建字符设备

* `nvme_reset_ctrl()`: 修改 `nvme_ctrl` 结构体状态，将 `nvme_reset_work` 加入工作队列 `nvme_reset_wq`

4. nvme 设备重启

`nvme_reset_work()`: 重启设备，涉及许多nvme协议相关知识，还有设备各阶段对寄存器的操作

* `nvme_dev_disable(dev, false)`: 如果 controller 是 live 的, disable 掉 nvme

* `nvme_pci_enable()`: 初始化 pci 设备（BAR 寄存器，`MSI-X` 中断，CMB 等)

* `nvme_pci_configure_admin_queue()`: 映射 bar 寄存器，申请并初始化 admin 队列

* `nvme_alloc_admin_tags()`: 初始化 `dev->admin_tagset` 结构体，并创建请求队列

* `nvme_init_identify()`: 向 NVMe 设备发送 identify 命令

* `nvme_setup_io_queues()`: 向 NVMe 设备发送 set feature 命令，再次申请中断号，注册中断函数 申请空间，创建 IO 队列

* `nvme_dev_add()`: 填充 `dev->tagset` 

* `nvme_start_ctrl()` -> `nvme_queue_scan()` -> `ctrl->scan_work->nvme_scan_work()`: 

5. 创建块设备

`nvme_scan_work()`: 发送一系列 identify 命令，创建块设备

* 只需看 `nvme_scan_ns_list()` -> `nvme_validate_ns()` -> `nvme_alloc_ns()`

* nvme_alloc_ns(): 填充 ns，创建块设备

# 模块函数

一个模块 mod1 中定义一个函数 func1; 在另外一个模块 mod2 中定义一个函数 func2，func2 调用 func1。

1. 在模块 mod1 中，EXPORT_SYMBOL(func1);
2. 在模块 mod2 中，extern int func1();

就可以在 mod2 中调用 func1了。

# workqueue

Linux-workqueue讲解: https://www.cnblogs.com/vedic/p/11069249.html

workqueue 是**对内核线程封装**的用于处理各种工作项的一种处理方法，由于处理对象是用**链表拼接一个个工作项**，依次取出来处理，然后**从链表删除**，就像一个队列排好队依次处理一样，所以也称**工作队列**

所谓封装可以简单理解一个**中转站**， **一边**指向“合适”的**内核线程**，**一边**接受你丢过来的**工作项**，用结构体 `workqueue_srtuct` 表示， 而所谓**工作项**也是个**结构体** – `work_struct`，里面有个**成员指针**，指向你最终要**实现的函数**

```cpp
struct workqueue_struct *workqueue_test;

struct work_struct work_test;

void work_test_func(struct work_struct *work)
{
    printk("%s()\n", __func__);

    //mdelay(1000);
    //queue_work(workqueue_test, &work_test);
}


static int test_init(void)
{
    printk("Hello,world!\n");

    /* 1. 自己创建一个workqueue， 中间参数为0，默认配置 */
    workqueue_test = alloc_workqueue("workqueue_test", 0, 0);

    /* 2. 初始化一个工作项，并添加自己实现的函数 */
    INIT_WORK(&work_test, work_test_func);

    /* 3. 将自己的工作项添加到指定的工作队列去， 同时唤醒相应线程处理 */
    queue_work(workqueue_test, &work_test);
    
    return 0;
}
```

# alloc_chrdev_region

alloc_chrdev_region 是让内核**分配**给我们一个**尚未使用的主设备号**，不是由我们自己指定的，该函数的四个参数意义如下：

dev: alloc_chrdev_region函数向内核申请下来的设备号

baseminor: 次设备号的起始

count: 申请次设备号的个数

name: 执行 cat /proc/devices显示的名称



# reference


NVMe驱动学习记录-2: https://blog.csdn.net/freedom1523646952/article/details/124734309

