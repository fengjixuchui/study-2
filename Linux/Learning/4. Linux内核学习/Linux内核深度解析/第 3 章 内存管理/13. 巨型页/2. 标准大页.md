
<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [1. 大页](#1-大页)
  - [1.1. 永久大页](#11-永久大页)
  - [1.2. 临时大页](#12-临时大页)
  - [1.3. 大页池](#13-大页池)
- [2. 内核编译](#2-内核编译)
- [3. 内核启动参数](#3-内核启动参数)
  - [3.1. hugepagesz](#31-hugepagesz)
  - [3.2. hugepages](#32-hugepages)
  - [3.3. default_hugepagesz](#33-default_hugepagesz)
  - [3.4. hugetlb_free_vmemmap](#34-hugetlb_free_vmemmap)
- [4. 总的 HugePages](#4-总的-hugepages)
  - [4.1. 整体使用情况查看](#41-整体使用情况查看)
  - [4.2. 整体设置接口](#42-整体设置接口)
    - [4.2.1. 永久 nr_hugepages](#421-永久-nr_hugepages)
    - [4.2.2. 分配策略 nr_hugepages_mempolicy](#422-分配策略-nr_hugepages_mempolicy)
    - [4.2.3. 超发 nr_overcommit_hugepages](#423-超发-nr_overcommit_hugepages)
  - [4.3. 不同场景的信息示例](#43-不同场景的信息示例)
  - [4.4. 参数之间的关系](#44-参数之间的关系)
  - [4.5. 不同大小的设置或查看](#45-不同大小的设置或查看)
- [5. NUMA node 的 HugePages](#5-numa-node-的-hugepages)
  - [5.1. 单个 NUMA node 的设置接口](#51-单个-numa-node-的设置接口)
  - [5.2. 单个 NUMA node 的使用情况查看](#52-单个-numa-node-的使用情况查看)
  - [5.3. 不同场景的信息示例](#53-不同场景的信息示例)
- [6. 使用方法](#6-使用方法)
  - [mmap、SYSV共享内存异同](#mmap-sysv共享内存异同)
  - [6.1. 匿名映射](#61-匿名映射)
  - [6.3. 基于文件的映射](#63-基于文件的映射)
    - [6.3.1. 挂载 hugetlbfs](#631-挂载-hugetlbfs)
    - [6.3.2. 创建文件和创建内存映射](#632-创建文件和创建内存映射)
  - [6.2. shm](#62-shm)
  - [6.4. hugetlbfs 库](#64-hugetlbfs-库)
- [7. 实现原理](#7-实现原理)
  - [7.1. 大页池 struct hstate](#71-大页池-struct-hstate)
  - [7.2. 预先分配永久大页](#72-预先分配永久大页)
    - [7.2.1. 内核参数 hugepages](#721-内核参数-hugepages)
    - [7.2.2. HugePage 分配器初始化](#722-hugepage-分配器初始化)
    - [7.2.3. 写 nr_hugepages 文件](#723-写-nr_hugepages-文件)
  - [7.3. 写超发 nr_overcommit_hugepages](#73-写超发-nr_overcommit_hugepages)
  - [7.4. 挂载hugetlbfs文件系统](#74-挂载hugetlbfs文件系统)
  - [7.5. 创建文件](#75-创建文件)
  - [7.6. 创建内存映射](#76-创建内存映射)
    - [7.6.1. 申请预留大页](#761-申请预留大页)
  - [7.7. 分配和映射到大页](#77-分配和映射到大页)
  - [7.8. 写时复制](#78-写时复制)
- [性能对比](#性能对比)
- [8. reference](#8-reference)

<!-- /code_chunk_output -->

# 1. 大页

## 1.1. 永久大页

> persistent hugepage

**写文件** “`/proc/sys/vm/nr_hugepages`” 指定**大页池**中**永久大页**的**数量**, **预先分配指定数量的永久大页**到大页池中.

另一种方法是在**引导内核时**指定**内核参数** “`hugepages=N`” 以分配永久大页, 这是分配大页最可靠的方法, 因为**内存还没有碎片化**.

## 1.2. 临时大页

> surplus hugepage

在 Hugetlb 大页机制中, 系统可以**提前分配**指定数量的大页用于 hugetlbfs 或者匿名大页使用, 系统同时也支持超发大页, 所谓超发大页就是系统设置一个超发的上限, 当系统**可用大页没有**的时候, 可以**动态**的从 Buddy 中分配内存充当大页, 只要分配大页的数量不超过系统允许超发的上限就行, 这种方式称为大页的超发. `alloc_surplus_huge_page()` 函数的作用就是**动态的分配超发的大页**, 其底层逻辑是: 函数首先调用 `hstate_is_gigantic()` 函数判断大页是否 1Gig 大页, 如果是, 那么函数**不支持动态分配 1Gig 大页**, 函数直接返回 NULL; 反之函数继续动态分配 2MiB 的大页.

通过写文件 “`/proc/sys/vm/nr_overcommit_hugepages`” 指定**大页池**中**最大临时大页的数量**.

当**永久大页用完**的时候, 可以从页分配器**申请临时大页**.

## 1.3. 大页池

> hugepage pool

`nr_hugepages` 是**大页池**的**最小数量**(大小), (`nr_hugepages + nr_overcommit_hugepages`) 是**大页池**的**最大数量**(大小). 这两个参数的默认值都是0, 至少要设置一个, 不然分配大页会失败.

# 2. 内核编译

> kernel config

编译内核时需要打开配置宏 `CONFIG_HUGETLBFS` 和`CONFIG_HUGETLB_PAGE`.

打开配置宏 `CONFIG_HUGETLBFS` 的时候会自动打开.

# 3. 内核启动参数

HugePages内存页是不会被系统交换出去(swapped out)的.

由于 HugePages 需要更大的**连续物理内存**, 所以在**系统启动**时**更容易**获得更多的 HugePages 内存, 并且还能尽量保证这些 HugePages 内存页**连续**.

可以通过添加对应的内核启动参数来实现:

<table style="width:100%">
  <tr>
    <th>参数</th>
    <th>描述</th>
  </tr>
  <tr>
    <td>hugepagesz</td>
    <td>单个 HugePages 字节大小</td>
  </tr>
  <tr>
    <td>hugepages</td>
    <td>预分配(永久)的 HugePages 个数</td>
  </tr>
  <tr>
    <td>default_hugepagesz</td>
    <td>默认 HugePages 字节大小</td>
  </tr>
  <tr>
    <td>hugetlb_free_vmemmap</td>
    <td></td>
  </tr>
</table>

如果系统**不支持**设置的**默认** HugePages **内存页大小**, **实际的 HugePages** 内存页大小会保持**2M**.

> hugepagesz=2M hugepages=5 hugepagesz=1G hugepages=4

## 3.1. hugepagesz

有些平台支持**多种大页长度**. 如果要分配特定长度的大页, 必须在内核参数 “`hugepages`” 前面添加选择**大页长度的参数** “`hugepagesz=<size>[kKmMgG]`”.

一种大小的 hugepagesz 只能声明一次. 通常后面接 hugepages 参数来预分配大页.

> hugepagesz=2M hugepages=512

## 3.2. hugepages

通常在 hugepagesz 或者 default_hugepagesz 参数的后面.

如果 hugepages 是**第一个参数**或者**唯一参数**, 它隐式地(implicitly)指定要分配的**默认大小大页**的**数量**. 如果**默认大小大页的数量**是隐式指定的(上面的**第一个参数**方法), 那就**不能**通过 `hugepagesz,hugepages` 参数**覆写**.

> hugepages=256 hugepagesz=2M hugepages=512

这里隐式指定了默认大小大页是 256 个, 后面两个参数对默认大小页的设置会被忽略.

同时支持 node 格式的参数

> hugepagesz=2M hugepages=0:1,1:2

在 node0 分配 1 个 2M 大小的 hugepage; 在 node1 分配 2 个 2M 大小的 hugepage.

## 3.3. default_hugepagesz

“`default_hugepagesz=<size>[kKmMgG]`” 用于选择**默认的大页长度**. 这个参数仅能出现一次.

在 `default_hugepagesz` **后面**可以选择跟 **hugepages** 参数, 以**预分配特定数量**的默认大小的大页. 默认大小的**大页数量**也可以**隐式指定**(前面hugepages部分).

在默认 2M 大页的架构中, 下面几种方式都能设置 256 个默认 2M 大页

> hugepages=256
> default_hugepagesz=2M hugepages=256
> hugepages=256 default_hugepagesz=2M

## 3.4. hugetlb_free_vmemmap

当 `CONFIG_HUGETLB_PAGE_FREE_VMEMMAP` 设置后, 就能释放 HugeTLB page 关联的 unused vmemmap pages.

---

在支持**多种 size 大页**的系统中, “`/proc/sys/vm/nr_hugepages`” 表示**当前系统中默认长度的永久大页的数量**.

如果系统**不支持**设置的默认 HugePages 内存页大小, **实际的 HugePages** 内存页大小会保持**2M**.

# 4. 总的 HugePages

获取当前系统使用页面大小命令:

```
getconf PAGESIZE
```

假设内核启动参数是 `hugepagesz=2M hugepages=5 hugepagesz=1G hugepages=4`

## 4.1. 整体使用情况查看

`/proc/meminfo` 有总的内存信息, 可以通过查找 Huge 关键字

```
# cat /proc/meminfo | grep -i huge
AnonHugePages:    151552 kB
ShmemHugePages:        0 kB
FileHugePages:         0 kB
HugePages_Total:       5
HugePages_Free:        5
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:       2048 kB
Hugetlb:         4204544 kB
```

> 这里显示的是默认size大页的信息

<table style="width:100%">
  <tr>
    <th>项目</th>
    <th>描述</th>
  </tr>
  <tr>
    <td>HugePages_Total</td>
    <td>系统当前总共拥有的 HugePages 个数</td>
  </tr>
  <tr>
    <td>HugePages_Free</td>
    <td>系统当前总共拥有的<b>空闲</b> HugePages 个数</td>
  </tr>
  <tr>
    <td>HugePages_Rsvd</td>
    <td>(reserved)系统当前总共<b>预留</b>的 HugePages 个数</td>
  </tr>
  <tr>
    <td>HugePages_Surp</td>
    <td>(surplus)<b>实际使用</b>的超发 HugePages 个数</td>
  </tr>
  <tr>
    <td>Hugepagesize</td>
    <td>每一页HugePages的字节大小</td>
  </tr>
  <tr>
    <td>Hugetlb</td>
    <td>Hugetlb 子系统所占内存的字节大小</td>
  </tr>
</table>

(1) `HugePages_Total`: **大页池**的**大小**.

(2) `HugePages_Free`: 大页池中**没有分配**的大页的**数量**(空闲).

> 只要没有读写, 都算 free 计数

(3) `HugePages_Rsvd`: “Rsvd”是“Reserved”的缩写, 意思是“预留的”, 是**已经承诺从大页池中分配**但是**还没有分配**的大页的**数量**. 预留的大页保证**应用程序**在**发生缺页异常**的时候能够从大页池中分配一个大页.

程序**已经**向系统**申请**, 但是由于**程序**还**没有**对 HugePages 实质的**读写**操作, 系统**尚未实际分配**给程序的 HugePages 个数

(4) `HugePages_Surp`: “Surp”是“Surplus”的缩写, 意思是“**多余的**”, 是大页池中**已经使用**的**临时大页**的数量. 临时大页的最大数量由“`/proc/sys/vm/nr_overcommit_hugepages`”控制.

> 这是实际使用的, `nr_overcommit_hugepages` 设置了但是没使用这里不会计数

(5) Hugepagesize: 大页的大小(每一页HugePages的字节大小).

## 4.2. 整体设置接口

`/proc/sys/vm`, **调节系统中 HugePages 参数**.

<table style="width:100%">
<caption>/proc/sys/vm</caption>
  <tr>
    <th>文件</th>
    <th>描述</th>
    <th>权限</th>
  </tr>
  <tr>
    <td>nr_hugepages</td>
    <td>写入或读取实际的永久大页个数</td>
    <td><b>读写</b></td>
  </tr>
  <tr>
    <td>nr_hugepages_mempolicy</td>
    <td>NUMA node的HugePages个数</td>
    <td><b>读写</b></td>
  </tr>
  <tr>
    <td>nr_overcommit_hugepages</td>
    <td>临时大页的最大个数</td>
    <td><b>读写</b></td>
  </tr>
</table>

```
# ll /proc/sys/vm/*huge*
-rw-r--r-- 1 root root 0 Feb 26 09:54 /proc/sys/vm/hugetlb_shm_group
-rw-r--r-- 1 root root 0 Feb 26 09:54 /proc/sys/vm/nr_hugepages
-rw-r--r-- 1 root root 0 Feb 26 09:54 /proc/sys/vm/nr_hugepages_mempolicy
-rw-r--r-- 1 root root 0 Feb 26 09:54 /proc/sys/vm/nr_overcommit_hugepages

# grep . /proc/sys/vm/*huge*
/proc/sys/vm/hugetlb_shm_group:0
/proc/sys/vm/nr_hugepages:5
/proc/sys/vm/nr_hugepages_mempolicy:5
/proc/sys/vm/nr_overcommit_hugepages:0
```

### 4.2.1. 永久 nr_hugepages

`/proc/sys/vm/nr_hugepages` 用于设定**永久 HugePages 个数**, 可读可写.

1. 设定的之前先清零, 重新生成HugePages页面.
2. 设定的时候, 系统会根据当前**可用物理内存**计算出可以组成的 HugePages 个数.
3. 读取, 就可以得到**实际分配**的HugePages个数.

```
# 清零
echo 0 > /proc/sys/vm/nr_hugepages

# 设置 100个 `HugePages`
echo 100 > /proc/sys/vm/nr_hugepages

# 查询实际`HugePages`的个数
cat /proc/sys/vm/nr_hugepages
100
```

### 4.2.2. 分配策略 nr_hugepages_mempolicy

> 注意: 这个接口相对落后, 请使用单个NUMA node的HugePages的分配代替.

`/proc/sys/vm/nr_hugepages_mempolicy` 用于设定**多个 NUMA node 的 HugePages 个数**, 可读可写.

* **默认**情况下系统**所有 NUMA node 平均分配**所有 HugePages.
* 除非 **NUMA node** 本身**没有足够的内存**来生成HugePages, 那么将由**另外一个生成**.
* **指定 NUMA node** 分配 HugePages, 需要配合 `numactl -m` 一起使用.

```
# 所有`NUMA node`平均分配所有`HugePages`
echo ${huge_page_count} > /proc/sys/vm/nr_hugepages_mempolicy

# 指定`${node-list}`分配`HugePages`
numactl -m ${node-list} echo ${huge_page_count} > /proc/sys/vm/nr_hugepages_mempolicy
```

例子:

```
# 清零
echo 0 > /proc/sys/vm/nr_hugepages

# 从`NUMA node0`分配. 总数40 个`HugePages`
numactl -m 0 echo 40 > /proc/sys/vm/nr_hugepages_mempolicy

# 查看所有分配
cat /sys/devices/system/node/node*/meminfo | grep Huge
>	# `NUMA node0`分配 新增40 个`HugePages`
>   Node 0 HugePages_Total:    40
>   Node 0 HugePages_Free:     40
>   Node 0 HugePages_Surp:      0
>	# `NUMA node1`保持 `HugePages` 不变
>   Node 1 HugePages_Total:     0
>   Node 1 HugePages_Free:      0
>   Node 1 HugePages_Surp:      0

# 从`NUMA node1`分配. 总数60 个`HugePages`
numactl -m 1 echo 60 > /proc/sys/vm/nr_hugepages_mempolicy
# 查看所有分配
cat /sys/devices/system/node/node*/meminfo | grep Huge
>	# `NUMA node0`保持 `HugePages` 不变
>   Node 0 HugePages_Total:    40
>   Node 0 HugePages_Free:     40
>   Node 0 HugePages_Surp:      0
>	# `NUMA node1`分配 新增20 个`HugePages`
>   Node 1 HugePages_Total:    20
>   Node 1 HugePages_Free:     20
>   Node 1 HugePages_Surp:      0

# 所有`NUMA node`平均分配. 总数80 个`HugePages`
echo 80 >/proc/sys/vm/nr_hugepages_mempolicy
# 查看所有分配
cat /sys/devices/system/node/node*/meminfo | grep Huge
>	# `NUMA node0`分配 新增10 个`HugePages`
>   Node 0 HugePages_Total:    50
>   Node 0 HugePages_Free:     50
>   Node 0 HugePages_Surp:      0
>	# `NUMA node1`分配 新增10 个`HugePages`
>   Node 1 HugePages_Total:    30
>   Node 1 HugePages_Free:     30
>   Node 1 HugePages_Surp:      0

# 所有`NUMA node`平均分配. 总数100 个`HugePages`
# 注意`/proc/sys/vm/nr_hugepages`无法配合`numactl`使用
numactl -m 1 echo 100 > /proc/sys/vm/nr_hugepages
# 查看所有分配
cat /sys/devices/system/node/node*/meminfo | grep Huge
>	# `NUMA node0`分配 新增10 个`HugePages`
>   Node 0 HugePages_Total:    60
>   Node 0 HugePages_Free:     60
>   Node 0 HugePages_Surp:      0
>	# `NUMA node1`分配 新增10 个`HugePages`
>   Node 1 HugePages_Total:    40
>   Node 1 HugePages_Free:     40
>   Node 1 HugePages_Surp:      0

# 所有`NUMA node`平均分配. 总数80 个`HugePages`
# 注意`/proc/sys/vm/nr_hugepages`无法配合`numactl`使用
numactl -m 1 echo 80 > /proc/sys/vm/nr_hugepages
# 查看所有分配
cat /sys/devices/system/node/node*/meminfo | grep Huge
>	# `NUMA node0`分配 减少10 个`HugePages`
>   Node 0 HugePages_Total:    50
>   Node 0 HugePages_Free:     50
>   Node 0 HugePages_Surp:      0
>	# `NUMA node1`分配 减少10 个`HugePages`
>   Node 1 HugePages_Total:    30
>   Node 1 HugePages_Free:     30
>   Node 1 HugePages_Surp:      0

# 从`NUMA node1`分配. 总数60 个`HugePages`
numactl -m 1 echo 60 > /proc/sys/vm/nr_hugepages_mempolicy
# 查看所有分配
cat /sys/devices/system/node/node*/meminfo | grep Huge
>	# `NUMA node0`保持 `HugePages` 不变
>   Node 0 HugePages_Total:    50
>   Node 0 HugePages_Free:     50
>   Node 0 HugePages_Surp:      0
>	# `NUMA node1`分配 减少20 个`HugePages`
>   Node 1 HugePages_Total:    10
>   Node 1 HugePages_Free:     10
>   Node 1 HugePages_Surp:      0

# 从`NUMA node0`分配. 总数30 个`HugePages`
numactl -m 0 echo 30 > /proc/sys/vm/nr_hugepages_mempolicy
# 查看所有分配
cat /sys/devices/system/node/node*/meminfo | grep Huge
>	# `NUMA node0`分配 减少20 个`HugePages`
>   Node 0 HugePages_Total:    20
>   Node 0 HugePages_Free:     20
>   Node 0 HugePages_Surp:      0
>	# `NUMA node1`保持 `HugePages` 不变
>   Node 1 HugePages_Total:    10
>   Node 1 HugePages_Free:     10
>   Node 1 HugePages_Surp:      0
```
### 4.2.3. 超发 nr_overcommit_hugepages

`/proc/sys/vm/nr_overcommit_hugepages`, 表示**最大超发**的 HugePages **个数**, 可读可写.

```
# 清零
echo 0 > /proc/sys/vm/nr_hugepages
echo 0 > /proc/sys/vm/nr_overcommit_hugepages

# 设置常驻`HugePages`个数是4
echo 4 > /proc/sys/vm/nr_hugepages
# 设置超发`HugePages`个数是4
echo 4 > /proc/sys/vm/nr_overcommit_hugepages
```

## 4.3. 不同场景的信息示例

```
# [没有使用`HugePages`]
cat /proc/meminfo | grep HugePages_
>	HugePages_Total:       4
>	HugePages_Free:        4
>	HugePages_Rsvd:        0
>	HugePages_Surp:        0
```

![2022-02-26-21-47-26.png](./images/2022-02-26-21-47-26.png)

---

```
# [合计使用了3个`HugePages`. 但是没有读写]
cat /proc/meminfo | grep HugePages_
>	HugePages_Total:       4	# 总共分配了4个`HugePages`
>	HugePages_Free:        4	# 由于没有读写. 所以空闲4个`HugePages`
>	HugePages_Rsvd:        3	# 由于没有读写. 所以保留3个`HugePages`
>	HugePages_Surp:        0	# 实际超发了0个`HugePages`
```

![2022-02-26-21-50-10.png](./images/2022-02-26-21-50-10.png)

---

```
# [合计用了3个`HugePages`. 并且已经读写]
cat /proc/meminfo | grep HugePages_
>	HugePages_Total:       4	# 总共分配了4个`HugePages`
>	HugePages_Free:        1	# 由于已经读写. 所以空闲1个`HugePages`
>	HugePages_Rsvd:        0	# 由于已经读写. 所以没有保留`HugePages`
>	HugePages_Surp:        0	# 实际超发了0个`HugePages`
```

![2022-02-26-21-50-41.png](./images/2022-02-26-21-50-41.png)

---

```
# [合计用了6个`HugePages`. 但是没有读写]
cat /proc/meminfo | grep HugePages_
>	HugePages_Total:       6	# 总共分配了6个`HugePages`
>	HugePages_Free:        6	# 由于没有读写. 所以空闲6个`HugePages`
>	HugePages_Rsvd:        6	# 由于没有读写. 所以保留6个`HugePages`
>	HugePages_Surp:        2	# 实际超发了2个`HugePages`
```

![2022-02-26-21-50-57.png](./images/2022-02-26-21-50-57.png)

---

```
# [合计用了6个`HugePages`. 并且已经读写]
cat /proc/meminfo | grep HugePages_
>	HugePages_Total:       6	# 总共分配了6个`HugePages`
>	HugePages_Free:        0	# 由于已经读写. 所以没有空闲`HugePages`
>	HugePages_Rsvd:        0	# 由于已经读写. 所以没有保留`HugePages`
>	HugePages_Surp:        2	# 实际超发了2个`HugePages`
```

![2022-02-26-21-51-18.png](./images/2022-02-26-21-51-18.png)

## 4.4. 参数之间的关系

`nr_hugepages <= HugePages_Total <= (nr_hugepages + nr_overcommit_hugepages)`

`HugePages_Rsvd <= HugePages_Free <= HugePages_Total`

`HugePages_Surp == (HugePages_Total - nr_hugepages) <= nr_overcommit_hugepages`

## 4.5. 不同大小的设置或查看

如果系统支持**多种大小**的 HugePages, 就会有**多个** `/sys/kernel/mm/hugepages/hugepages-${huge_page_size}` **目录**.

`${huge_page_size}`为 HugePages 的字节数. 默认是**2048kB**.

每一个目录下存在相同的的一些 HugePages 系统文件. 可用于**设置或者查看系统信息**.

<table style="width:100%">
<caption> /sys/kernel/mm/hugepages/hugepages-${huge_page_size} </caption>
  <tr>
    <th>文件</th>
    <th>描述</th>
    <th>权限</th>
  </tr>
  <tr>
    <td>nr_hugepages</td>
    <td>写入或读取实际的永久大页个数</td>
    <td>读写</td>
  </tr>
  <tr>
    <td>nr_hugepages_mempolicy</td>
    <td>NUMA node的 HugePages 个数</td>
    <td>读写</td>
  </tr>
  <tr>
    <td>nr_overcommit_hugepages</td>
    <td>临时大页的最大个数</td>
    <td>读写</td>
  </tr>
  <tr>
    <td>free_hugepages</td>
    <td>系统中空闲的 persistent Huge Page 个数</td>
    <td><b>只读</b></td>
  </tr>
  <tr>
    <td>resv_hugepages</td>
    <td>已经被分配但是未被实际使用的 Huge Page 个数</td>
    <td><b>只读</b></td>
  </tr>
  <tr>
    <td>surplus_hugepages</td>
    <td>实际使用中的超发HugePages个数</td>
    <td><b>只读</b></td>
  </tr>
</table>

假设系统启动参数如下:

> hugepagesz=2M hugepages=5 hugepagesz=1G hugepages=4

```
# 打印`HugePages`系统文件
# ll /sys/kernel/mm/hugepages/ -R
/sys/kernel/mm/hugepages/:
total 0
drwxr-xr-x 4 root root 0 Feb 26 20:17 ./
drwxr-xr-x 6 root root 0 Feb 26 20:17 ../
drwxr-xr-x 2 root root 0 Feb 27 19:32 hugepages-1048576kB/
drwxr-xr-x 2 root root 0 Feb 27 19:32 hugepages-2048kB/

/sys/kernel/mm/hugepages/hugepages-1048576kB:
total 0
drwxr-xr-x 2 root root    0 Feb 27 19:32 ./
drwxr-xr-x 4 root root    0 Feb 26 20:17 ../
--w------- 1 root root 4096 Feb 27 19:32 demote
-rw-r--r-- 1 root root 4096 Feb 27 19:32 demote_size
-r--r--r-- 1 root root 4096 Feb 27 19:32 free_hugepages
-rw-r--r-- 1 root root 4096 Feb 27 19:32 nr_hugepages
-rw-r--r-- 1 root root 4096 Feb 27 19:32 nr_hugepages_mempolicy
-rw-r--r-- 1 root root 4096 Feb 27 19:32 nr_overcommit_hugepages
-r--r--r-- 1 root root 4096 Feb 27 19:32 resv_hugepages
-r--r--r-- 1 root root 4096 Feb 27 19:32 surplus_hugepages

/sys/kernel/mm/hugepages/hugepages-2048kB:
total 0
drwxr-xr-x 2 root root    0 Feb 27 19:32 ./
drwxr-xr-x 4 root root    0 Feb 26 20:17 ../
-r--r--r-- 1 root root 4096 Feb 27 19:32 free_hugepages
-rw-r--r-- 1 root root 4096 Feb 27 19:32 nr_hugepages
-rw-r--r-- 1 root root 4096 Feb 27 19:32 nr_hugepages_mempolicy
-rw-r--r-- 1 root root 4096 Feb 27 19:32 nr_overcommit_hugepages
-r--r--r-- 1 root root 4096 Feb 27 19:32 resv_hugepages
-r--r--r-- 1 root root 4096 Feb 27 19:32 surplus_hugepages

# 打印`HugePages`系统文件的数值
# grep . -r /sys/kernel/mm/hugepages/
/sys/kernel/mm/hugepages/hugepages-2048kB/free_hugepages:5
/sys/kernel/mm/hugepages/hugepages-2048kB/resv_hugepages:0
/sys/kernel/mm/hugepages/hugepages-2048kB/surplus_hugepages:0
/sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages_mempolicy:5
/sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages:5
/sys/kernel/mm/hugepages/hugepages-2048kB/nr_overcommit_hugepages:0
/sys/kernel/mm/hugepages/hugepages-1048576kB/demote_size:2048kB
/sys/kernel/mm/hugepages/hugepages-1048576kB/free_hugepages:4
/sys/kernel/mm/hugepages/hugepages-1048576kB/resv_hugepages:0
/sys/kernel/mm/hugepages/hugepages-1048576kB/surplus_hugepages:0
/sys/kernel/mm/hugepages/hugepages-1048576kB/nr_hugepages_mempolicy:4
grep: /sys/kernel/mm/hugepages/hugepages-1048576kB/demote: Permission denied
/sys/kernel/mm/hugepages/hugepages-1048576kB/nr_hugepages:4
/sys/kernel/mm/hugepages/hugepages-1048576kB/nr_overcommit_hugepages:0
```

# 5. NUMA node 的 HugePages

**单个 NUMA node** 的 HugePages 的 `/sys` 接口, 可以提供**更加准确的配置和查看功能**.

注:

* **无法配置** `nr_overcommit_hugepages`
* **无法查看** `resv_hugepages`

## 5.1. 单个 NUMA node 的设置接口

如果系统支持**多种大小**的 HugePages, **每个 NUMA node** 就会有**多个** `/sys/devices/system/node/node${numa_node_id}/hugepages/hugepages-${huge_page_size}` 目录.

* `${numa_node_id}` 的数值范围为整数, 计数从零开始.
* `${huge_page_size}` 为HugePages的字节数. 默认是2048kB.

每一个目录下存在相同的的一些 HugePages 系统文件. 可用于设置或者查看系统信息

<table style="width:100%">
<caption> /sys/devices/system/node/node${id}/hugepages/hugepages-${size} </caption>
  <tr>
    <th>文件</th>
    <th>描述</th>
    <th>权限</th>
  </tr>
  <tr>
    <td>nr_hugepages</td>
    <td>写入或读取实际的永久大页个数</td>
    <td>读写</td>
  </tr>
  <tr>
    <td>free_hugepages</td>
    <td>空闲的 HugePages 个数</td>
    <td>读写</td>
  </tr>
  <tr>
    <td>surplus_hugepages</td>
    <td>实际使用的超发HugePages个数</td>
    <td>读写</td>
  </tr>
</table>

例子:

```
# l -R /sys/devices/system/node/node*/hugepages/
--- noed0 ---
/sys/devices/system/node/node0/hugepages/:
hugepages-1048576kB/  hugepages-2048kB/

/sys/devices/system/node/node0/hugepages/hugepages-1048576kB:
demote  demote_size  free_hugepages  nr_hugepages  surplus_hugepages

/sys/devices/system/node/node0/hugepages/hugepages-2048kB:
free_hugepages  nr_hugepages  surplus_hugepages

--- node1 ---
/sys/devices/system/node/node1/hugepages/:
hugepages-1048576kB/  hugepages-2048kB/

/sys/devices/system/node/node1/hugepages/hugepages-1048576kB:
demote  demote_size  free_hugepages  nr_hugepages  surplus_hugepages

/sys/devices/system/node/node1/hugepages/hugepages-2048kB:
free_hugepages  nr_hugepages  surplus_hugepages


# grep . -r /sys/devices/system/node/node*/hugepages/
--- node0 ---
/sys/devices/system/node/node0/hugepages/hugepages-2048kB/free_hugepages:3
/sys/devices/system/node/node0/hugepages/hugepages-2048kB/surplus_hugepages:0
/sys/devices/system/node/node0/hugepages/hugepages-2048kB/nr_hugepages:3
/sys/devices/system/node/node0/hugepages/hugepages-1048576kB/demote_size:2048kB
/sys/devices/system/node/node0/hugepages/hugepages-1048576kB/free_hugepages:2
/sys/devices/system/node/node0/hugepages/hugepages-1048576kB/surplus_hugepages:0
grep: /sys/devices/system/node/node0/hugepages/hugepages-1048576kB/demote: Permission denied
/sys/devices/system/node/node0/hugepages/hugepages-1048576kB/nr_hugepages:2
--- node1 ---
/sys/devices/system/node/node1/hugepages/hugepages-2048kB/free_hugepages:2
/sys/devices/system/node/node1/hugepages/hugepages-2048kB/surplus_hugepages:0
/sys/devices/system/node/node1/hugepages/hugepages-2048kB/nr_hugepages:2
/sys/devices/system/node/node1/hugepages/hugepages-1048576kB/demote_size:2048kB
/sys/devices/system/node/node1/hugepages/hugepages-1048576kB/free_hugepages:2
/sys/devices/system/node/node1/hugepages/hugepages-1048576kB/surplus_hugepages:0
grep: /sys/devices/system/node/node1/hugepages/hugepages-1048576kB/demote: Permission denied
/sys/devices/system/node/node1/hugepages/hugepages-1048576kB/nr_hugepages:2
```

例子:

```
# 清零
echo 0 > /sys/devices/system/node/node0/hugepages/hugepages-2048kB/nr_hugepages

# `NUMA node0`分配 4 个 `HugePages`
echo 4 > /sys/devices/system/node/node0/hugepages/hugepages-2048kB/nr_hugepages

# 超发`HugePages`为 4
echo 4 > /sys/kernel/mm/hugepages/hugepages-2048kB/nr_overcommit_hugepages
```

## 5.2. 单个 NUMA node 的使用情况查看

`/sys/devices/system/node/node${numa_node_id}/meminfo`, 可以查看 `${numa_node_id}` 的 HugePages 内存信息.

例子:

```
# grep Huge /sys/devices/system/node/node*/meminfo
--- node0 ---
/sys/devices/system/node/node0/meminfo:Node 0 AnonHugePages:    188416 kB
/sys/devices/system/node/node0/meminfo:Node 0 ShmemHugePages:        0 kB
/sys/devices/system/node/node0/meminfo:Node 0 FileHugePages:        0 kB
/sys/devices/system/node/node0/meminfo:Node 0 HugePages_Total:     3
/sys/devices/system/node/node0/meminfo:Node 0 HugePages_Free:      3
/sys/devices/system/node/node0/meminfo:Node 0 HugePages_Surp:      0
--- node1 ---
/sys/devices/system/node/node1/meminfo:Node 1 AnonHugePages:     10240 kB
/sys/devices/system/node/node1/meminfo:Node 1 ShmemHugePages:        0 kB
/sys/devices/system/node/node1/meminfo:Node 1 FileHugePages:        0 kB
/sys/devices/system/node/node1/meminfo:Node 1 HugePages_Total:     2
/sys/devices/system/node/node1/meminfo:Node 1 HugePages_Free:      2
/sys/devices/system/node/node1/meminfo:Node 1 HugePages_Surp:      0
```

以下的 `free_hugepages` 和 `surplus_hugepages` 也可以用于查看单个NUMA node的HugePages内存信息.

```
/sys/devices/system/node/node${numa_node_id}/hugepages/hugepages-${huge_page_size}/free_hugepages

/sys/devices/system/node/node${numa_node_id}/hugepages/hugepages-${huge_page_size}/surplus_hugepages
```

> `${huge_page_size}` 为 `HugePages` 的**字节数**. 默认是2048kB.

## 5.3. 不同场景的信息示例

```
# [没有使用`HugePages`]
grep . -r /sys/kernel/mm/hugepages/
>	/sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages:4
>	/sys/kernel/mm/hugepages/hugepages-2048kB/nr_overcommit_hugepages:4
>	/sys/kernel/mm/hugepages/hugepages-2048kB/free_hugepages:4
>	/sys/kernel/mm/hugepages/hugepages-2048kB/resv_hugepages:0
>	/sys/kernel/mm/hugepages/hugepages-2048kB/surplus_hugepages:0
>	/sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages_mempolicy:4

grep . -r /sys/devices/system/node/node*/hugepages/
>	/sys/devices/system/node/node0/hugepages/hugepages-2048kB/nr_hugepages:4
>	/sys/devices/system/node/node0/hugepages/hugepages-2048kB/free_hugepages:4
>	/sys/devices/system/node/node0/hugepages/hugepages-2048kB/surplus_hugepages:0
```

![2022-02-27-21-29-57.png](./images/2022-02-27-21-29-57.png)

---

```
# [合计使用了3个`HugePages`. 但是没有读写]
grep . -r /sys/kernel/mm/hugepages/
>	/sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages:4
>	/sys/kernel/mm/hugepages/hugepages-2048kB/nr_overcommit_hugepages:4
>	/sys/kernel/mm/hugepages/hugepages-2048kB/free_hugepages:4
>	/sys/kernel/mm/hugepages/hugepages-2048kB/resv_hugepages:3
>	/sys/kernel/mm/hugepages/hugepages-2048kB/surplus_hugepages:0
>	/sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages_mempolicy:4

grep . -r /sys/devices/system/node/node*/hugepages/
>	/sys/devices/system/node/node0/hugepages/hugepages-2048kB/nr_hugepages:4
>	/sys/devices/system/node/node0/hugepages/hugepages-2048kB/free_hugepages:4
>	/sys/devices/system/node/node0/hugepages/hugepages-2048kB/surplus_hugepages:0
```

![2022-02-27-21-30-46.png](./images/2022-02-27-21-30-46.png)

---

```
# [合计用了3个`HugePages`. 并且已经读写]
grep . -r /sys/kernel/mm/hugepages/
>	/sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages:4
>	/sys/kernel/mm/hugepages/hugepages-2048kB/nr_overcommit_hugepages:4
>	/sys/kernel/mm/hugepages/hugepages-2048kB/free_hugepages:1
>	/sys/kernel/mm/hugepages/hugepages-2048kB/resv_hugepages:0
>	/sys/kernel/mm/hugepages/hugepages-2048kB/surplus_hugepages:0
>	/sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages_mempolicy:4

grep . -r /sys/devices/system/node/node*/hugepages/
>	/sys/devices/system/node/node0/hugepages/hugepages-2048kB/nr_hugepages:4
>	/sys/devices/system/node/node0/hugepages/hugepages-2048kB/free_hugepages:1
>	/sys/devices/system/node/node0/hugepages/hugepages-2048kB/surplus_hugepages:0
```

![2022-02-27-21-31-42.png](./images/2022-02-27-21-31-42.png)

---

```
# [合计用了6个`HugePages`. 但是没有读写]
grep . -r /sys/kernel/mm/hugepages/
>	/sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages:6
>	/sys/kernel/mm/hugepages/hugepages-2048kB/nr_overcommit_hugepages:4
>	/sys/kernel/mm/hugepages/hugepages-2048kB/free_hugepages:6
>	/sys/kernel/mm/hugepages/hugepages-2048kB/resv_hugepages:6
>	/sys/kernel/mm/hugepages/hugepages-2048kB/surplus_hugepages:2
>	/sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages_mempolicy:6

grep . -r /sys/devices/system/node/node*/hugepages/
>	/sys/devices/system/node/node0/hugepages/hugepages-2048kB/nr_hugepages:6
>	/sys/devices/system/node/node0/hugepages/hugepages-2048kB/free_hugepages:6
>	/sys/devices/system/node/node0/hugepages/hugepages-2048kB/surplus_hugepages:2
```

![2022-02-27-21-34-30.png](./images/2022-02-27-21-34-30.png)

---

```
# [合计用了6个`HugePages`. 并且已经读写]
grep . -r /sys/kernel/mm/hugepages/
>	/sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages:6
>	/sys/kernel/mm/hugepages/hugepages-2048kB/nr_overcommit_hugepages:4
>	/sys/kernel/mm/hugepages/hugepages-2048kB/free_hugepages:0
>	/sys/kernel/mm/hugepages/hugepages-2048kB/resv_hugepages:0
>	/sys/kernel/mm/hugepages/hugepages-2048kB/surplus_hugepages:2
>	/sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages_mempolicy:6

grep . -r /sys/devices/system/node/node*/hugepages/
>	/sys/devices/system/node/node0/hugepages/hugepages-2048kB/nr_hugepages:6
>	/sys/devices/system/node/node0/hugepages/hugepages-2048kB/free_hugepages:0
>	/sys/devices/system/node/node0/hugepages/hugepages-2048kB/surplus_hugepages:2
```

![2022-02-27-21-34-54.png](./images/2022-02-27-21-34-54.png)

# 6. 使用方法

内存映射, 简而言之就是将**内核空间**的**一段内存区域**映射到**用户空间**. 映射成功后, 用户对**这段内存区域的修改**可以直接反映到**内核空间**, 同时, 内核空间对这段区域的修改也**直接反映用户空间**. 那么对于内核空间与用户空间两者之间需要大量数据传输等操作的话效率是非常高的. 当然, 也可以将**内核空间**的一段内存区域**同时映射**到**多个进程**, 这样还可以实现**进程间的共享内存通信**.

用户空间 `mmap()` 函数

```
void *mmap(void *start, size_t length, int prot, int flags,int fd, off_t offset)
```

* start: 用户进程中要映射的**用户空间**的**起始地址**, 通常为NULL(由**内核来指定**)
* length: 要映射的内存区域的大小
* prot: 期望的内存保护标志
* flags: 指定映射对象的类型
* fd: 文件描述符(由open函数返回), 表示这个是一个**文件映射**, fd是打开**文件的句柄**.
* offset: 设置在内核空间中已经分配好的的内存区域中的偏移, 例如文件的偏移量, 大小为PAGE_SIZE的整数倍
* 返回值: mmap() 返回被映射区的指针, 该指针就是需要映射的内核空间在用户空间的**虚拟地址**

参数 fd 可以看出mmap映射是否和文件相关联, 因此在Linux内核中**映射**可以分成**匿名映射**和**文件映射**.

- **匿名映射**: 没有文件支持的内存映射, 把**物理内存**映射到**进程的虚拟地址空间**, 没有数据源, 这种映射的**内存区域的内容会被初始化为0**.

- **文件映射**: 映射和实际文件相关联, 通常是把**文件的内容**映射到**进程地址空间**, 这样**应用程序**就可以像**操作进程地址空间**一样**读写文件**.

## mmap、SYSV共享内存异同

mmap 有名和匿名, 以及SysV共享内存都使用内核中HugePage. 

有名 mmap 需要借助 hugetlbfs 文件系统; 匿名 mmap 和 SysV 共享内存只需要使用特殊标志位(`MAP_HUGETLB/SHM_HUGETLB`).

有名 mmap 和 SYSV 共享内存都可以跨进程访问; 匿名mmap只能本进程访问.

## 6.1. 匿名映射

```
#define MAP_LENGTH (10 * 1024 * 1024)
addr = mmap(0, MAP_LENGTH, PROT_READ | PROT_WRITE, MAP_ANONYMOUS | MAP_HUGETLB, -1, 0);
```

创建**匿名的大页映射**, 代码参见 `tools/testing/selftests/vm/map_hugetlb.c`

---

mmap 匿名映射: `mmap.anonymous.c`

输出:

```
Returned address is 0x7ff600200000
First hex is 0
First hex is 3020100
```

查看进程的maps如下:

```
...
7ff600200000-7ff610200000 rw-p 00000000 00:0f 10940472                   /anon_hugepage (deleted)
...
```

## 6.3. 基于文件的映射

创建**基于文件的大页映射**, 代码参见 `tools/testing/selftests/vm/hugepage-mmap.c`

---

附件 `mmap.fs.c`

输出:

```
Returned address is 0x7f2d8ba00000
First hex is 0
First hex is 3020100
```

查看进程 maps 如下:

```
...
7f2d8ba00000-7f2d9ba00000 rw-s 00000000 00:9b 10940003                   /home/al/hugepage/huge/hugepagefile
...
```

查看文件系统:

```
-rwxr-xr-x 1 root root 258M 11月 29 23:06 hugepagefile
```

### 6.3.1. 挂载 hugetlbfs

1. 首先管理员需要在**某个目录**下**挂载 hugetlbfs 文件系统**:

```
mount -t hugetlbfs \
-o uid=<value>,gid=<value>,mode=<value>,pagesize=<value>,size=<value>, \
min_size=<value>,nr_inodes=<value> none <目录>
```

各选项的意思如下.

(1) 选项 uid 和 gid 指定**文件系统的根目录**的**用户**和**组**, 默认取**当前进程的用户和组**.

(2) 选项 mode 指定文件系统的根目录的模式, 默认值是0755.

(3) 如果**平台支持多种大页长度**, 可以使用选项 pagesize 指定大页长度和关联的大页池. 如果不使用选项 pagesize, 表示使用**默认的大页长度**.

(4) 选项 size 指定允许文件系统使用的**大页**的**最大数量**. 如果不指定选项size, 表示没有限制.

(5) 选项 `min_size` 指定允许文件系统使用的**大页**的**最小数量**. 挂载文件系统的时候, 申请大页池为这个文件系统**预留**选项 `min_size` 指定的大页数量. 如果不指定选项`min_size`, 表示没有限制.

(6) 选项 `nr_inodes` 指定文件系统中文件(一个文件对应一个索引节点) 的最大数量. 如果不指定选项nr_inodes, 表示没有限制.

### 6.3.2. 创建文件和创建内存映射

假设在**目录** “`/mnt/huge`” 下**挂载**了 **hugetlbfs** 文件系统, **应用程序**在 hugetlbfs 文件系统中**创建文件**, 然后**创建基于文件的内存映射**, 这个内存映射就会使用大页.

```
#define MAP_LENGTH (10 * 1024 * 1024)
fd = open("/mnt/huge/test", O_CREAT | O_RDWR, S_IREXU);
addr = mmap(0, MAP_LENGTH, PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0);
```

> 两个进程针对同一个文件创建共享的内存映射, 实现共享内存.

## 6.2. shm

参见文件 `shm.1.c`

初始化并查看系统的 hugepages 信息

```
# 整个系统使用情况
# cat /proc/meminfo | grep -i huge
AnonHugePages:    196608 kB
ShmemHugePages:        0 kB
FileHugePages:         0 kB
HugePages_Total:      10
HugePages_Free:       10
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:       2048 kB
Hugetlb:           20480 kB

# 整个系统设置情况
# grep . /proc/sys/vm/*huge*
/proc/sys/vm/hugetlb_shm_group:0
/proc/sys/vm/nr_hugepages:10
/proc/sys/vm/nr_hugepages_mempolicy:10
/proc/sys/vm/nr_overcommit_hugepages:4

# 整个系统不同 size 的设置情况
# grep . -r /sys/kernel/mm/hugepages/
/sys/kernel/mm/hugepages/hugepages-2048kB/free_hugepages:10
/sys/kernel/mm/hugepages/hugepages-2048kB/resv_hugepages:0
/sys/kernel/mm/hugepages/hugepages-2048kB/surplus_hugepages:0
/sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages_mempolicy:10
/sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages:10
/sys/kernel/mm/hugepages/hugepages-2048kB/nr_overcommit_hugepages:4
/sys/kernel/mm/hugepages/hugepages-1048576kB/demote_size:2048kB
/sys/kernel/mm/hugepages/hugepages-1048576kB/free_hugepages:0
/sys/kernel/mm/hugepages/hugepages-1048576kB/resv_hugepages:0
/sys/kernel/mm/hugepages/hugepages-1048576kB/surplus_hugepages:0
/sys/kernel/mm/hugepages/hugepages-1048576kB/nr_hugepages_mempolicy:0
grep: /sys/kernel/mm/hugepages/hugepages-1048576kB/demote: Permission denied
/sys/kernel/mm/hugepages/hugepages-1048576kB/nr_hugepages:0
/sys/kernel/mm/hugepages/hugepages-1048576kB/nr_overcommit_hugepages:0

-----------------------------------

# 不同 node 的设置情况
# grep . -r /sys/devices/system/node/node*/hugepages/
/sys/devices/system/node/node0/hugepages/hugepages-2048kB/free_hugepages:5
/sys/devices/system/node/node0/hugepages/hugepages-2048kB/surplus_hugepages:0
/sys/devices/system/node/node0/hugepages/hugepages-2048kB/nr_hugepages:5
/sys/devices/system/node/node0/hugepages/hugepages-1048576kB/demote_size:2048kB
/sys/devices/system/node/node0/hugepages/hugepages-1048576kB/free_hugepages:0
/sys/devices/system/node/node0/hugepages/hugepages-1048576kB/surplus_hugepages:0
grep: /sys/devices/system/node/node0/hugepages/hugepages-1048576kB/demote: Permission denied
/sys/devices/system/node/node0/hugepages/hugepages-1048576kB/nr_hugepages:0
/sys/devices/system/node/node1/hugepages/hugepages-2048kB/free_hugepages:5
/sys/devices/system/node/node1/hugepages/hugepages-2048kB/surplus_hugepages:0
/sys/devices/system/node/node1/hugepages/hugepages-2048kB/nr_hugepages:5
/sys/devices/system/node/node1/hugepages/hugepages-1048576kB/demote_size:2048kB
/sys/devices/system/node/node1/hugepages/hugepages-1048576kB/free_hugepages:0
/sys/devices/system/node/node1/hugepages/hugepages-1048576kB/surplus_hugepages:0
grep: /sys/devices/system/node/node1/hugepages/hugepages-1048576kB/demote: Permission denied
/sys/devices/system/node/node1/hugepages/hugepages-1048576kB/nr_hugepages:0

# 不同 node 的使用情况
# grep Huge /sys/devices/system/node/node*/meminfo
/sys/devices/system/node/node0/meminfo:Node 0 AnonHugePages:     57344 kB
/sys/devices/system/node/node0/meminfo:Node 0 ShmemHugePages:        0 kB
/sys/devices/system/node/node0/meminfo:Node 0 FileHugePages:        0 kB
/sys/devices/system/node/node0/meminfo:Node 0 HugePages_Total:     5
/sys/devices/system/node/node0/meminfo:Node 0 HugePages_Free:      5
/sys/devices/system/node/node0/meminfo:Node 0 HugePages_Surp:      0
/sys/devices/system/node/node1/meminfo:Node 1 AnonHugePages:    143360 kB
/sys/devices/system/node/node1/meminfo:Node 1 ShmemHugePages:        0 kB
/sys/devices/system/node/node1/meminfo:Node 1 FileHugePages:        0 kB
/sys/devices/system/node/node1/meminfo:Node 1 HugePages_Total:     5
/sys/devices/system/node/node1/meminfo:Node 1 HugePages_Free:      5
/sys/devices/system/node/node1/meminfo:Node 1 HugePages_Surp:      0
```

在执行过程中可以查看系统信息.

---

`tools/testing/selftests/vm/hugepage-shm.c`

---

附件: `shm.2.c`

执行结果如下:

```
shmid: 32407590
shmaddr: 0x7f1fc2c00000
Starting the writes:
................................................................................................................................................................................................................................................................
Starting the Check...Done.
```

查看进程maps如下:

```
...
7f1fc2c00000-7f1fd2c00000 rw-s 00000000 00:0f 32407590                   /SYSV12345678 (deleted)
...
```

共享内存使用情况如下:

```
------ Shared Memory Segments --------
key        shmid      owner      perms      bytes      nattch     status
...
0x12345678 32407590   root       600        268435456  1
```

## 6.4. hugetlbfs 库

源码: https://github.com/libhugetlbfs/libhugetlbfs

**应用程序**可以使用**开源**的 **hugetlbfs 库**, 这个库**对 hugetlbfs 文件系统做了封装**.

使用 hugetlbfs 库的好处如下:

(1) **启动程序**时使用**环境变量** “`LD_PRELOAD=libhugetlbfs.so`” 把 **hugetlbfs 库**设置成优先级最高的**动态库**, `malloc()` **使用大页**, 对应用程序完全透明, **应用程序不需要修改代码**.

(2) 可以把代码段、数据段和未初始化数据段都放在大页中.

《HOWTO》对libhugetlbfs做了详细介绍：

1. 通过 libhugetlbfs 对使用 hugetlbfs 提供了一套方便的应用程序接口；使用libhugetblfs替代目前库中malloc()函数, 使内存分配在HugePage上进行；libhugetlbfs能使进程 test/data/bss 段在HugePage上分配. 
2. 支持libhugetlbfs的硬件、内核、工具链、配置. 
3. 如何编译安装libhugetlbfs. 
4. 如何使用libhugetlbfs：替代malloc()、共享内存、进程text/data/bss段. 

安装libhugetlgfs:

```
sudo apt-get install libhugetlbfs libhugetlbfs-tests
```

建立挂载点:

```
sudo mount none /home/al/hugepage/huge -t hugetlbfs
```

使用hugeadm查看挂载情况:

```
hugeadm --list-all-mounts：
libhugetlbfs: ERROR: Line too long when parsing mounts
Mount Point            Options
/dev/hugepages         rw,relatime,pagesize=2M
/home/al/hugepage/huge rw,relatime,pagesize=2M
hugeadm --pool-list：
libhugetlbfs: ERROR: Line too long when parsing mounts
      Size  Minimum  Current  Maximum  Default
   2097152      512      512      512        *
1073741824        0        0        0
```

使用如下脚本进行测试:

```
sudo /usr/lib/libhugetlbfs/tests/run_tests.py
```

执行结果:

```
run_tests.py: The 32 bit word size is not compatible with 2M pages
zero_filesize_segment (2M: 64):    PASS
test_root (2M: 64):    PASS
meminfo_nohuge (2M: 64):    PASS
gethugepagesize (2M: 64):    PASS
gethugepagesizes (2M: 64):    PASS
HUGETLB_VERBOSE=1 empty_mounts (2M: 64):    PASS
HUGETLB_VERBOSE=1 large_mounts (2M: 64):    PASS
find_path (2M: 64):    PASS
unlinked_fd (2M: 64):    PASS
readback (2M: 64):    PASS
truncate (2M: 64):    PASS
shared (2M: 64):    PASS
mprotect (2M: 64):    PASS
mlock (2M: 64):    PASS
misalign (2M: 64):    PASS
ptrace-write-hugepage (2M: 64):    PASS
icache-hygiene (2M: 64):    PASS
slbpacaflush (2M: 64):    PASS (inconclusive)
straddle_4GB_static (2M: 64):    PASS
huge_at_4GB_normal_below_static (2M: 64):    PASS
huge_below_4GB_normal_above_static (2M: 64):    PASS
map_high_truncate_2 (2M: 64):    PASS
misaligned_offset (2M: 64):    PASS (inconclusive)
truncate_above_4GB (2M: 64):    PASS
brk_near_huge (2M: 64):    brk_near_huge: malloc.c:2401: sysmalloc: Assertion `(old_top == initial_top (av) && old_size == 0) || ((unsigned long) (old_size) >= MINSIZE && prev_inuse (old_top) && ((unsigned long) old_end & (pagesize - 1)) == 0)' failed.
...
```

# 7. 实现原理

## 7.1. 大页池 struct hstate

内核使用**大页池**管理大页. 大页池的**数据结构**是结构体 `hstate`.

有的**处理器架构**支持多种大页长度, **一种大页长度**对应**一个大页池**.

有一个**默认的大页长度**, 默认**只创建**大页长度是**默认长度的大页池**. 例如 ARM64 架构在页长度为4KB的时候支持的大页长度是1GB、32MB、2MB和64KB, **默认的大页长度**是**2MB**, 默认只创建大页长度是2MB的**大页池**.

如果需要**创建**大页长度**不是默认长度的大页池**, 可以在引导内核时指定**内核参数** “`hugepagesz=<size>[kKmMgG]`”, 长度必须是处理器支持的长度. 可以使用内核参数 “`default_hugepagesz=<size>[kKmMgG]`” 选择默认的大页长度.

```cpp
// mm/hugetlb.c
int hugetlb_max_hstate __read_mostly;
unsigned int default_hstate_idx;
struct hstate hstates[HUGE_MAX_HSTATE];
```

* 全局数组 **hstates** 是**大页池数组**
* 全局变量 `hugetlb_max_hstate` 是**大页池的数量**
* 全局变量 `default_hstate_idx` 是**默认大页池**的**索引**.

**大页池**中的**大页**分为两种.

1) **永久大页**: 永久大页是保留的, 不能有其他用途, **被预先分配到大页池**, 当**进程释放**永久大页的时候, 永久大页被归还到**大页池**.

2) **临时大页**: 也称为**多余的**(`surplus`) 大页, 当永久大页用完的时候, 可以从**页分配器**分配**临时大页**；进程释放临时大页的时候, **直接释放到页分配器**. 当设备长时间运行后, 内存可能碎片化, 分配临时大页可能失败.

大页池的数据结构 hstate 的主要成员如下表所示.

![2022-02-21-22-39-30.png](./images/2022-02-21-22-39-30.png)

## 7.2. 预先分配永久大页

预先分配指定数量的永久大页到大页池中有两种方法.

1) 最可靠的方法是在**引导内核**时指定**内核参数** “`hugepages=N`” 来分配永久大页, 因为内核初始化的时候**内存还没有碎片化**.

有些处理器架构支持多种大页长度. 如果要分配特定长度的大页, 必须在内核参数 “hugepages” 前面添加选择大页长度的参数
“`hugepagesz=<size>[kKmMgG]`”.

2) 通过文件 “`/proc/sys/vm/nr_hugepages`” 指定默认长度的永久大页的数量.

### 7.2.1. 内核参数 hugepages

> hugepagesz=2M hugepages=N hugepagesz=1G hugepages=N

> 1. 会先解析 hugepagesz, 再解析 hugepages, 没有 sz, pages 就是默认 sz 的; 不同 size 的都会调用
>
> 内核参数 “`hugepagesz=N`” 的处理函数是 `hugepagesz_setup`, 核心是调用 `hugetlb_add_hstate(ilog2(size) - PAGE_SHIFT)`, **初始化相应 size 的 hstate**.
>
> 2. 针对 1G 大页从 bootmem 中提前分配内存

由于 `__setup()` 在 `initcall()` 之前执行, 所以下面的命令都在 `hugetlb_init()` 之前执行.

内核参数 “`hugepages=N`” 的处理函数是 `hugetlb_nrpages_setup`, 其代码如下:

```cpp
// mm/hugetlb.c
1   static int __init hugetlb_nrpages_setup(char *s)
2   {
3    unsigned long *mhp;
4    static unsigned long *last_mhp;
5
6    if (!parsed_valid_hugepagesz) {
7         pr_warn("hugepages = %s preceded by "
8               "an unsupported hugepagesz, ignoring\n", s);
9         parsed_valid_hugepagesz = true;
10        return 1;
11   }
12   /*
13    * “! hugetlb_max_hstate”意味着没有解析一个“hugepagesz=”参数,
14    * 所以这个“hugepages=”参数对应默认的大页池.
15    */
16   else if (!hugetlb_max_hstate)
17        mhp = &default_hstate_max_huge_pages;
18   else
19        mhp = &parsed_hstate->max_huge_pages;
20
21   if (mhp == last_mhp) {
22        pr_warn("hugepages= specified twice without interleaving hugepagesz=,
	ignoring\n");
23        return 1;
24   }
25
26   if (sscanf(s, "%lu", mhp) <= 0)
27        *mhp = 0;
28
29   if (hugetlb_max_hstate && parsed_hstate->order >= MAX_ORDER)
30        hugetlb_hstate_alloc_pages(parsed_hstate);
31
32   last_mhp = mhp;
33
34   return 1;
35  }
36  __setup("hugepages=", hugetlb_nrpages_setup);
```

第6～10行代码, 如果**前一个内核参数** “`hugepagesz=`” 指定的**大页长度**是**非法**的, 直接返回.

第16行和第17行代码, 如果前面**没有内核参数** “`hugepagesz=`”, 那么内核参数 “`hugepages=`” 指定的是**默认大页池**的永久大页的**数量**.

第18行和第19行代码, 如果前面有内核参数 “`hugepagesz=`” 指定**大页长度**, 那么内核参数 “`hugepages=`” 指定**该大小大页**对应的大页池的永久大页的**数量**.

第26行代码, 解析并保存内核参数 “`hugepagesz=`” 的值.

第29行和第30行代码, 如果前面有内核参数 “`hugepagesz=`” 指定大页长度, 并且**大页长度**超过**页分配器**支持的**最大阶数**, 那么需要从**引导内存分配器**分配大页. 如果**大页长度小于页分配器支持的最大阶数**, 那就在**大页子系统初始化**的时候从**页分配器**分配大页.

> 这里并**没有**针对大页长度order小于页分配器 order(11, 2M) 这种情况分配页面

函数 `hugetlb_hstate_alloc_pages` 负责**预先分配**指定数量的永久大页, 其代码如下:

```cpp
// mm/hugetlb.c
1   static void __init hugetlb_hstate_alloc_pages(struct hstate *h)
2   {
3    unsigned long i;
4
5    for (i = 0; i < h->max_huge_pages; ++i) {
6         if (hstate_is_gigantic(h)) {
7               if (!alloc_bootmem_huge_page(h))
8                    break;
9         } else if (!alloc_fresh_huge_page(h,
10                          &node_states[N_MEMORY]))
11              break;
12   }
13   h->max_huge_pages = i;
14  }
```

第6行和第7行代码, 如果**大页长度超过页分配器支持的最大阶数**, 那么从**引导内存分配器**分配大页.

第9行代码, 如果**大页长度小于或等于页分配器支持的最大阶数**, 那么从**页分配器**分配大页.

函数 `alloc_bootmem_huge_page` 负责从**引导内存分配器**分配大页, 其代码如下:

```cpp
// mm/hugetlb.c
1   int __weak alloc_bootmem_huge_page(struct hstate *h)
2   {
3    struct huge_bootmem_page *m;
4    int nr_nodes, node;
5
6    for_each_node_mask_to_alloc(h, nr_nodes, node, &node_states[N_MEMORY]) {
7         void *addr;
8
9         addr = memblock_virt_alloc_try_nid_nopanic(
10                   huge_page_size(h), huge_page_size(h),
11                   0, BOOTMEM_ALLOC_ACCESSIBLE, node);
12        if (addr) {
13              m = addr;
14              goto found;
15        }
16   }
17   return 0;
18
19   found:
20    BUG_ON(! IS_ALIGNED(virt_to_phys(m), huge_page_size(h)));
21    /* 先把它们放到私有链表中, 因为 mem_map 还没准备好 */
22    list_add(&m->list, &huge_boot_pages);
23    m->hstate = h;
24    return 1;
25   }
```

第9行代码, 从内存节点分配**大页**.

第22行代码, 把大页添加到**链表** `huge_boot_pages` 中.

### 7.2.2. HugePage 分配器初始化

> 不是 hugetlbfs 文件系统
>
> 初始化所有大小的 hstate: 初始化 空闲大内存页链表; 针对 2M 大页从页分配器分配内存并初始化; 将大页放到空闲链表; 初始化前面 1G (bootmem) 大页并添加到空闲链表

`hugetlb_init()` 是Huge Page初始化入口, 属于 `subsys_initcall()`, 在 `arch_initcall()` 之后, `fs_initcall()` 之前.

HugePages 分配器初始化的调用链为:

```cpp
hugetlb_init()
 ├─ hugetlb_add_hstate(HUGETLB_PAGE_ORDER); // 添加一个大页池 hstate(确保HPAGE_SIZE 的 hstate 存在), hugepagesz_setup 也会调用
 │  ├─ 	if (size_to_hstate(PAGE_SIZE << order)) { return; } // 一种 page size 的 hstate 只能有一个
 │  ├─ h = &hstates[hugetlb_max_hstate++]; // hstate 增加一个
 │  ├─ h->order = order;
 │  ├─ for(i < MAX_NUMNODES) { INIT_LIST_HEAD(&h->hugepage_freelists[i]); } // 初始化空闲大页链表
 │  ├─ INIT_LIST_HEAD(&h->hugepage_activelist);
 │  ├─ snprintf(h->name, HSTATE_NAME_LEN, "hugepages-%lukB", huge_page_size(h)/1024); // hstate 名字
 │  └─ hugetlb_vmemmap_init(h);
 ├─ hugetlb_init_hstates(); // 初始化所有的 hstates
 │  ├─ for_each_hstate(h) // 遍历所有 hstate
 │  ├─ hugetlb_hstate_alloc_pages(h); // 针对 2M 大页分配内存; 1G的在之前 parse hugepages 时分配
 │  │  └─ alloc_pool_huge_page(); // 大页size小, 从页分配器分配
 │  │     ├─ alloc_fresh_huge_page();
 │  │     │  ├─ struct page *page = alloc_buddy_huge_page(); // 本质是调用__alloc_pages, 直接分配了一个大页(2M/1G), 一共有很多page结构体, 返回首个page结构体
 │  │     │  └─ prep_new_huge_page(h, page, page_to_nide(page)); // 所有大页都会调用其进行初始化
 │  │     │     ├─ INIT_LIST_HEAD(&page->lru);
; // page->lru 初始化
 │  │     │     ├─ set_compound_page_dtor(page, HUGETLB_PAGE_DTOR); // page[1].compound_dtor = HUGETLB_PAGE_DTOR, 对应的 destructor 函数是 free_huge_page
 │  │     │     ├─ hugetlb_set_page_subpool(page, NULL); // page[1].private=NULL
 │  │     │     ├─ set_hugetlb_cgroup(page, NULL); // 设置page[2].private为NULL
 │  │     │     ├─ set_hugetlb_cgroup_rsvd(page, NULL); // 设置 page[3].private 为NULL
 │  │     │     ├─ h->nr_huge_pages++; //
 │  │     │     └─ h->nr_huge_pages_node[nid]++; //
 │  │     └─ put_page(page); // 释放给 hugetlb allocator, 会调用上面的 destructor
 │  │        └─ __put_page(page);
 │  │           └─ __put_compound_page(page);
 │  │              └─ destroy_compound_page(page);
 │  │                 └─ compound_page_dtors[page[1].compound_dtor](page); // 调用 free_huge_page
 │  │                    └─ enqueue_huge_page(page); // 把大内存页放置到空闲大内存页链表中
 │  │                       ├─ list_move(&page->lru, &h->hugepage_freelists[nid]); // 把大内存页放置到空闲大内存页链表中
 │  │                       ├─ h->free_huge_pages++; // 增加计数器
 │  │                       └─ h->free_huge_pages_node[nid]++;
 │  └─ hugetlb_vmemmap_init(h);
 ├─ gather_bootmem_prealloc(); // 仅仅处理 bootmem 的大页
 │  ├─ 	list_for_each_entry(&huge_boot_pages) // 遍历 bootmem 的大页
 │  ├─ 	prep_compound_gigantic_page(page, huge_page_order(h)) //
 │  ├─ 	prep_new_huge_page(h, page, page_to_nide(page)); // 所有大页都会调用其进行初始化
 │  └─ put_page(page); // 把大内存页放置到空闲大内存页链表中
 ├─ report_hugepages(); // 输出当前系统支持的不同Huge Page大小以及分配页数
 ├─ hugetlb_sysfs_init(); // 在/sys/kernel/mm/hugepages目录下针对不同大小的Huge Paeg创建目录
 ├─ hugetlb_register_all_nodes(); // 处理NUMA架构下不同node的Huge Page
 └─ hugetlb_cgroup_file_init(); // 创建/sys/fs/cgroup/hugetlb下节点：hugetlb.2MB.failcnt、hugetlb.2MB.limit_in_bytes、hugetlb.2MB.max_usage_in_bytes、hugetlb.2MB.usage_in_bytes
```

在内核初始化时, 会调用 `hugetlb_init` 函数对 HugePages 分配器进行初始化, 其实现如下:

```cpp
static int __init hugetlb_init(void)
{
    unsigned long i;

    // 1. 初始化空闲大内存页链表 hugepage_freelists,
    // 内核使用 hugepage_freelists 链表把空闲的大内存页连接起来,
    // 为了分析简单, 我们可以把 MAX_NUMNODES 当成 1
    for (i = 0; i < MAX_NUMNODES; ++i)
        INIT_LIST_HEAD(&hugepage_freelists[i]);

    // 2. max_huge_pages 为系统能够使用的大页内存的数量,
    // 由系统启动项 hugepages 指定,
    // 这里主要申请大内存页, 并且保存到 hugepage_freelists 链表中.
    for (i = 0; i < max_huge_pages; ++i) {
        if (!alloc_fresh_huge_page())
            break;
    }

    max_huge_pages = free_huge_pages = nr_huge_pages = i;

    return 0;
}
```

`hugetlb_init` 函数主要完成**两个工作**:

* 初始化**空闲大内存页链表** `hugepage_freelists[]`, 这个链表保存了系统中能够使用的大内存. 每个 NUMA node 一个链表.
* 为系统**申请空闲的大内存页**, 并且保存到 `hugepage_freelists[]` 链表中.

在 Hugetlb 大页支持中, 同时支持 2MiB 和 1Gig 粒度的大页, 大页的物理内存来自 Buddy 内存分配器分配的复合页, 从 buddy 中分配的大页是特殊复合页构成的大页, 其复合页的析构函数为 `HUGETLB_PAGE_DTOR`. `alloc_fresh_huge_page()` 函数的作用就是从 Buddy 分配器中分配物理大页, 其底层逻辑是: 函数首先调用 hstate_is_gigantic() 函数判断大页是 1Gig 的大页还是 2MiB 的大页, 如果是 1Gig 的大页, 函数调用 alloc_gigantic_page() 函数分配 1Gig 的物理内存; 反之如果 2MiB 的大页, 函数调用 alloc_buddy_huge_page() 函数分配 2MiB 的物理内存. 如果物理内存分配失败, 函数直接返回 NULL; 反之分配成功, 此时大页是 1Gig 的大页, 那么函数调用 prep_compound_gigantic_page() 函数初始化 1Gig 的大页. 函数最后调用 prep_new_huge_page() 函数初始化大页.

分析下 `alloc_fresh_huge_page` 函数是怎么**申请大内存页**的, 其实现如下:

```cpp
static int alloc_fresh_huge_page(void)
{
    static int prev_nid;
    struct page *page;
    int nid;
    ...
    // 1. 申请一个大的物理内存页...
    page = alloc_pages_node(nid, htlb_alloc_mask|__GFP_COMP|__GFP_NOWARN,
                            HUGETLB_PAGE_ORDER);

    if (page) {
        // 2. 设置释放大内存页的回调函数为 free_huge_page
        set_compound_page_dtor(page, free_huge_page);
        ...
        // 3. put_page 函数将会调用上面设置的 free_huge_page 函数把内存页放入到空闲大页链表中
        // put_page 意思是将 page 从 buddy 中释放
        put_page(page);

        return 1;
    }

    return 0;
}
```

所以, `alloc_fresh_huge_page` 函数主要完成三个工作:

* 调用 `alloc_pages_node` 函数**申请一个大内存页**(2MB).
* 设置大内存页的**释放回调函数**为 `free_huge_page`, 当释放大内存页时, 将会调用这个函数进行释放操作.
* 调用 `put_page` 函数**从 buddy 中释放大内存页**, 其将会调用 `free_huge_page` 函数进行相关操作.

那么, 我们来看看 `free_huge_page` 函数是怎么释放大内存页的, 其实现如下:

```cpp
static void free_huge_page(struct page *page)
{
    ...
    enqueue_huge_page(page);     // 把大内存页放置到空闲大内存页链表中
    ...
}

static void enqueue_huge_page(struct page *page)
{
    int nid = page_to_nid(page); // 我们假设这里一定返回 0

    // 把大内存页添加到空闲链表 hugepage_freelists 中
    list_add(&page->lru, &hugepage_freelists[nid]);

    // 增加计数器
    free_huge_pages++;
    free_huge_pages_node[nid]++;
}
```

从上面的实现可知, `enqueue_huge_page` 函数只是简单的把**大内存页**添加到**空闲链表** `hugepage_freelists` 中, 并且增加计数器.

假如我们设置了系统能够使用的大内存页为 100 个, 那么空闲大内存页链表 `hugepage_freelists` 的结构如下图所示:

![2022-03-08-09-50-33.png](./images/2022-03-08-09-50-33.png)

### 7.2.3. 写 nr_hugepages 文件

系统运行起来后, 可以通过 `/proc/sys/vm/nr_hugepages` 设置, 系统根据实际情况分配或释放 HugePages.

> 设置默认大小的大页池的 hugepage 数量:
>
> 当增加 `nr_hugepages`, 系统优先使用 surplus 中页面. 然后才会分配新的 Huge Page 来满足需求. 从页分配器分配大页, surplus 只是调整 hstate 属性
>
> 当减小 `nr_hugepages`, 导致使用中页面大于 `nr_hugepages` 时, 将**使用中的页面**转换成 surplus 页面
> 当减小 nr_hugepaes, 导致使用中页面大于 `nr_hugepages + nr_overcommit_hugepages` 时, 同样会将超出 `nr_hugepages` 的页面转成 surplus 页面. 直到 nr_hugepages + nr_overcommit_hugepages 足够大, 或者释放足够多的surplus页面, 否则不会继续申请 surplus 页面.

文件 “`/proc/sys/vm/nr_hugepages`” 的处理函数是 `hugetlb_sysctl_handler`, 最终调用函数 `set_max_huge_pages` 来**增加或减少永久大页**, 其代码如下:

```cpp
// hugetlb_sysctl_handler() -> hugetlb_sysctl_handler_common() -> __nr_hugepages_store_common() -> set_max_huge_pages()
// mm/hugetlb.c
1   static unsigned long set_max_huge_pages(struct hstate *h, unsigned long count,
2                               nodemask_t *nodes_allowed)
3   {
4    unsigned long min_count, ret;
5
6    if (hstate_is_gigantic(h) && ! gigantic_page_supported())
7         return h->max_huge_pages;
8
9    spin_lock(&hugetlb_lock);
#    // 增加永久大页, 如果有 surplus page, 减少 surplug page 属性
10   while (h->surplus_huge_pages && count > persistent_huge_pages(h)) {
11        if (!adjust_pool_surplus(h, nodes_allowed, -1))
12              break;
13   }
14   // 增加永久大页, 分配大页
15   while (count > persistent_huge_pages(h)) {
16         spin_unlock(&hugetlb_lock);
17
18         /* 让出处理器, 避免死锁(soft lockup) */
19         cond_resched();
20         // 1G 大页
21         if (hstate_is_gigantic(h))
22               ret = alloc_fresh_gigantic_page(h, nodes_allowed);
23         else
24               ret = alloc_fresh_huge_page(h, nodes_allowed);
25         spin_lock(&hugetlb_lock);
#          // 上面分配成功会返回 1, 这里不会跳出
#          // 失败了才跳出去
26         if (!ret)
27               goto out;
28
29         /* 去处理信号, 用户可能按下ctrl+c组合键 */
30         if (signal_pending(current))
31               goto out;
32   }
33   // 上面分配大页成功也还是会执行这里的代码
34   min_count = h->resv_huge_pages + h->nr_huge_pages - h->free_huge_pages;
35   min_count = max(count, min_count);
36   try_to_free_low(h, min_count, nodes_allowed);
37   while (min_count < persistent_huge_pages(h)) {
38         if (!free_pool_huge_page(h, nodes_allowed, 0))
39               break;
40         cond_resched_lock(&hugetlb_lock);
41   }
42   while (count < persistent_huge_pages(h)) {
43         if (!adjust_pool_surplus(h, nodes_allowed, 1))
44               break;
45   }
46  out:
47   ret = persistent_huge_pages(h);
48   spin_unlock(&hugetlb_lock);
49   return ret;
50  }
```

参数 count 指定**永久大页**的**最大数量**.

第10～32行代码, 如果**增加永久大页**的数量, 处理如下:

* 第10～13行代码, 如果有**临时大页**, 那么**把临时大页转换为永久大页**. 这里**只是调整**了 `h->surplus_huge_pages` 和 `h->surplus_huge_pages_node[node]` 属性值, 下面的**分配大页代码**还是会执行.

```cpp
static int adjust_pool_surplus(struct hstate *h, nodemask_t *nodes_allowed,
				int delta)
{
	int nr_nodes, node;

	lockdep_assert_held(&hugetlb_lock);
	VM_BUG_ON(delta != -1 && delta != 1);

	if (delta < 0) {
		// 遍历 nodes_allowed 参数指定的 NUMA NODE,
		// 如果某个 node 的 h->surplus_huge_pages_node[node] 存在, 则确定有 surplus page
		for_each_node_mask_to_alloc(h, nr_nodes, node, nodes_allowed) {
			if (h->surplus_huge_pages_node[node])
				goto found;
		}
	} else {
		for_each_node_mask_to_free(h, nr_nodes, node, nodes_allowed) {
			if (h->surplus_huge_pages_node[node] <
					h->nr_huge_pages_node[node])
				goto found;
		}
	}
	return 0;

found:
	h->surplus_huge_pages += delta;
	h->surplus_huge_pages_node[node] += delta;
	return 1;
}
```

* 第`15～32`行代码, **分配大页**. 分配失败的话跳到 out 退出. 分配成功, **继续执行**.

第 34 行开始就是**减小永久大页的数量**的逻辑. (尽管增加永久大页逻辑也会执行到这里).

第34行代码, `min_count` 等于(**大页总数** − **空闲大页数量** + **预留大页数量** ), 即 `min_count` 等于以及(**分配出去的大页** + **已经预留的大页**), 可以理解为等于**已经被系统使用的大页**.

> 注意**预留大页数量**包含在空闲大页数量里面, 进程创建内存映射的时候已经申请预留大页, 当 write 才会真正分配.

第35行代码, `max(count, min_count)`, **设置值**和**已被使用值**两者取其大.

* 第36行代码, **如果支持高端内存区域**, 优先把从低端内存区域分配的**没有预留的空闲大页**归还给**页分配器**.

* 第37～41行代码, 如果**系统现有永久大页的数量**超过 `min_count`, 那么把**没有预留的空闲大页**归还给**页分配器**.

* 第42～45行代码, 如果**系统现有永久大页**的数量**超过指定的最大数量**, 那么**逐页**把**永久大页**转换为**临时大页**. 同样, 仅仅是调整了属性值.

## 7.3. 写超发 nr_overcommit_hugepages

>设置默认大小的大页池的**属性值**

其实只是设置了 `default_hstate->nr_overcommit_huge_pages` **属性值**, 并**不会预分配**

```cpp
// hugetlb_sysctl_handler() ->  hugetlb_overcommit_handler() ->  proc_hugetlb_doulongvec_minmax() ->  proc_doulongvec_minmax() ->  do_proc_doulongvec_minmax()
// mm/hugetlb.c
#ifdef CONFIG_SYSCTL
int hugetlb_overcommit_handler(struct ctl_table *table, int write,
		void *buffer, size_t *length, loff_t *ppos)
{
	struct hstate *h = &default_hstate;
	unsigned long tmp;
	int ret;

	if (!hugepages_supported())
		return -EOPNOTSUPP;

	tmp = h->nr_overcommit_huge_pages;

	if (write && hstate_is_gigantic(h))
		return -EINVAL;

	ret = proc_hugetlb_doulongvec_minmax(table, write, buffer, length, ppos,
					     &tmp);
	if (ret)
		goto out;

	if (write) {
		spin_lock_irq(&hugetlb_lock);
		h->nr_overcommit_huge_pages = tmp;
		spin_unlock_irq(&hugetlb_lock);
	}
out:
	return ret;
}
#endif /* CONFIG_SYSCTL */
```

## 7.4. 挂载hugetlbfs文件系统

> 关联大页池 hstate: super_block -> hugetlbfs_sb_info -> hugepage_subpool | hstate

hugetlbfs 文件系统在初始化的时候, 调用函数 `register_filesystem` 以注册 hugetlbfs 文件系统, hugetlbfs 文件系统的结构体如下:

```cpp
// fs/hugetlbfs/inode.c
static struct file_system_type hugetlbfs_fs_type = {
	.name          = "hugetlbfs",
	.mount         = hugetlbfs_mount,
	.kill_sb       = kill_litter_super,
};
```

挂载 hugetlbfs 文件系统的时候, 挂载函数调用 hugetlbfs 文件系统的挂载函数 `hugetlbfs_mount`, 创建**超级块**和**根目录**, 把**文件系统**和**大页池**关联起来.

如图3.66所示, **超级块** `super_block` 中:

* 成员 `s_fs_info` 指向 hugetblfs 文件系统的**私有信息**；
* 成员 `s_blocksize` 是**块长度**, 被设置为**大页的长度**.

图3.66 hugetlbfs文件系统关联大页池

![2022-02-22-10-51-48.png](./images/2022-02-22-10-51-48.png)

结构体 `hugetlbfs_sb_info` 描述 hugetblfs 文件系统的**私有信息**.

1) 成员 `max_inode` 是允许的**索引节点最大数量**.

2) 成员 `free_inodes` 是**空闲的索引节点数量**.

3) 成员 `hstate` 指向**关联的大页池**.

4) 如果指定了**最大大页数量**或**最小大页数量**, 那么为大页池创建一个**子池**, 成员 spool 指向**子池**.

结构体 `hugepage_subpool` 描述**子池的信息**.

1) 成员 `max_hpages` 是允许的最大大页数量.

2) `used_hpages` 是已使用的大页数量, 包括分配的和预留的.

3) 成员 `hstate` 指向大页池.

4) 成员 `min_hpages` 是最小大页数量.

5) 成员 `rsv_hpages` 是子池向大页池申请预留的大页的数量.

## 7.5. 创建文件

> 创建 inode; 设置 fops 为 `hugetlbfs_file_operations`, 核心是 `hugetlbfs_file_mmap()`

调用系统调用 `open()`, 在 hugetlbfs 文件系统的一个目录下创建一个文件的时候, 系统调用 open 最终调用函数 `hugetlbfs_create()` 为文件**分配索引节点**(结构体 `inode`) 并且初始化.

**索引节点**的成员 `i_fop` 指向 hugetlbfs 文件系统特有的**文件操作集合** `hugetlbfs_file_operations`, 这个文件操作集合的成员 **mmap** 方法是函数 `hugetlbfs_file_mmap()`, 这个函数在**创建内存映射**的时候很关键.

## 7.6. 创建内存映射

> 设置 VM_HUGETLB 标志位; 从 hstate 申请预留

在 hugetlbfs 文件系统中打开文件, 然后**基于这个文件创建内存映射**时, 系统调用 mmap 将会调用函数 `hugetlbfs_file_mmap()`.

```cpp
// fs/hugetlbfs/inode.c
const struct file_operations hugetlbfs_file_operations = {
	.read_iter		= hugetlbfs_read_iter,
	.mmap			= hugetlbfs_file_mmap,
	.fsync			= noop_fsync,
	.get_unmapped_area	= hugetlb_get_unmapped_area,
	.llseek			= default_llseek,
	.fallocate		= hugetlbfs_fallocate,
};
```

> 可以看出 hugetlbfs 文件系统中**文件**只支持 read/mmap/ummap 等操作, **不支持write**

函数 `hugetlbfs_file_mmap()` 的主要功能如下.

1) 设置虚拟内存区域标志: **标准大页标志** `VM_HUGETLB`; **不允许扩展标志** `VM_DONTEXPAND`.

> 为虚拟内存分区对象设置 `VM_HUGETLB` **标志位**的作用是: 当对**虚拟内存分区**进行**物理内存映射**时, 会进行特殊的处理, 下面将会介绍.

3) 虚拟内存区域的成员 `vm_ops` 指向大页特有的**虚拟内存操作集合** `hugetlb_vm_ops`.

4) 检查文件的偏移是不是大页长度的整数倍.

5) 调用函数 `hugetlb_reserve_pages()`, 向大页池**申请预留大页**. 并**不是真正申请**, **只是预留**.

### 7.6.1. 申请预留大页

> 向大页池**申请预留大页**

函数 `hugetlb_reserve_pages()` 的**主要功能**如下.

1. 如果设置标志位 `VM_NORESERVE` 指定**不需要预留大页**, 直接返回.

2. 如果是**共享映射**, 那么使用**文件的索引节点**的**预留图**(结构体 `resv_map`), 如图3.67所示, 在预留图中查看**从文件的起始偏移**到**结束偏移**有**哪些部分**以前**没有预留**, 计算**需要预留的大页的数量 N**.

图3.67 共享映射的预留图:

![2022-02-22-10-53-33.png](./images/2022-02-22-10-53-33.png)

3. 如果是**私有映射**, 那么**创建预留图**, 虚拟内存区域的成员 `vm_private_data` 指向**预留图**, 并且设置标志 `HPAGE_RESV_OWNER` 指明**该虚拟内存区域拥有这个预留**, 如下图所示. 计算**需要预留的大页的数量** N =(文件的结束偏移 − 起始偏移), 偏移的单位是**大页长度**.

图3.68 私有映射的预留图:

![2022-02-22-10-54-16.png](./images/2022-02-22-10-54-16.png)

**虚拟内存区域**的成员 `vm_private_data` 的**最低两位**用来存储**标志位**.

* 标志位 `HPAGE_RESV_OWNER`, 值为1, 指明当前进程是**预留的拥有者**.
* 标志位 `HPAGE_RESV_UNMAPPED`, 值为2. 对于**私有映射**, 如果创建映射的进程在执行写时复制时分配大页失败, 那么删除所有子进程的映射, 设置该标志, 让**子进程**在发生**页错误异常**时被杀死.

4. 如果文件系统**创建**了**大页子池**, 计算子池需要向大页池申请预留的大页的数量, 否则需要向大页池申请预留的大页的数量是 N.

如果子池以前申请预留的大页数量大于或等于N, 那么子池不需要向大页池申请预留.

如果子池以前申请预留的大页数量小于N, 那么子池需要向大页池申请预留的数量等于(N − 子池以前申请预留的大页数量) .

5. 向大页池**申请预留**指定数量的大页.

6. 如果是**共享映射**, 那么在**预留图的区域链表**中增加 1 个 `file_region` 实例, 记录预留区域.

## 7.7. 分配和映射到大页

> 缺页异常: 从 hstate->freelist 真正申请大页; 设置页中间目录项;

使用 mmap 函数映射到 hugetlbfs 文件后, 会返回一个**虚拟内存地址**. 当对这个虚拟内存地址进行**访问**(读写)时, 由于此虚拟内存地址还**没有与物理内存地址进行映射**, 将会触发**缺页异常**, 内核会调用 `do_page_fault` 函数对 缺页异常 进行修复.

我们来看看整个流程, 如下图所示:

![2022-03-08-10-10-27.png](./images/2022-03-08-10-10-27.png)

所以, 最终会调用 `do_page_fault` 函数对 缺页异常 进行修复操作, 我们来看看 `do_page_fault` 做了什么工作, 实现如下:

```cpp
asmlinkage void
__kprobes do_page_fault(struct pt_regs *regs, unsigned long error_code)
{
    ...
    struct mm_struct *mm;
    struct vm_area_struct *vma;
    unsigned long address;
    ...

    mm = tsk->mm;         // 1. 获取当前进程对应的内存管理对象
    address = read_cr2(); // 2. 获取触发缺页异常的虚拟内存地址

    ...
    vma = find_vma(mm, address); // 3. 通过虚拟内存地址获取对应的虚拟内存分区对象
    ...

    // 4. 调用 handle_mm_fault 函数对异常进行修复
    fault = handle_mm_fault(mm, vma, address, write);
    ...

    return;
}
```

上面代码对 `do_page_fault` 进行了精简, 精简后主要完成4个工作:

* 获取当前进程对应的内存管理对象.
* 调用 `read_cr2` 获取触发缺页异常的**虚拟内存地址**.
* 通过触发 缺页异常 的**虚拟内存地址**获取对应的**虚拟内存分区对象**.
* 调用 `handle_mm_fault` 函数对 缺页异常 进行**修复**.

看 `handle_mm_fault` 函数的实现, 代码如下:

```cpp
int handle_mm_fault(struct mm_struct *mm, struct vm_area_struct *vma,
                    unsigned long address, int write_access)
{
    ...
    // 这里就检查了 VM_HUGETLB 标志位
    if (unlikely(is_vm_hugetlb_page(vma))) // 虚拟内存分区是否需要使用 HugePages
        return hugetlb_fault(mm, vma, address, write_access); // 如果使用 HugePages, 就调用 hugetlb_fault 进行处理
    ...
}
```

`hugetlb_fault` 函数主要**对进程的页表进行填充**, 所以我们先来回顾一下 HugePages 对应的页表结构.

64 位 Linux 操作系统四级页表示意图:

![2022-03-29-16-55-13.png](./images/2022-03-29-16-55-13.png)

64 位 Linux 操作系统三级页表示意图:

![2022-03-29-16-55-50.png](./images/2022-03-29-16-55-50.png)

![2022-03-08-10-25-17.png](./images/2022-03-08-10-25-17.png)

以 4KB 为基本分页单位的 64 位 Linux 操作系统来采用**四级页表**管理虚实映射. 如上图 1. **每个页表项**占据 **64 位**(`8 Bytes`), 因此**每个**作为**页表**的**物理页面**可以存放 **512** 个**页表项**, 从而**最末级页表**(`Page table`)所映射的**物理内存大小**为 `512 * 4KB = 2MB`, 依此类推, 在**上一级页表**(`PMD`)中, **每一个 PMD 表项**可映射 **2MB** 的**物理内存**. 

当采用 **2MB** 作为**分页**的**基本单位**时, 内核中则设置了三级页表, 如图 2. 在三级页表中, **最末一级页表**为 `PMD` 表, 同样地, **每一个 PMD 表项**指出了一个 **2MB** 的**大页面**, 也即虚拟地址的低 21 位作为大页面的页内偏移, 而高位则作为大页面的页面编号(pfn). 为了能让 MMU 正确地进行虚实地址转换, 必须告知 MMU 哪个页表项映射的是 4KB 的物理页面, 哪个页表项映射的是 2MB 的大页面, 这是通过页表项中的标志位 _PAGE_PSE 来区分的, 这一般是通过内联函数 `pte_mkhuge()` 设置的.




从上图可以看出, 使用 HugePages 后, **页中间目录项** 直接指向**物理内存页**. 所以, `hugetlb_fault` 函数主要就是对 **页中间目录项** 进行填充. 实现如下:

```cpp
int hugetlb_fault(struct mm_struct *mm, struct vm_area_struct *vma,
                  unsigned long address, int write_access)
{
    pte_t *ptep;
    pte_t entry;
    int ret;

    ptep = huge_pte_alloc(mm, address); // 1. 找到虚拟内存地址对应的页中间目录项
    ...
    entry = *ptep;

    if (pte_none(entry)) { // 如果页中间目录项还没进行映射
        // 2. 那么调用 hugetlb_no_page 函数进行映射操作
        ret = hugetlb_no_page(mm, vma, address, ptep, write_access);
        ...
        return ret;
    }
    ...
}
```

对 `hugetlb_fault` 函数进行精简后, 主要完成两个工作:

* 通过触发 缺页异常 的**虚拟内存地址**找到其对应的 **页中间目录项**.
* 发现**页中间目录项**是**空表项**, 调用 `hugetlb_no_page` 函数对 **页中间目录项** 进行**映射操作**.

我们再来看看 `hugetlb_no_page` 函数怎么对 **页中间目录项** 进行填充:

```cpp
static int
hugetlb_no_page(struct mm_struct *mm, struct vm_area_struct *vma,
                unsigned long address, pte_t *ptep, int write_access)
{
    ...
    page = find_lock_page(mapping, idx);
    if (!page) {
        ...
        // 1. 从空闲大内存页链表 hugepage_freelists 中申请一个大内存页
        page = alloc_huge_page(vma, address);
        ...
    }
    ...
    // 2. 通过大内存页的物理地址生成页中间目录项的值
    new_pte = make_huge_pte(vma, page, ((vma->vm_flags & VM_WRITE)
                                            && (vma->vm_flags & VM_SHARED)));

    // 3. 设置页中间目录项的值为上面生成的值
    set_huge_pte_at(mm, address, ptep, new_pte);
    ...
    return ret;
}
```

通过对 `hugetlb_no_page` 函数进行精简后, 主要完成3个工作:

* 在文件的**页缓存**中根据**文件的页偏移**查找页.
* 调用 `alloc_huge_page` 函数从**空闲大内存页链表** `hugepage_freelists` 中**申请一个大内存页**. 如果是**共享映射**, 那么把大页加入文件的**页缓存**, 以便和其他进程共享页.
* 通过**大内存页**的**物理地址**生成**页中间目录项的值**.
* 设置页中间目录项的值为上面生成的值.
* 如果第一步在**页缓存**中找到页, 映射是**私有的**, 并且执行写操作, 那么**执行写时复制**.

函数 `alloc_huge_page` 的执行过程如下.

1) 检查**预留图**, 确定进程是否预留过要分配的大页.

2) 如果进程没有预留大页, 检查分配是否超过子池的限制.

3) 从大页池中目标内存节点的空闲链表中分配永久大页.

4) 如果分配永久大页失败, 那么尝试从页分配器分配临时大页.

至此, HugePages 的映射过程已经完成.

还有个问题, 就是 **CPU** 怎么知道 页中间表项 指向的是 页表 还是 大内存页 呢？

这是因为 **页中间表项** 有个 **PSE** 的标志位, 如果将其设置为1, 那么就表明其指向 大内存页, 否则就指向 页表.

## 7.8. 写时复制

> 分配大页; 复制数据; 修改页表项

假设进程1创建了**私有**的**大页映射**, 然后进程1分叉生成进程2和进程3. 其中一个进程试图写大页的时候, 触发页错误异常, 大页的页错误处理函数 `hugetlb_fault` 调用函数 `hugetlb_cow` 以执行写时复制.

函数 `hugetlb_cow` 的执行过程如下.

1) 如果**只有一个虚拟页**映射到该**物理页**, 并且是**匿名映射**, 那么不需要复制, 直接修改页表项设置**可写**.

2) 分配大页.

3) 处理分配大页失败的情况.

如果触发页错误异常的进程是创建**私有映射**的**进程**, 那么删除所有子进程的映射, 为子进程的虚拟内存区域的成员 `vm_private_data` 设置标志 `HPAGE_RESV_UNMAPPED`, 让子进程在发生页错误异常的时候被杀死. 如果触发页错误异常的进程不是创建私有映射的进程, 返回错误.

4) 把旧页的数据复制到新页.

5) 修改页表项, 映射到新页, 并且设置可写.

# 性能对比

构建测试用例: 分别在 2MB HugePagemmap() 和 4K 页面内存上映射 256M 内存, 然后每隔4KB写一个字节, 再读出进行验证. 最后 munmap() 解除映射.

如此确保每个 256M 空间每个页面都会被申请到, 其中 HugePage 内存每隔 2MB 才会发生缺页异常; 4K页面每次都会发生缺页异常, 总量是2MB的512倍.

```cpp
#include <stdlib.h>
#include <stdio.h>
#include <unistd.h>
#include <sys/mman.h>
#include <fcntl.h>
#include <time.h>

#define PAGE_4K (0x1000)
#define LENGTH (256UL*1024*1024)
#define PROTECTION (PROT_READ | PROT_WRITE)

#ifndef MAP_HUGETLB
#define MAP_HUGETLB 0x40000 /* arch specific */
#endif

#define ADDR (void *)(0x0UL)
#define FLAGS_HP (MAP_PRIVATE | MAP_ANONYMOUS | MAP_HUGETLB)
#define FLAGS (MAP_PRIVATE | MAP_ANONYMOUS)

static void write_bytes(char *addr)
{
    unsigned long i;

    for (i = 0; i < LENGTH/PAGE_4K; i++)
        *(addr + PAGE_4K*i) = (char)i;
}

static int read_bytes(char *addr)
{
    unsigned long i;

    for (i = 0; i < LENGTH/PAGE_4K; i++)
        if (*(addr + PAGE_4K*i) != (char)i) {
            printf("Mismatch at %lu\n", i);
            return 1;
        }
    return 0;
}

long int mmap_perf_test(int flags)
{
    void *addr;
    int ret;
    struct timespec time_start, time_end;
    long int duration = 0;

    clock_gettime(CLOCK_REALTIME, &time_start);
    addr = mmap(ADDR, LENGTH, PROTECTION, flags, 0, 0);
    if (addr == MAP_FAILED) {
        perror("mmap");
        exit(1);
    }

    write_bytes(addr);
    ret = read_bytes(addr);
    /* munmap() length of MAP_HUGETLB memory must be hugepage aligned */
    if (munmap(addr, LENGTH)) {
        perror("munmap");
        exit(1);
    }
    clock_gettime(CLOCK_REALTIME, &time_end);
    duration = (time_end.tv_sec - time_start.tv_sec)*1000000000 + (time_end.tv_nsec - time_start.tv_nsec);
    return duration;
}

int main(int argc, char** argv)
{
    long int ret_hp = 0, ret = 0;
    float percentage = 0.0;

    if(argc != 2)
        return -1;

    int count=atoi(argv[1]);

    for(int i = 0; i < count; i++) {
        ret_hp = mmap_perf_test(FLAGS_HP);
        ret = mmap_perf_test(FLAGS);
        percentage = (float)ret_hp/(float)ret*100.0;
        printf("%d, %ld, %ld, %f\n", i+1, ret_hp, ret, percentage);
    }

    return ret;
}
```

分别进行 1 次和连续 50 次对比如下:

```
al@al-B250-HD3:~/hugepage$ ./cmp_hugepage 1
1, 19186196, 81868224, 23.435461
al@al-B250-HD3:~/hugepage$ ./cmp_hugepage 1
1, 45560431, 83335020, 54.671406
al@al-B250-HD3:~/hugepage$ ./cmp_hugepage 1
1, 17648359, 82351069, 21.430639
al@al-B250-HD3:~/hugepage$ ./cmp_hugepage 1
1, 43837351, 83531347, 52.480125
al@al-B250-HD3:~/hugepage$ ./cmp_hugepage 1
1, 45677763, 83840517, 54.481728
```

可以看出只执行1次, HugePage上耗时不太稳定. 耗时可能是20%或50%左右. (原因细节未继续跟进)

```
al@al-B250-HD3:~/hugepage$ ./cmp_hugepage 50
1, 18256274, 81718710, 22.340384
2, 15377806, 82897401, 18.550407
3, 14953090, 81918232, 18.253677
...
48, 15206444, 83194584, 18.278166
49, 15137777, 85130700, 17.781807
50, 15088148, 83927648, 17.977566
```

当执行50的时候, 第1次可能是20%或50%, 但是后面基本在20%左右. 

所以说在本Case下, 最坏情况HugePage领先4K 50%, 最好领先80%左右. 

通过读取trace event /sys/kernel/debug/tracing/events/exceptionspage_fault_user, 可以看出缺页异常次数对比. 

当执行256M 4KB页面, 内核 do_page_fault() 次数为:

```
cat /sys/kernel/debug/tracing/trace | grep 0x4006cd | wc -l
65536
```

当使用HugePage后, 内核do_page_fault()次数为：

```
cat /sys/kernel/debug/tracing/trace | grep 0x4006cd | wc -l
128
```

其中 `0x4006cd` 是写内存的地方:

```cpp
static void write_bytes(char *addr)
{
  4006ac:    55                       push   %rbp
  4006ad:    48 89 e5                 mov    %rsp,%rbp
  4006b0:    48 89 7d e8              mov    %rdi,-0x18(%rbp)
    unsigned long i;

    for (i = 0; i < LENGTH; i++)
  4006b4:    48 c7 45 f8 00 00 00     movq   $0x0,-0x8(%rbp)
  4006bb:    00 
  4006bc:    eb 16                    jmp    4006d4 <write_bytes+0x28>
        *(addr + i) = (char)i;
  4006be:    48 8b 55 e8              mov    -0x18(%rbp),%rdx
  4006c2:    48 8b 45 f8              mov    -0x8(%rbp),%rax
  4006c6:    48 01 d0                 add    %rdx,%rax
  4006c9:    48 8b 55 f8              mov    -0x8(%rbp),%rdx
  4006cd:    88 10                    mov    %dl,(%rax)
  ......
}
```

# 8. reference

`Documentation/admin-guide/mm/hugetlbpage.rst`

examples:

* `map_hugetlb`:

  * ``map_hugetlb``: tools/testing/selftests/vm/map_hugetlb.c

  * ``hugepage-shm``: tools/testing/selftests/vm/hugepage-shm.c

  * ``hugepage-mmap``: tools/testing/selftests/vm/hugepage-mmap.c

* `libhugetlbfs` library: https://github.com/libhugetlbfs/libhugetlbfs

* 一文读懂 HugePages(大内存页)的原理: https://mp.weixin.qq.com/s?__biz=MzA3NzYzODg1OA==&mid=2648464691&idx=2&sn=5a55c7171e591f0041779925957cdfda


《Linux hugepage使用与实现》: http://blog.chinaunix.net/uid-28541347-id-5783934.html

《Hugepage介绍以及实践》: 

《hugepage总结》

《hugepage的优势与使用》

《HugeTLB Pages》

《Linux Hugetlbfs内核源码简析-----(一)Hugetlbfs初始化》

《Linux Hugetlbfs内核源码简析-----(二)Hugetlbfs挂载》 
