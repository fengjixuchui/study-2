
NVMe 本身是一个块设备, 因此NVMe的驱动也是遵循块设备的驱动架构. 本文通过两部分介绍NVMe的驱动程序, 一部分是操作系统如何创建NVMe块设备, 另外一部分是分析一下NVMe的主要流程, 包括读写流程和管理流程等.

# 创建NVMe块设备

对于 Linux 的**块设备**来说, 其主要的是通过 `device_add_disk` 或者 `add_disk` 函数(后者是对前者的简单包装)来向操作系统**添加一个设备实例**. 其基本原理就是通过调用该函数, 就会创建在 `/dev` 目录下看到的类似 **sdX** 的块设备.

NVMe 本身也是块设备, 自然也不会跳出这个大框架. 

首先从**硬件层面**上, **任何设备**必须通过**某个总线与CPU相连接**, NVMe 则正是通过 **PCIe 总线与CPU相连**. 

![2023-02-09-21-26-38.png](./images/2023-02-09-21-26-38.png)

当然, 目前NVMe除了可以通过PCIe总线与CPU相连外, 还可以通过**其它通道**连接, 比如**FC**或者**IB**. 后者则是一种将 NVMe 设备从计算节点独立出来的方式, 也就是此时 **NVMe** 就**不再是一个卡设备**, 而是一个**独立机箱的设备**. 无论何种方式相连接, 其本质是一样等. 

然后是**操作系统**软件层面. 硬件的连通性是基础, 当硬件已经连通后, 就可以在 Linux 内核层面**发现设备**, 并**进行初始化**了. 软件层面的初始化有**两种情况**:

* 一种是计算机**启动的时候**, 操作系统会**扫描总线上的设备**, 并完成**初始化**；
* 另外一种情况是设备在**系统启动后连接的**, 此时需要**手动触发扫描**的过程. 

与其它块设备类似, NVMe 设备初始化完成后会在 `/dev` 目录下出现一个文件. NVMe 设备会出现一个形如 nvmeXnY 的设备文件. 如图所示, 红色方框中的为一个NVMe块设备. 

![2023-02-09-21-38-27.png](./images/2023-02-09-21-38-27.png)

上一篇内容中, 在驱动加载过程中(系统启动), 会**调用总线**的 `probe` 函数, 如果总线没有 `probe` 函数, 则调用**设备驱动**的 `probe` 函数

```cpp
// drivers/pci/pci-driver.c
// pci 总线的 ops
struct bus_type pci_bus_type = {
    .probe| |       = pci_device_probe,
}

// drivers/nvme/host/pci.c
// nvme 驱动的 ops
static struct pci_driver nvme_driver = {
    .probe| |       = nvme_probe,
}
```

那问题来了，PCI Bus 是怎么将 nvme driver 匹配到对应的 NVMe 设备的呢？

系统启动时，BIOS 会枚举整个 PCI Bus, 之后将扫描到的设备通过 ACPI tables 传给操作系统。当操作系统加载时，PCI Bus 驱动则会根据此信息读取各个 PCI 设备的 Header Config 空间，从 **class code 寄存器**获得一个特征值。class code 就是 PCI bus 用来选择哪个驱动加载设备的唯一根据。`PCI Code and ID Assignment Specification` 定义 NVMe 设备的 Class code=0x010802h。

在 `struct pci_driver nvme_driver` 中就定义了 `id_table = nvme_id_table`:

```cpp
// include/linux/mod_devicetable.h
struct pci_device_id {
	__u32 vendor, device;		/* Vendor and device ID or PCI_ANY_ID*/
	__u32 subvendor, subdevice;	/* Subsystem ID's or PCI_ANY_ID */
	__u32 class, class_mask;	/* (class,subclass,prog-if) triplet */
	kernel_ulong_t driver_data;	/* Data private to the driver */
	__u32 override_only;
};

// include/linux/pci.h
// class 和 class_mask 都是 0
#define PCI_VDEVICE(vend, dev) \
	.vendor = PCI_VENDOR_ID_##vend, .device = (dev), \
	.subvendor = PCI_ANY_ID, .subdevice = PCI_ANY_ID, 0, 0

#define PCI_DEVICE_CLASS(dev_class,dev_class_mask) \
	.class = (dev_class), .class_mask = (dev_class_mask), \
	.vendor = PCI_ANY_ID, .device = PCI_ANY_ID, \
	.subvendor = PCI_ANY_ID, .subdevice = PCI_ANY_ID

// include/linux/pci_ids.h
// class code
#define PCI_CLASS_STORAGE_EXPRESS      0x010802

// drivers/nvme/host/pci.c
static const struct pci_device_id nvme_id_table[] = {
    // 这个并没有 class code
	{ PCI_VDEVICE(INTEL, 0x0953),	/* Intel 750/P3500/P3600/P3700 */
		.driver_data = NVME_QUIRK_STRIPE_SIZE |
				NVME_QUIRK_DEALLOCATE_ZEROES, },
    ......
	{ PCI_DEVICE_CLASS(PCI_CLASS_STORAGE_EXPRESS, 0xffffff) },
	{ 0, }
};
MODULE_DEVICE_TABLE(pci, nvme_id_table);
```

`pci_match_device(struct pci_driver *drv, struct pci_dev *dev)` 函数提供给一个 driver, 用来判断一个 PCI 设备是否被它支持.

```cpp
// drivers/pci/pci-driver.c
static const struct pci_device_id *pci_match_device(struct pci_driver *drv,
						    struct pci_dev *dev)
{
	struct pci_dynid *dynid;
	const struct pci_device_id *found_id = NULL, *ids;

	/* When driver_override is set, only bind to the matching driver */
    // 如果该设备只绑定到指定驱动, 并且, 不是指定的驱动
    // 则返回 NULL
	if (dev->driver_override && strcmp(dev->driver_override, drv->name))
		return NULL;

	/* Look at the dynamic ids first, before the static ones */
	spin_lock(&drv->dynids.lock);
    // 遍历驱动中动态 id(dynids),找到则返回id;动态id就是sysfs中的new_id文件内容
	list_for_each_entry(dynid, &drv->dynids.list, node) {
		if (pci_match_one_device(&dynid->id, dev)) {
			found_id = &dynid->id;
			break;
		}
	}
	spin_unlock(&drv->dynids.lock);

	if (found_id)
		return found_id;
    // 遍历静态id(NVMe驱动中的id_table表)
	for (ids = drv->id_table; (found_id = pci_match_id(ids, dev));
	     ids = found_id + 1) {
		/*
		 * The match table is split based on driver_override.
		 * In case override_only was set, enforce driver_override
		 * matching.
		 */
		if (found_id->override_only) {
			if (dev->driver_override)
				return found_id;
		} else {
			return found_id;
		}
	}

	/* driver_override will always match, send a dummy id */
    // 设备的driver_override已经设置, 并且, 设备指定的驱动等于传入的参数
    // 但是在驱动的支持列表中又没找到
    // 仍然返回一个 pci_device_id_any
	if (dev->driver_override)
		return &pci_device_id_any;
	return NULL;
}

const struct pci_device_id *pci_match_id(const struct pci_device_id *ids,
					 struct pci_dev *dev)
{
	if (ids) {
		while (ids->vendor || ids->subvendor || ids->class_mask) {
			if (pci_match_one_device(ids, dev))
				return ids;
			ids++;
		}
	}
	return NULL;
}
```

可以看到 `pci_match_one_device` 是关键

```cpp
// 
static inline const struct pci_device_id *
pci_match_one_device(const struct pci_device_id *id, const struct pci_dev *dev)
{
    // vendor ID任意 或者 相等, 并且
    // device ID任意 或者 相等, 并且
    // device ID任意 或者 相等, 并且
	if ((id->vendor == PCI_ANY_ID || id->vendor == dev->vendor) &&
	    (id->device == PCI_ANY_ID || id->device == dev->device) &&
	    (id->subvendor == PCI_ANY_ID || id->subvendor == dev->subsystem_vendor) &&
	    (id->subdevice == PCI_ANY_ID || id->subdevice == dev->subsystem_device) &&
	    !((id->class ^ dev->class) & id->class_mask))
		return id;
	return NULL;
}
```


这里注意的是 `pci_device_id` 中 **class** 和 **classmask** 合计 **32 位**, 实际有效的是 **class** 的 **16 位**, 另外 16 位是为了掩盖 `pci_device` 中 32 位 class 中无效的 16 位.




无论是**系统启动**也好, 还是**手动触发扫描**也好, NVMe 发现设备的核心流程是一样的. 

![2023-02-09-21-34-56.png](./images/2023-02-09-21-34-56.png)

## 核心实现分析

在上面初始化流程中需要重点关注的是 `nvme_alloc_ns` 函数的流程. 该函数完成了块设备创建、基本信息填充和块设备注册到内核等工作. 这部分片段完成了函数指针的初始化、命令队列初始化和设备名称的初始化等工作. 具体关于 `nvme_alloc_ns` 函数源代码的逻辑请自行阅读代码, 本文不再赘述. 

```cpp

```

在整个初始化流程中比较关键的是对**请求队列**(`request_queue`)中请求处理函数指针(`make_request_fn`)的初始化及**多队列函数集**(`mq_ops`)的初始化. 因为, 这里的函数正是 NVMe 区别于 SCSI 等类型设备数据处理流程的地方. 



# NVMe设备的IO流程

为了便于理解NVMe的处理流程, 我们给出了传统SCSI及NVMe数据处理的对比流程. 如图5所示, 整个流程是从通用块层的接口(submit_bio)开始的, 这个函数大家都非常清楚了. 

![2023-02-09-21-45-27.png](./images/2023-02-09-21-45-27.png)

对于NVMe设备来说, 在初始化的时候初始化函数指针make_request_fn为nvme_queue_rq, 该函数就是NVMe驱动程序的请求处理接口. 该函数最终会将请求写入NVMe中的SQ队列当中, 并通知控制器处理请求. 



当 nvme.ko 已经加载完了(注册了nvme driver), 这时候如果有 nvme 盘插入 pcie 插槽, **pci** 会**自动识别**到, 并交给 nvme driver 去处理, 而 **nvme driver** 就是调用 `nvme_probe` 去处理这个新加入的设备.

