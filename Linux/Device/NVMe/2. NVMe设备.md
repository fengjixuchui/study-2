
NVMe 本身是一个块设备, 因此NVMe的驱动也是遵循块设备的驱动架构. 本文通过两部分介绍NVMe的驱动程序, 一部分是操作系统如何创建NVMe块设备, 另外一部分是分析一下NVMe的主要流程, 包括读写流程和管理流程等.

# 创建NVMe块设备

对于 Linux 的**块设备**来说, 其主要的是通过 `device_add_disk` 或者 `add_disk` 函数(后者是对前者的简单包装)来向操作系统**添加一个设备实例**. 其基本原理就是通过调用该函数, 就会创建在 `/dev` 目录下看到的类似 **sdX** 的块设备.

NVMe 本身也是块设备, 自然也不会跳出这个大框架. 

首先从**硬件层面**上, **任何设备**必须通过**某个总线与CPU相连接**, NVMe 则正是通过 **PCIe 总线与CPU相连**. 

![2023-02-09-21-26-38.png](./images/2023-02-09-21-26-38.png)

当然, 目前NVMe除了可以通过PCIe总线与CPU相连外, 还可以通过**其它通道**连接, 比如**FC**或者**IB**. 后者则是一种将 NVMe 设备从计算节点独立出来的方式, 也就是此时 **NVMe** 就**不再是一个卡设备**, 而是一个**独立机箱的设备**. 无论何种方式相连接, 其本质是一样等. 

然后是**操作系统**软件层面. 硬件的连通性是基础, 当硬件已经连通后, 就可以在 Linux 内核层面**发现设备**, 并**进行初始化**了. 软件层面的初始化有**两种情况**:

* 一种是计算机**启动的时候**, 操作系统会**扫描总线上的设备**, 并完成**初始化**；
* 另外一种情况是设备在**系统启动后连接的**, 此时需要**手动触发扫描**的过程. 

与其它块设备类似, NVMe 设备初始化完成后会在 `/dev` 目录下出现一个文件. NVMe 设备会出现一个形如 nvmeXnY 的设备文件. 如图所示, 红色方框中的为一个NVMe块设备. 

![2023-02-09-21-38-27.png](./images/2023-02-09-21-38-27.png)

## PCI Bus的识别

那问题来了, PCI Bus 是怎么将 nvme driver 匹配到对应的 NVMe 设备的呢？

系统启动时, BIOS 会枚举整个 PCI Bus, 之后将扫描到的设备通过 ACPI tables 传给操作系统。当操作系统加载时, PCI Bus 驱动则会根据此信息读取各个 PCI 设备的 Header Config 空间, 从 **class code 寄存器**获得一个特征值。class code 就是 PCI bus 用来选择哪个驱动加载设备的唯一根据。`PCI Code and ID Assignment Specification` 定义 NVMe 设备的 Class code=0x010802h。

在 `struct pci_driver nvme_driver` 中就定义了 `id_table = nvme_id_table`:

```cpp
// include/linux/mod_devicetable.h
struct pci_device_id {
	__u32 vendor, device;		/* Vendor and device ID or PCI_ANY_ID*/
	__u32 subvendor, subdevice;	/* Subsystem ID's or PCI_ANY_ID */
	__u32 class, class_mask;	/* (class,subclass,prog-if) triplet */
	kernel_ulong_t driver_data;	/* Data private to the driver */
	__u32 override_only;
};

// include/linux/pci.h
// class 和 class_mask 都是 0
#define PCI_VDEVICE(vend, dev) \
	.vendor = PCI_VENDOR_ID_##vend, .device = (dev), \
	.subvendor = PCI_ANY_ID, .subdevice = PCI_ANY_ID, 0, 0

#define PCI_DEVICE_CLASS(dev_class,dev_class_mask) \
	.class = (dev_class), .class_mask = (dev_class_mask), \
	.vendor = PCI_ANY_ID, .device = PCI_ANY_ID, \
	.subvendor = PCI_ANY_ID, .subdevice = PCI_ANY_ID

// include/linux/pci_ids.h
// class code
#define PCI_CLASS_STORAGE_EXPRESS      0x010802

// drivers/nvme/host/pci.c
static const struct pci_device_id nvme_id_table[] = {
    // 这个并没有 class code
	{ PCI_VDEVICE(INTEL, 0x0953),	/* Intel 750/P3500/P3600/P3700 */
		.driver_data = NVME_QUIRK_STRIPE_SIZE |
				NVME_QUIRK_DEALLOCATE_ZEROES, },
    ......
	{ PCI_DEVICE_CLASS(PCI_CLASS_STORAGE_EXPRESS, 0xffffff) },
	{ 0, }
};
MODULE_DEVICE_TABLE(pci, nvme_id_table);
```

`pci_match_device(struct pci_driver *drv, struct pci_dev *dev)` 函数提供给一个 driver, 用来判断一个 PCI 设备是否被它支持.

```cpp
// drivers/pci/pci-driver.c
static const struct pci_device_id *pci_match_device(struct pci_driver *drv,
						    struct pci_dev *dev)
{
	struct pci_dynid *dynid;
	const struct pci_device_id *found_id = NULL, *ids;

	/* When driver_override is set, only bind to the matching driver */
    // 如果该设备只绑定到指定驱动, 并且, 不是指定的驱动
    // 则返回 NULL
	if (dev->driver_override && strcmp(dev->driver_override, drv->name))
		return NULL;

	/* Look at the dynamic ids first, before the static ones */
	spin_lock(&drv->dynids.lock);
    // 遍历驱动中动态 id(dynids),找到则返回id;动态id就是sysfs中的new_id文件内容
	list_for_each_entry(dynid, &drv->dynids.list, node) {
		if (pci_match_one_device(&dynid->id, dev)) {
			found_id = &dynid->id;
			break;
		}
	}
	spin_unlock(&drv->dynids.lock);

	if (found_id)
		return found_id;
    // 遍历静态id(NVMe驱动中的id_table表)
	for (ids = drv->id_table; (found_id = pci_match_id(ids, dev));
	     ids = found_id + 1) {
		/*
		 * The match table is split based on driver_override.
		 * In case override_only was set, enforce driver_override
		 * matching.
		 */
		if (found_id->override_only) {
			if (dev->driver_override)
				return found_id;
		} else {
			return found_id;
		}
	}

	/* driver_override will always match, send a dummy id */
    // 设备的driver_override已经设置, 并且, 设备指定的驱动等于传入的参数
    // 但是在驱动的支持列表中又没找到
    // 仍然返回一个 pci_device_id_any
	if (dev->driver_override)
		return &pci_device_id_any;
	return NULL;
}

const struct pci_device_id *pci_match_id(const struct pci_device_id *ids,
					 struct pci_dev *dev)
{
	if (ids) {
		while (ids->vendor || ids->subvendor || ids->class_mask) {
			if (pci_match_one_device(ids, dev))
				return ids;
			ids++;
		}
	}
	return NULL;
}
```

可以看到 `pci_match_one_device` 是关键

```cpp
// 
static inline const struct pci_device_id *
pci_match_one_device(const struct pci_device_id *id, const struct pci_dev *dev)
{
    // vendor ID任意 或者 相等, 并且
    // device ID任意 或者 相等, 并且
    // subsystem vendor ID任意 或者 相等, 并且
    // subsystem device ID任意 或者 相等, 并且
    // class code 
	if ((id->vendor == PCI_ANY_ID || id->vendor == dev->vendor) &&
	    (id->device == PCI_ANY_ID || id->device == dev->device) &&
	    (id->subvendor == PCI_ANY_ID || id->subvendor == dev->subsystem_vendor) &&
	    (id->subdevice == PCI_ANY_ID || id->subdevice == dev->subsystem_device) &&
	    !((id->class ^ dev->class) & id->class_mask))
		return id;
	return NULL;
}
```

这里注意的是 `pci_device_id` 中 **class** 和 **classmask** 合计 **32 位**, 实际有效的是 **class** 的 **16 位**, 另外 16 位是为了掩盖 `pci_device` 中 32 位 class 中无效的 16 位.

`pci_register_driver()` 函数(nvme驱动加载)将 `nvme_driver` 注册到了 `PCI Bus`, 所以 PCI Bus 就明白了这个驱动是给 NVMe 设备(Class code=0x010802h)用的。

## NVMe驱动加载

> nvme驱动加载见上篇

在 NVMe 驱动加载中(系统启动), 如果总线支持自动 probe 设备(`/sys/bus/pci/drivers_autoprobe`, PCI 总线是支持的), 则会调用 `driver_attach(struct device_driver *drv)`函数

> `driver_attach(struct device_driver *drv)` 是一个全局方法, 也就是其他驱动都可以调用, 会遍历**该驱动**的所有设备, 并尝试将设备绑定到驱动上, 即实现设备的 probe.

`driver_attach()` 会调用 `bus_for_each_dev` **遍历该总线**(`drv->bus(pci_bus_type)`)上**所有设备**, 调用 `__driver_attach` **将设备绑定到该驱动上**.

```cpp
// drivers/base/dd.c
static int __driver_attach(struct device *dev, void *data)
{
	struct device_driver *drv = data;
	bool async = false;
	int ret;

	/*
	 * Lock device and try to bind to it. We drop the error
	 * here and always return 0, because we need to keep trying
	 * to bind to devices and some drivers will return an error
	 * simply if it didn't support the device.
	 *
	 * driver_probe_device() will spit a warning if there
	 * is an error.
	 */
    // 检查这个 device 是否匹配这个驱动
	ret = driver_match_device(drv, dev);
	if (ret == 0) {
		/* no match */
		return 0;
	} else if (ret == -EPROBE_DEFER) {
		dev_dbg(dev, "Device match requests probe deferral\n");
		dev->can_match = true;
		driver_deferred_probe_add(dev);
		/*
		 * Driver could not match with device, but may match with
		 * another device on the bus.
		 */
		return 0;
	} else if (ret < 0) {
		dev_dbg(dev, "Bus failed to match device: %d\n", ret);
		return ret;
	} /* ret > 0 means positive match */

    // 驱动支持异步probe
    // driver的probe_type属性或者内核启动参数driver_async_probe=drv_name1,drv_name2,...
	if (driver_allows_async_probing(drv)) {
		/*
		 * Instead of probing the device synchronously we will
		 * probe it asynchronously to allow for more parallelism.
		 *
		 * We only take the device lock here in order to guarantee
		 * that the dev->driver and async_driver fields are protected
		 */
		dev_dbg(dev, "probing driver %s asynchronously\n", drv->name);
		device_lock(dev);
		if (!dev->driver && !dev->p->async_driver) {
			get_device(dev);
			dev->p->async_driver = drv;
			async = true;
		}
		device_unlock(dev);
		if (async)
			async_schedule_dev(__driver_attach_async_helper, dev);
		return 0;
	}

	__device_driver_lock(dev, dev->parent);
    // 走到这里, 表明设备和驱动是匹配的
    // 将设备和驱动绑定, 也就是设备的 probe
	driver_probe_device(drv, dev);
	__device_driver_unlock(dev, dev->parent);

	return 0;
}
```

该函数其先调用 `driver_match_device`, 其调用**总线**(`struct bus_type pci_bus_type`)的 **match** 函数, **pci 总线**的就是函数 `pci_bus_match`. 用来检查一个 pci device 在 pci driver 中是否有一个匹配的 `struct pci_device_id`.

```cpp
// drivers/pci/pci-driver.c
static int pci_bus_match(struct device *dev, struct device_driver *drv)
{
    // 属于 pci 设备
    // struct device 转换成 struct pci_dev
	struct pci_dev *pci_dev = to_pci_dev(dev);
    // pci 设备自然使用 pci 驱动
	struct pci_driver *pci_drv;
	const struct pci_device_id *found_id;
    // 该设备已经和驱动匹配, 直接返回
	if (!pci_dev->match_driver)
		return 0;
    // 通用结构体 device_driver 转换成 pci_driver
	pci_drv = to_pci_driver(drv);
    // pci driver检查一个pci设备是否在它支持的列表中 或 在动态id列表
    // 返回 id
	found_id = pci_match_device(pci_drv, pci_dev);
	if (found_id)
		return 1;

	return 0;
}
```

如果**设备还没有和驱动匹配**, 则调用 `pci_match_device` 判断 `pci_dev` **设备**是否被 `pci_drv` **驱动**支持.

回到 `__driver_attach`, 如果执行 `driver_match_device` 出错, 并且返回错误是 `-EPROBE_DEFER`, 则需要调用 `driver_deferred_probe_add`, 来**将设备**通过 `dev->p->deferred_probe` 添加到 `deferred_probe_pending_list` **推迟 probe 的 pending 链表**中, 并正常返回, 表明**驱动不匹配这个设备**, 但是驱动**可能匹配总线上的其他设备**.

继续, 当**设备和驱动是匹配的**(没有匹配直接返回), 调用 `driver_probe_device`(调用该函数需要先获取设备锁), 该函数负责**将设备和驱动绑定**. 函数先**判断设备** `dev->kobj.state_in_sysfs` **是否已经有 sysfs 节点**(没有直接返回), 然后调用 `really_probe`(这里其实还会涉及 linux 电源管理的动作, 此处为了简化问题暂时不展开).

`really_probe` 函数中:

1. 设置 `dev->driver = drv`, **设置设备的驱动**;

2. 在 **driver** 的 sysfs 节点目录下添加一个 **device_name 链接**; 在 **device** 的 sysfs 节点目录下添加名为 "driver" 链接; 如果 `CONFIG_DEV_COREDUMP` **使能**并且驱动有 coredump 功能, 则在 device 的 sysfs 目录下创建一个 coredump 文件

```
# ll /sys/bus/pci/drivers/nvme/
lrwxrwxrwx 1 root root    0 Feb  8 01:56 0000:04:00.0 -> ../../../../devices/pci0000:00/0000:00:11.0/0000:04:00.0

# ll /sys/bus/pci/drivers/nvme/0000\:04\:00.0/
lrwxrwxrwx 1 root root     0 Feb  7 15:17 driver -> ../../../../bus/pci/drivers/nvme
```

3. **调用总线**的 `probe` 函数, 如果总线没有 `probe` 函数, 则调用**设备驱动**的 `probe` 函数

```cpp
// drivers/pci/pci-driver.c
// pci 总线的 ops
struct bus_type pci_bus_type = {
    .probe| |       = pci_device_probe,
}

// drivers/nvme/host/pci.c
// nvme 驱动的 ops
static struct pci_driver nvme_driver = {
    .probe| |       = nvme_probe,
}
```

4. 同时会调用 `driver_bound` 函数, 将**设备**也绑定到**驱动相关链表**中






## pci总线枚举

否则 NVMe 驱动加载只是能找到 PCI Bus 上面驱动与 NVMe 设备的对应关系。nvme_init 执行完毕后, nvme驱动就啥事不做了, 直到 pci 总线枚举出了这个 nvme 设备, 就开始调用 `nvme_probe()` 函数开始干活。

PCI bus 上就多了一个 `pci_driver nvme_driver`。当读到一个设备的 class code 是 010802h 时, 就会调用这个 nvme_driver 结构体的 probe 函数, 也就是说当设备和驱动匹配了之后, 驱动的 probe函数就会被调用, 来实现驱动的加载。

## 热插拔

当 nvme.ko 已经加载完了(注册了nvme driver), 这时候如果有 nvme 盘插入 pcie 插槽, **pci** 会**自动识别**到, 并交给 nvme driver 去处理, 而 **nvme driver** 就是调用 `nvme_probe` 去处理这个新加入的设备.

## 核心流程

无论是**系统启动**也好, 还是**手动触发扫描**也好, NVMe 发现设备的核心流程是一样的. 

![2023-02-09-21-34-56.png](./images/2023-02-09-21-34-56.png)

## nvme数据结构

在说 nvme_probe 之前, 先说一下 nvme 设备的数据结构, 首先, 内核使用一个 `nvme_dev` **结构体**来描述一个 nvme 设备, **每一个 nvme_dev** 都是**一个 PCI function**, nvme_dev 如下:

```cpp
// drivers/nvme/host/pci.c
struct nvme_dev {
    // 设备的queue,一个nvme设备至少有2个queue,一个admin queue,一个io queue
    // 实际情况一般都是一个admin queue,多个io queue,并且io queue会与CPU做绑定
    // 所以默认是 cpus 数目+1个
	struct nvme_queue *queues;
	struct blk_mq_tag_set tagset;
	struct blk_mq_tag_set admin_tagset;
    // ((void __iomem *)dev->bar) + 4096
	u32 __iomem *dbs;
    // 此nvme设备对应的struct device, 
	struct device *dev;
    // dma池,主要是以4k为大小的dma块,用于dma分配
	struct dma_pool *prp_page_pool;
    // 也是dma池,但是不是以4k为大小的,是小于4k时使用
	struct dma_pool *prp_small_pool;
    // 在线可以使用的queue数量,跟online cpu有关
	unsigned online_queues;
    // 最大的queue id
	unsigned max_qid;
    // unsigned short的数组,每个CPU占一个,主要用于存放CPU上绑定的io queue的qid,一个CPU绑定一个queues,一个queues绑定到1到多个CPU上
	unsigned io_queues[HCTX_MAX_TYPES];
	unsigned int num_vecs;
    // nvme queue支持的最大cmd数量,为((bar->cap) & 0xffff)或者1024的最小值
	u32 q_depth;
	int io_sqes;
    // 1 << (((bar->cap) >> 32) & 0xf),应该是每个io queue占用的bar空间
	u32 db_stride;
    // bar的映射地址，默认是映射8192，当io queue过多时，有可能会大于8192
	void __iomem *bar;
    // bar空间映射的大小
	unsigned long bar_mapped_size;
	struct work_struct remove_work;
	struct mutex shutdown_lock;
	bool subsystem;
	u64 cmb_size;
	bool cmb_use_sqes;
	u32 cmbsz;
	u32 cmbloc;
	struct nvme_ctrl ctrl;
	u32 last_ps;
	bool hmb;

	mempool_t *iod_mempool;

	/* shadow doorbell buffer support: */
	u32 *dbbuf_dbs;
	dma_addr_t dbbuf_dbs_dma_addr;
	u32 *dbbuf_eis;
	dma_addr_t dbbuf_eis_dma_addr;

	/* host memory buffer support: */
	u64 host_mem_size;
	u32 nr_host_mem_descs;
	dma_addr_t host_mem_descs_dma;
	struct nvme_host_mem_buf_desc *host_mem_descs;
	void **host_mem_desc_bufs;
    // 分配的 queue 数目, 默认是 cpus数目+1
	unsigned int nr_allocated_queues;
    // write_queue 的数量
	unsigned int nr_write_queues;
    // poll 模式的 queue 数量
	unsigned int nr_poll_queues;

	bool attrs_added;
};

// 控制 nvme 相关结体
struct nvme_ctrl {
	bool comp_seen;
	enum nvme_ctrl_state state;
	bool identified;
	spinlock_t lock;
	struct mutex scan_lock;
	const struct nvme_ctrl_ops *ops;
	struct request_queue *admin_q;
	struct request_queue *connect_q;
	struct request_queue *fabrics_q;
	struct device *dev;
	int instance;
	int numa_node;
	struct blk_mq_tag_set *tagset;
	struct blk_mq_tag_set *admin_tagset;
    // 其实就是块设备,一张nvme卡有可能会有多个块设备
	struct list_head namespaces;
	struct rw_semaphore namespaces_rwsem;
	struct device ctrl_device;
	struct device *device;	/* char device */
#ifdef CONFIG_NVME_HWMON
	struct device *hwmon_device;
#endif
	struct cdev cdev;
	struct work_struct reset_work;
	struct work_struct delete_work;
	wait_queue_head_t state_wq;

	struct nvme_subsystem *subsys;
	struct list_head subsys_entry;

	struct opal_dev *opal_dev;
    // 这个nvme设备的名字,为nvme%d
	char name[12];
	u16 cntlid;

    /*    初始化设置的值
     *    dev->ctrl_config = NVME_CC_ENABLE | NVME_CC_CSS_NVM;
     *    dev->ctrl_config |= (PAGE_SHIFT - 12) << NVME_CC_MPS_SHIFT;
     *    dev->ctrl_config |= NVME_CC_ARB_RR | NVME_CC_SHN_NONE;
     *    dev->ctrl_config |= NVME_CC_IOSQES | NVME_CC_IOCQES;
     */
	u32 ctrl_config;
	u16 mtfa;
	u32 queue_count;

	u64 cap;
	u32 max_hw_sectors;
	u32 max_segments;
	u32 max_integrity_segments;
	u32 max_discard_sectors;
	u32 max_discard_segments;
	u32 max_zeroes_sectors;
#ifdef CONFIG_BLK_DEV_ZONED
	u32 max_zone_append;
#endif
	u16 crdt[3];
	u16 oncs;
	u32 dmrsl;
	u16 oacs;
	u16 sqsize;
	u32 max_namespaces;
	atomic_t abort_limit;
	u8 vwc;
	u32 vs;
	u32 sgls;
	u16 kas;
	u8 npss;
	u8 apsta;
	u16 wctemp;
	u16 cctemp;
	u32 oaes;
	u32 aen_result;
	u32 ctratt;
	unsigned int shutdown_timeout;
	unsigned int kato;
	bool subsystem;
	unsigned long quirks;
	struct nvme_id_power_state psd[32];
	struct nvme_effects_log *effects;
	struct xarray cels;
	struct work_struct scan_work;
	struct work_struct async_event_work;
	struct delayed_work ka_work;
	struct delayed_work failfast_work;
	struct nvme_command ka_cmd;
	struct work_struct fw_act_work;
	unsigned long events;

	/* Power saving configuration */
	u64 ps_max_latency_us;
	bool apst_enabled;

	/* PCIe only: */
	u32 hmpre;
	u32 hmmin;
	u32 hmminds;
	u16 hmmaxd;

	/* Fabrics only */
	u32 ioccsz;
	u32 iorcsz;
	u16 icdoff;
	u16 maxcmd;
	int nr_reconnects;
	unsigned long flags;
	struct nvmf_ctrl_options *opts;

	struct page *discard_page;
	unsigned long discard_page_busy;

	struct nvme_fault_inject fault_inject;

	enum nvme_ctrl_type cntrltype;
	enum nvme_dctype dctype;
};





/* nvme设备描述符,描述一个nvme设备 */
struct nvme_dev {
    struct list_head node;
    /* 设备的queue,一个nvme设备至少有2个queue,一个admin queue,一个io queue,实际情况一般都是一个admin queue,多个io queue,并且io queue会与CPU做绑定 */
    struct nvme_queue __rcu **queues;
    /* unsigned short的数组, 每个CPU占一个,主要用于存放CPU上绑定的io queue的qid,一个CPU绑定一个queues,一个queues绑定到1到多个CPU上 */
    unsigned short __percpu *io_queue;
    /* ((void __iomem *)dev->bar) + 4096 */
    u32 __iomem *dbs;
    /* 此nvme设备对应的pci dev */
    struct pci_dev *pci_dev;
    /* dma池,主要是以4k为大小的dma块,用于dma分配 */
    struct dma_pool *prp_page_pool;
    /* 也是dma池,但是不是以4k为大小的,是小于4k时使用 */
    struct dma_pool *prp_small_pool;
    /* 实例的id,第一个加入的nvme dev,它的instance为0,第二个加入的nvme,instance为1,也用于做/dev/nvme%d的显示,%d实际就是instance的数值 */
    int instance;
    /* queue的数量, 等于admin queue + io queue */
    unsigned queue_count;
    /* 在线可以使用的queue数量,跟online cpu有关 */
    unsigned online_queues;
    /* 最大的queue id */
    unsigned max_qid;
    /* nvme queue支持的最大cmd数量, 为((bar->cap) & 0xffff)或者1024的最小值 */
    int q_depth;
    /* 1 << (((bar->cap) >> 32) & 0xf), 应该是每个io queue占用的bar空间 */
    u32 db_stride;
    /*    初始化设置的值
     *    dev->ctrl_config = NVME_CC_ENABLE | NVME_CC_CSS_NVM;
     *    dev->ctrl_config |= (PAGE_SHIFT - 12) << NVME_CC_MPS_SHIFT;
     *    dev->ctrl_config |= NVME_CC_ARB_RR | NVME_CC_SHN_NONE;
     *    dev->ctrl_config |= NVME_CC_IOSQES | NVME_CC_IOCQES;
     */
    u32 ctrl_config;
    /* msix中断所使用的entry,指针表示会使用多个msix中断,使用的中断的个数与io queue对等,多少个io queue就会申请多少个中断
     * 并且让每个io queue的中断尽量分到不同的CPU上运行
     */
    struct msix_entry *entry;
    /* bar的映射地址, 默认是映射8192, 当io queue过多时, 有可能会大于8192 */
    struct nvme_bar __iomem *bar;
    /* 其实就是块设备,一张nvme卡有可能会有多个块设备 */
    struct list_head namespaces;
    /* 对应的在/sys下的结构 */
    struct kref kref;
    /* 对应的字符设备,用于ioctl操作 */
    struct miscdevice miscdev;
    /* 2个work,暂时还不知道什么用 */
    work_func_t reset_workfn;
    struct work_struct reset_work;
    struct work_struct cpu_work;
    /* 这个nvme设备的名字,为nvme%d */
    char name[12];
    /* SN号 */
    char serial[20];
    char model[40];
    char firmware_rev[8];
    /* 这些值都是从nvme盘上获取 */
    u32 max_hw_sectors;
    u32 stripe_size;
    u16 oncs;
    u16 abort_limit;
    u8 vwc;
    u8 initialized;
};
```

在 `nvme_dev` 结构中, 最重要的数据就是 `struct nvme_queue`, 用来表示一个 nvme 的 queue, **每一个** `nvme_queue` 会申请**自己的中断**, 也有**自己的中断处理函数**, 也就是**每个 nvme_queue** 在**驱动层面**是**完全独立的**. nvme_queue 有两种, 一种是 admin queue,一种是 io queue, 这两种 queue 都用 `struct nvme_queue` 来描述,而这两种 queue 的区别如下:

* admin queue: 用于发送控制命令的queue, 所有非io命令都会通过此queue发送给nvme设备,一个nvme设备只有一个admin queue,在nvme_dev中,使用 queues[0] 来描述.
* io queue: 用于发送io命令的queue,所有io命令都是通过此queue发送给nvme设备,简单来说读/写操作都是通过io queue发送给nvme设备的,一个nvme设备有一个或多个io queue,每个io queue的中断会绑定到不同的一个或多个CPU上. 在nvme_dev中,使用queues[1~N]来描述.

以上说的 io 命令和非 io 命令都是 nvme 命令, 比如**块层**下发一个**写 request**, **nvme 驱动**就会根据此 request 构造出一个**写命令**, 将这个写命令放入**某个io queue**中, 当 controller 完成了这个写命令后, 会通过此 io queue 的**中断**返回完成信息, 驱动再将此完成信息返回给块层. 明白了两种队列的作用, 我们看看具体的数据结构 `struct nvme_queue`

```cpp
// drivers/nvme/host/pci.c
// nvme的命令队列, 其中包括sq和cq。一个nvme设备至少包含两个命令队列
// 一个是控制命令队列, 一个是IO命令队列
struct nvme_queue {
    // 所属的nvme_dev
	struct nvme_dev *dev;
    // queue的锁
	spinlock_t sq_lock;
    // sq的虚拟地址空间, 主机需要发给设备的命令就存在这里面
	void *sq_cmds;
	 /* only used for poll queues: */
    // cq poll queue 的锁
	spinlock_t cq_poll_lock ____cacheline_aligned_in_smp;
    // cq的虚拟地址空间, 设备返回的命令就存在这里面
	struct nvme_completion *cqes;
    // 实际就是sq_cmds的dma地址
	dma_addr_t sq_dma_addr;
    // cq的dma地址, 实际就是cqes对应的dma地址, 用于dma传输
	dma_addr_t cq_dma_addr;
    // 当前sq_tail位置, 是nvme设备上的一个寄存器, 告知设备最新的发送命令存在哪, 存在于bar空间中
	u32 __iomem *q_db;
    // cq和sq最大能够存放的command数量
	u32 q_depth;
    // 如果是admin queue, 那么为0, 之后的io queue按分配顺序依次增加, 主要用于获取对应的irq entry, 因为所有的queue的irq entry是一个数组
	u16 cq_vector;
    // 当有新的命令存放到sq时, sq_tail++, 如果sq_tail == q_depth, 
    // 那么sq_tail会被重新设置为0, 并且cq_phase翻转, 实际上就是一个环
	u16 sq_tail;
	u16 last_sq_tail;
    // 驱动已经处理完成的cmd位置,当cq_head == sq_tail时,表示cmd队列为空,当sq_tail == cq_head - 1时表示cmd队列已满
	u16 cq_head;
    // 此nvme queue在此nvme设备中的queue id 
    // 0: 控制命令队列
	u16 qid;
    // 初始设为1, 主要用于判断命令是否完成, 当cqe.status & 1 != cq_phase时, 表示命令还没有完成
    // 当每次sq_tail == q_depth时,此值会取反
	u8 cq_phase;
	u8 sqes;
	unsigned long flags;
#define NVMEQ_ENABLED		0
#define NVMEQ_SQ_CMB		1
#define NVMEQ_DELETE_ERROR	2
#define NVMEQ_POLLED		3
	u32 *dbbuf_sq_db;
	u32 *dbbuf_cq_db;
	u32 *dbbuf_sq_ei;
	u32 *dbbuf_cq_ei;
	struct completion delete_done;
};
```

nvme_queue 是 nvme 驱动最核心的数据结构, 它是 nvme 驱动和 nvme 设备通信的桥梁, 重点也要围绕 nvme_queue 来说, 之前也说过, 一个 nvme 设备有多个 nvme_queue(一个admin queue,至少一个io queue), 每个 nvme_queue 是独立的, 它们有

* 自己对应的中断(irq)

* 自己的 submission queue(sq), 用于将 struct nvme command 发送给 nvme 设备, 并且最多能存 dev->d_depth 个 nvme command

* 自己的 completion queue(cq), 用于 nvme 设备将完成的命令信息(struct nvme_completion)发送给 host, 并且最多能存 dev->d_depth 个 nvme_completion.

* 自己的 cmdinfo, 用于描述一个 nvme command. (struct nvme_cmd_info)

可以把 sq 想象成一个 `struct nvme_command sq[dev->d_depth]` 的数组, 而 cq 为 `struct nvme_completion cq[dev->d_depth]` 的数组.

struct nvme_command 主要用于存储一个 nvme 命令, 包括 io 命令, 或者控制命令, 当初始化好一个 struct nvme_command 后, 直接将其下发给 nvme 设备, nvme 设备就会根据它来执行对应操作, 其结构如下:




## nvme设备初始化

无论如何, 都会通过 `nvme_probe` 初始化 nvme 设备.

```cpp
// drivers/nvme/host/pci.c
static int nvme_probe(struct pci_dev *pdev, const struct pci_device_id *id)
{
	int node, result = -ENOMEM;
    // nvme设备描述符
	struct nvme_dev *dev;
	unsigned long quirks = id->driver_data;
	size_t alloc_size;
    // 当前 pci_dev 所在的 numa node id
    // /sys/devices/pci0000\:00/0000\:00\:11.0/0000\:04\:00.0/numa_node
	node = dev_to_node(&pdev->dev);
	if (node == NUMA_NO_NODE)
        // 如果没有的话, 默认用first_memory_node, 也就是第一个numa节点
		set_dev_node(&pdev->dev, first_memory_node);
    // 从当前numa节点上分配内存
    // 问题: 如果是NUMA_NO_NODE, 应该从first_memory_node分配??
	dev = kzalloc_node(sizeof(*dev), GFP_KERNEL, node);
	if (!dev)
		return -ENOMEM;
    // write_queues和poll_queues都是模块参数
    // 如果write_queues没有设置, 则read和write共享
    // 默认这两个都是没有设置的, 即0
	dev->nr_write_queues = write_queues;
	dev->nr_poll_queues = poll_queues;
    // 这里是默认值(cpus数目+1), 加1是因为有admin queue
	dev->nr_allocated_queues = nvme_max_io_queues(dev) + 1;
    // 从当前numa节点上分配内存, 分配queues, 就是nr_allocated_queues个
    // 每个cpu有一个io queue, 所有cpu共享一个admin queue
    // 这里的queue准确来说是一组sq和cq
	dev->queues = kcalloc_node(dev->nr_allocated_queues,
			sizeof(struct nvme_queue), GFP_KERNEL, node);
	if (!dev->queues)
		goto free;
    // nvme_dev对应的struct device, 增加struct device的引用计数
	dev->dev = get_device(&pdev->dev);
    // 设置pci_dev->dev(struct device)->driver_data等于当前nvme_dev
    // 结合上一行从而打通了struct nvme_dev,struct pci_dev和struct device
	pci_set_drvdata(pdev, dev);
    // 主要进行bar空间的映射
	result = nvme_dev_map(dev);
	if (result)
		goto put_pci;
    // 初始化两个work变量, 放在nvme_workq中执行
	INIT_WORK(&dev->ctrl.reset_work, nvme_reset_work);
	INIT_WORK(&dev->remove_work, nvme_remove_dead_ctrl_work);
    // 初始化互斥锁
	mutex_init(&dev->shutdown_lock);
    // 设置DMA需要的PRP内存池
	result = nvme_setup_prp_pools(dev);
	if (result)
		goto unmap;

	quirks |= check_vendor_combination_bug(pdev);

	if (!noacpi && acpi_storage_d3(&pdev->dev)) {
		/*
		 * Some systems use a bios work around to ask for D3 on
		 * platforms that support kernel managed suspend.
		 */
		dev_info(&pdev->dev,
			 "platform quirk: setting simple suspend\n");
		quirks |= NVME_QUIRK_SIMPLE_SUSPEND;
	}

	/*
	 * Double check that our mempool alloc size will cover the biggest
	 * command we support.
	 */
	alloc_size = nvme_pci_iod_alloc_size();
	WARN_ON_ONCE(alloc_size > PAGE_SIZE);

	dev->iod_mempool = mempool_create_node(1, mempool_kmalloc,
						mempool_kfree,
						(void *) alloc_size,
						GFP_KERNEL, node);
	if (!dev->iod_mempool) {
		result = -ENOMEM;
		goto release_pools;
	}
    // 初始化NVMe Controller结构体
	result = nvme_init_ctrl(&dev->ctrl, &pdev->dev, &nvme_pci_ctrl_ops,
			quirks);
	if (result)
		goto release_mempool;
    // 打印日志
	dev_info(dev->ctrl.device, "pci function %s\n", dev_name(&pdev->dev));
    // reset nvme controller
	nvme_reset_ctrl(&dev->ctrl);
	async_schedule(nvme_async_probe, dev);

	return 0;
}
```

### nvme_max_io_queues

`nvme_max_io_queues()` 获取 nvme 的最大 io_queues, 如下. 也就说 `poll_queue` 和 `write_queue` 是单独算的(why?):

```cpp
// drivers/nvme/host/pci.c
static unsigned int nvme_max_io_queues(struct nvme_dev *dev)
{
    // 总的cpu数目(包括offline的) +
    // write_queues +
    // poll_queue
	return num_possible_cpus() + dev->nr_write_queues + dev->nr_poll_queues;
}
```

### nvme_dev_map

`nvme_dev_map(struct nvme_dev *dev)` 主要将 nvme 设备 bar 空间的映射.

```cpp
// include/linux/nvme.h
enum {
    
	NVME_REG_DBS	= 0x1000,	/* SQ 0 Tail Doorbell, 4096 */
};

// drivers/nvme/host/pci.c
static int nvme_dev_map(struct nvme_dev *dev)
{
    // 转成 pci_dev
	struct pci_dev *pdev = to_pci_dev(dev->dev);
    // 获取bar空间并reserve,以防被其他人使用
	if (pci_request_mem_regions(pdev, "nvme"))
		return -ENODEV;
    // 将一个IO地址空间映射到内核的虚拟地址空间上去
    // 
	if (nvme_remap_bar(dev, NVME_REG_DBS + 4096))
		goto release;

	return 0;
}

// include/linux/ioport.h
// IO resources是有flags
// PCI设备会expose这些flags在自己的"resource" sysfs中
// /sys/devices/pci0000\:00/0000\:00\:11.0/0000\:04\:00.0/resource
// 
// IO地址空间
#define IORESOURCE_IO		0x00000100	/* PCI/ISA I/O ports */
// 内存地址空间
#define IORESOURCE_MEM		0x00000200

// include/linux/pci.h
static inline int
pci_request_mem_regions(struct pci_dev *pdev, const char *name)
{
	return pci_request_selected_regions(pdev,
			    pci_select_bars(pdev, IORESOURCE_MEM), name);
}

// drivers/pci/pci.c
int pci_select_bars(struct pci_dev *dev, unsigned long flags)
{
	int i, bars = 0;
	for (i = 0; i < PCI_NUM_RESOURCES; i++)
		if (pci_resource_flags(dev, i) & flags)
			bars |= (1 << i);
	return bars;
}

int pci_request_selected_regions(struct pci_dev *pdev, int bars,
				 const char *res_name)
{
    // 第二个参数就是 pci_select_bars 的返回
	return __pci_request_selected_regions(pdev, bars, res_name, 0);
}

static int __pci_request_selected_regions(struct pci_dev *pdev, int bars,
					  const char *res_name, int excl)
{
	int i;
    // 最多6个
	for (i = 0; i < PCI_STD_NUM_BARS; i++)
        // 每个 bit 代表一个 bar
		if (bars & (1 << i))
			if (__pci_request_region(pdev, i, res_name, excl))
				goto err_out;
	return 0;
}

// drivers/nvme/host/pci.c
static int nvme_remap_bar(struct nvme_dev *dev, unsigned long size)
{
    // 转成 pci_dev
	struct pci_dev *pdev = to_pci_dev(dev->dev);

	if (size <= dev->bar_mapped_size)
		return 0;
	if (size > pci_resource_len(pdev, 0))
		return -ENOMEM;
    // 已经映射的, 需要iounmap掉
	if (dev->bar)
		iounmap(dev->bar);
    // 进行map
	dev->bar = ioremap(pci_resource_start(pdev, 0), size);
	if (!dev->bar) {
		dev->bar_mapped_size = 0;
		return -ENOMEM;
	}
	dev->bar_mapped_size = size;
	dev->dbs = dev->bar + NVME_REG_DBS;

	return 0;
}
```

可以看到 `nvme_dev_map` 的执行过程可以分为三步

第一步: 调用 `pci_select_bars`, 其返回值是 mask。因为 pci 设备的 **header 配置空间**有 **6 个 32 位**的 **Bar 寄存器**(如下图), 所以 mark 中的**每一位**的值就代表**其中一个 Bar 是否被置起**。

![2023-02-10-18-17-52.png](./images/2023-02-10-18-17-52.png)

第二步: 调用 `pci_request_selected_regions`, 这个函数的一个参数就是之前调用 `pci_select_bars` 返回的 mask 值, 作用就是**把对应的这个几个 bar 保留起来**, **不让别人使用**。 

第三步: 调用 **ioremap**。在 linux 中我们**无法直接访问物理地址**, 需要**映射到虚拟地址**, `ioremap` 就是这个作用。映射完后, 我们访问 `dev->bar` 就可以**直接操作 nvme 设备上的寄存器**了。但是代码中, 并**没有**根据 `pci_select_bars` 的**返回值**来**决定映射哪个 bar**, 而是直接映射 bar0, 原因是 nvme 协议中强制规定了 bar0 就是内存映射的基址。

![2023-02-10-22-09-59.png](./images/2023-02-10-22-09-59.png)

## nvme_setup_prp_pools



```cpp
// drivers/nvme/host/pci.c
static int nvme_setup_prp_pools(struct nvme_dev *dev)
{
	dev->prp_page_pool = dma_pool_create("prp list page", dev->dev,
						NVME_CTRL_PAGE_SIZE,
						NVME_CTRL_PAGE_SIZE, 0);
	if (!dev->prp_page_pool)
		return -ENOMEM;

	/* Optimisation for I/Os between 4k and 128k */
	dev->prp_small_pool = dma_pool_create("prp list 256", dev->dev,
						256, 256, 0);
	if (!dev->prp_small_pool) {
		dma_pool_destroy(dev->prp_page_pool);
		return -ENOMEM;
	}
	return 0;
}
```

主要是创建了两个 dma pool, 后面就可以通过其他 dma 函数从 dma pool 中获得 memory 了。prp_small_pool 里提供的是块大小为 256 字节的内存，prp_page_pool 提供的是块大小为 `Page_Size`(格式化时确定，例如4KB)的内存，主要是为了对于不一样长度的 prp list 来做优化。



## nvme_init_ctrl

主要是调用 `device_create_with_groups` 函数创建一个名字叫 **nvme0** 的**字符设备**。这个 nvme0 中的 **0** 是通过 `ida_alloc(&nvme_instance_ida, GFP_KERNEL)` 获得的。这个过程中，通过 `ida_get_new` 获得唯一的索引值。

```cpp
// drivers/nvme/host/core.c
int nvme_init_ctrl(struct nvme_ctrl *ctrl, struct device *dev,
		const struct nvme_ctrl_ops *ops, unsigned long quirks)
{
	int ret;

	ctrl->state = NVME_CTRL_NEW;
	clear_bit(NVME_CTRL_FAILFAST_EXPIRED, &ctrl->flags);
	spin_lock_init(&ctrl->lock);
	mutex_init(&ctrl->scan_lock);
	INIT_LIST_HEAD(&ctrl->namespaces);
	xa_init(&ctrl->cels);
	init_rwsem(&ctrl->namespaces_rwsem);
	ctrl->dev = dev;
	ctrl->ops = ops;
	ctrl->quirks = quirks;
	ctrl->numa_node = NUMA_NO_NODE;
	INIT_WORK(&ctrl->scan_work, nvme_scan_work);
	INIT_WORK(&ctrl->async_event_work, nvme_async_event_work);
	INIT_WORK(&ctrl->fw_act_work, nvme_fw_act_work);
	INIT_WORK(&ctrl->delete_work, nvme_delete_ctrl_work);
	init_waitqueue_head(&ctrl->state_wq);

	INIT_DELAYED_WORK(&ctrl->ka_work, nvme_keep_alive_work);
	INIT_DELAYED_WORK(&ctrl->failfast_work, nvme_failfast_work);
	memset(&ctrl->ka_cmd, 0, sizeof(ctrl->ka_cmd));
	ctrl->ka_cmd.common.opcode = nvme_admin_keep_alive;

	BUILD_BUG_ON(NVME_DSM_MAX_RANGES * sizeof(struct nvme_dsm_range) >
			PAGE_SIZE);
	ctrl->discard_page = alloc_page(GFP_KERNEL);
	if (!ctrl->discard_page) {
		ret = -ENOMEM;
		goto out;
	}
    // IDA是一种基于Radix的ID分配机制
    // 分配一个没有使用的ID
	ret = ida_alloc(&nvme_instance_ida, GFP_KERNEL);
	if (ret < 0)
		goto out;
	ctrl->instance = ret;

	device_initialize(&ctrl->ctrl_device);
	ctrl->device = &ctrl->ctrl_device;
	ctrl->device->devt = MKDEV(MAJOR(nvme_ctrl_base_chr_devt),
			ctrl->instance);
	ctrl->device->class = nvme_class;
	ctrl->device->parent = ctrl->dev;
	ctrl->device->groups = nvme_dev_attr_groups;
	ctrl->device->release = nvme_free_ctrl;
	dev_set_drvdata(ctrl->device, ctrl);
	ret = dev_set_name(ctrl->device, "nvme%d", ctrl->instance);
	if (ret)
		goto out_release_instance;

	nvme_get_ctrl(ctrl);
	cdev_init(&ctrl->cdev, &nvme_dev_fops);
	ctrl->cdev.owner = ops->module;
	ret = cdev_device_add(&ctrl->cdev, ctrl->device);
	if (ret)
		goto out_free_name;

	/*
	 * Initialize latency tolerance controls.  The sysfs files won't
	 * be visible to userspace unless the device actually supports APST.
	 */
	ctrl->device->power.set_latency_tolerance = nvme_set_latency_tolerance;
	dev_pm_qos_update_user_latency_tolerance(ctrl->device,
		min(default_ps_max_latency_us, (unsigned long)S32_MAX));

	nvme_fault_inject_init(&ctrl->fault_inject, dev_name(ctrl->device));
	nvme_mpath_init_ctrl(ctrl);
	nvme_auth_init_ctrl(ctrl);

	return 0;
}
```

### IDA

IDA 是一种基于 Radix 的 ID 分配机制, 通过 `DEFINE_IDA` 静态注册, 从而实现**多次获取累加的数字**.

```cpp
static DEFINE_IDA(nvme_instance_ida);
ret = ida_alloc(&nvme_instance_ida, GFP_KERNEL);
```

比如，我们用的pci 驱动，底下用 12 个 pcie 设备。换言之，12 个 pcie 设备 device 共用一个 pcie 驱动 driver。

这个时候，每个设备的名字编号，就使用上面的 ida 来进行获取。

相当于有个机制，帮你记录着编号，下次直接获取下一个编号。





## 核心实现分析

在上面初始化流程中需要重点关注的是 `nvme_alloc_ns` 函数的流程. 该函数完成了块设备创建、基本信息填充和块设备注册到内核等工作. 这部分片段完成了函数指针的初始化、命令队列初始化和设备名称的初始化等工作. 具体关于 `nvme_alloc_ns` 函数源代码的逻辑请自行阅读代码, 本文不再赘述. 

```cpp

```

在整个初始化流程中比较关键的是对**请求队列**(`request_queue`)中请求处理函数指针(`make_request_fn`)的初始化及**多队列函数集**(`mq_ops`)的初始化. 因为, 这里的函数正是 NVMe 区别于 SCSI 等类型设备数据处理流程的地方. 



# NVMe设备的IO流程

为了便于理解NVMe的处理流程, 我们给出了传统SCSI及NVMe数据处理的对比流程. 如图5所示, 整个流程是从通用块层的接口(submit_bio)开始的, 这个函数大家都非常清楚了. 

![2023-02-09-21-45-27.png](./images/2023-02-09-21-45-27.png)

对于NVMe设备来说, 在初始化的时候初始化函数指针make_request_fn为nvme_queue_rq, 该函数就是NVMe驱动程序的请求处理接口. 该函数最终会将请求写入NVMe中的SQ队列当中, 并通知控制器处理请求. 


