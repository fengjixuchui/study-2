
<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [1. 硬件虚拟化技术背景](#1-硬件虚拟化技术背景)
  - [1.1. 指令集](#11-指令集)
  - [1.2. MMU](#12-mmu)
  - [1.3. IO](#13-io)
- [2. 虚拟化方案与KVM](#2-虚拟化方案与kvm)
- [3. QEMU/KVM实现概述](#3-qemukvm实现概述)
  - [3.1. KVM/QEMU架构](#31-kvmqemu架构)
  - [3.2. KVM的抽象对象](#32-kvm的抽象对象)
  - [3.3. KVM的vcpu](#33-kvm的vcpu)
  - [3.4. QEMU/KVM工作原理](#34-qemukvm工作原理)
  - [3.5. 相关进程/线程](#35-相关进程线程)
  - [3.6. KVM运行时的三种模式](#36-kvm运行时的三种模式)
  - [3.7. 理解KVM与Qemu的关系](#37-理解kvm与qemu的关系)
- [4. CPU虚拟化](#4-cpu虚拟化)
- [5. Mem虚拟化](#5-mem虚拟化)
- [6. IO虚拟化](#6-io虚拟化)
  - [6.1. IO的虚拟化](#61-io的虚拟化)
  - [6.2. VirtIO](#62-virtio)
- [7. KVM IO可能优化地方](#7-kvm-io可能优化地方)
  - [7.1. Virt-IO的硬盘优化](#71-virt-io的硬盘优化)
  - [7.2. 普通设备的直接分配(Direct Assign)](#72-普通设备的直接分配direct-assign)
  - [7.3. 普通设备的复用](#73-普通设备的复用)
- [8. 参考](#8-参考)

<!-- /code_chunk_output -->

# 1. 硬件虚拟化技术背景

详细见`系统虚拟化`

**硬件虚拟化技术**通过虚拟化**指令集**、**MMU**(Memory Map Unit)以及**IO**来运行不加修改的操作系统. 

## 1.1. 指令集

传统的处理器通过选择不同的运行(Ring 特权)模式, 来选择指令集的范围, 内存的寻址方式, 中断发生方式等操作. 在原有的Ring特权等级的基础上, 处理器的硬件虚拟化技术带来了一个新的运行模式: Guest模式[1], 来实现指令集的虚拟化. 当切换到Guest模式时, 处理器提供了先前完整的特权等级, 让Guest操作系统可以不加修改的运行在物理的处理器上. Guest与Host模式的处理器上下文完全由硬件进行保存与切换. 此时, 虚拟机监视器(Virtual Machine Monitor)通过一个位于内存的数据结构(Intel称为VMCS, AMD称为VMCB)来控制Guest系统同Host系统的交互, 以完成整个平台的虚拟化. 

## 1.2. MMU

传统的操作系统通过硬件MMU完成虚拟地址到物理地址的映射. 在虚拟化环境中, Guest的虚拟地址需要更多一层的转换, 才能放到地址总线上: 

```
    Guest虚拟地址 -> Guest物理地址 -> Host物理地址
                 ^               ^
                 |               |
                MMU1            MMU2
```

其中MMU1可以由软件模拟(Shadow paging中的vTLB)或者硬件实现(Intel EPT、AMD NPT). MMU2由硬件提供. 

## 1.3. IO

系统的IO虚拟化技术, 通常是VMM捕捉Guest的IO请求, 通过软件模拟的传统设备将其请求传递给物理设备. 一些新的支持虚拟化技术的设备, 通过硬件技术(如Intel VT-d), 可以将其直接分配给Guest操作系统, 避免软件开销. 

# 2. 虚拟化方案与KVM

在所有虚拟化方案中, 都是由**hypervisor取代原生的OS**去控制具体硬件资源, 同时将资源分配具体的VM, VM中运行的是没有修改过的OS, 如果让VM中的OS能正常运行, hypervisor的任务就是模拟具体的硬件资源, 让OS不能识别出是真是假. 

图5-1 KVM 和 Xen 虚拟化方案比较:

![](./images/2019-07-05-12-04-36.png)

在**Xen**的体系结构中, **Xen Hypervisor**运行于**硬件之上**, 并且将系统资源进行了虚拟化操作, 将虚拟化的资源分配给上层的虚拟机(VM), 然后通过虚拟机VM来运行相应的客户机操作系统. 

KVM全称是Kernel\-based Virtual Machine, 是一个基于Linux环境的开源虚拟化解决方案. 

KVM 与 Xen、VMware 等提供完整解决方案的商业化虚拟产品不同, KVM的**思想**是在**Linux内核的基础**上添加**虚拟机管理模块**, 这个样子[**内存分配**](http://www.oenhan.com/kernel-program-exec), [**进程调度**](http://www.oenhan.com/task-group-sched), **IO管理**等就**无需重写代码**, 如此hypervisor就是所谓的host, VM中的OS就是guest. 

因此, KVM并**不是一个完整的模拟器**, 而只是一个提供了虚拟化功能的**内核模块**, 具体的**模拟器工作**是借助**QEMU**来完成的. 

# 3. QEMU/KVM实现概述

见`Virtualization/Learning/KVM实战: 原理、进阶与性能调优`

## 3.1. KVM/QEMU架构

KVM是基于硬件辅助虚拟化技术(Intel的`VT-x`或者`AMD-V`)的全虚拟化解决方案, 客户机操作系统能够不经过修改直接在 KVM 的虚拟机中运行, **每一台虚拟机**能够享有**独立**的**虚拟硬件资源**: **网卡**、**磁盘**、**图形适配器**等. 

基于KVM/QEMU架构实现的虚拟化, 相关的架构如下图所示: (仔细看图)

![2019-12-11-15-28-12.png](./images/Kernel-based_Virtual_Machine.svg)

![Kernel-based_Virtual_Machine_zh-CN.png](./images/Kernel-based_Virtual_Machine_zh-CN.svg)

https://zh.wikipedia.org/wiki/File:Kernel-based_Virtual_Machine_zh-CN.svg

![2020-04-15-19-55-06.png](./images/2020-04-15-19-55-06.png)

**KVM**支持**虚拟化嵌套**, 即在虚拟化出来的主机中, KVM能够再次进行虚拟机中的虚拟化, 但是此项功能**只针对KVM**, 不支持Xen、VMwaer等其他虚拟化架构方案. 

KVM是Linux内核的**一个模块**, 基于**硬件虚拟化技术**实现VMM的功能. 该模块的工作主要是通过操作**与处理器共享的数据结构**来实现**指令集**以及**MMU**的**虚拟化**, 捕捉**Guest**的**IO指令**(包括`Port IO`和`mmap IO`)以及实现**中断虚拟化**. 

至于**IO设备的软件模拟**, 是通过**用户程序QEMU**来实现的. QEMU负责解释**IO指令流**, 并将其请求换成**系统调用**或者**库函数**传给**Host操作系统**, 让Host上的驱动去完成**真正的IO操作**. 它们之间的关系如下图所示: 

```
    +--------------+               +--------+    
    | Qemu         |               |        |   
    |              |               |        |               
    | +---+  +----+|               | Guest  |               
    | |vHD|  |vNIC||<-----+        |        |               
    | +---+  +----+|      |        |        |               
    +--------------+      |        +--------+               
         ^                |            ^                    
         | syscall        |IO stream   |                    
         | via FDs        |            |                      
    +----|----------------|------------|--------+             
    |    |                |            v        |             
    |    v                |       +----------+  |   
    |  +--------+         +------>|          |  |
    |  |drivers |<--+             |  kvm.ko  |  |
    |  +--------+   |             +----------+  |
    |    ^          |   Host kernel             |   
    +----|----------|---------------------------+
         v          v                      
    +--------+    +---+                    
    | HDD    |    |NIC|                    
    +--------+    +---+
```

从Host操作系统的角度来看, **Guest**相当于**一个进程**运行在系统上, 这个进程就是**qemu**, 普通的命令如kill、top、taskset等可以作用于该Guest. 该**进程**的**用户虚拟空间**就是**Guest的物理空间**, 该**进程的线程**对应着**Guest的处理器**. 

从Qemu的角度来看, KVM模块抽象出了**三个对象**, 分别代表**KVM自己**, **Guest的虚拟空间**(VM)以及**运行虚拟处理器**(VCPU). 这三个对象分别对应着**三个文件描述符**, Qemu通过**文件描述符**用**系统调用IOCTL**来操作这三个对象, 同KVM交互. 此时, Qemu主要**只模拟设备**, 它以前的**CPU**和**MMU的模拟逻辑**都被`kvm.ko`取代了. 

## 3.2. KVM的抽象对象

KVM同应用程序(Qemu)的交互接口为`/dev/kvm`, 通过**open**以及**ioctl系统调用**可以获取并操作KVM抽象出来的**三个对象**, 
- Guest的虚拟处理器(`fd_vcpu[N]`), 
- Guest的地址空间(`fd_vm`), 
- KVM本身(`fd_kvm`). 

其中每一个Guest可以含有多个vcpu, 每一个vcpu对应着Host系统上的一个线程. 

Qemu启动Guest系统时, 通过`/dev/kvm`获取`fd_kvm`和`fd_vm`, 然后通过`fd_vm`将**Guest**的"**物理空间**"**mmap**到**Qemu进程**的**虚拟空间**, 并根据配置信息创建`vcpu[N]`线程, 返回`fd_vcpu[N]`. 然后Qemu将操作`fd_vcpu`在其自己的进程空间mmap一块KVM的数据结构区域. 该数据结构(下图中的shared)用于同kvm.ko交互, 包含Guest的IO信息, 如端口号, 读写方向, 内存地址等. Qemu通过这些信息, 调用虚拟设备注册的回调函数来模拟设备的行为, 并将Guest IO请求换成系统请求发送给Host系统. 由于Guest的地址空间已经映射到Qemu的进程空间里面, Qemu的虚拟设备逻辑可以很方便的存取Guest地址空间里面的数据. 

三个对象之间的关系如下图所示: 

QEMU通过KVM等fd进行IOCTL控制KVM驱动的运行过程. 

![config](images/2.png)

```
    +----------+            |         +--------+
    | Qemu     | Host user  |         |        |
    |          |            |         |        |
    |          |            |         | Guest  |
    |  +------+|            |         | user   |
    |  |shared||            |         |        |
    |  +------+|            |         |        |
    |       ^  |            |         |        |
    +-------|--+            |         |        |
        |   |               |         |        |
     fds|   |               |         |        |
  ------|---|---------------|         |--------|
        |   |               |         |        |
        v   v   Host kernel |         | Guest  |
     +---------+            |         | kernel |
     |         |            |         |        |
     |  kvm.ko |----+       |         |        |
     |         |    |fd_kvm |         |        |
     +---------+    |       |         +--------+
                    v       |             ^
                  +----+    |   fd_vm     |
                  |vmcs|----+--------------
      +------+    +----+    |          +------+
      | host |              |          | Guest|
      | mode |              |fd_vcpu   | mode |       
      +------+              |          +------+
          ^                 v             ^
          |             +-------+         |
          | vm exit     |  Phy  | vm entry|
          +-------------|  CPU  |---------+
                        +-------+
```

图中`vm-exit`代表处理器进入host模式, 执行kvm和Qemu的逻辑. `vm-entry`代表处理器进入Guest模式, 执行整个Guest系统的逻辑. 如图所示, Qemu通过三个文件描述符同kvm.ko交互, 然后kvm.ko通过vmcs这个数据结构同处理器交互, 最终达到控制Guest系统的效果. 其中`fd_kvm`主要用于Qemu同KVM本身的交互, 比如获取KVM的版本号, 创建地址空间、vcpu等. `fd_vcpu`主要用于控制处理器的模式切换, 设置进入Guest mode前的处理器状态等等(内存寻址模式, 段寄存器、控制寄存器、指令指针等), 同时Qemu需要通过`fd_vcpu`来mmap一块KVM的数据结构区域. `fd_vm`主要用于Qemu控制Guest的地址空间, 向Guest注入虚拟中断等. 

## 3.3. KVM的vcpu

如前文所述, KVM的vcpu对应着host系统上的一个线程. 从Qemu的角度来看, 她运行在一个loop中: 

```cpp
	for (;;) {
		kvm_run(vcpu);
		switch (shared_data->exit_reason) {
		...
		case KVM_IO: 
			handle_io(vcpu);
			break;
		case KVM_MMIO:
			handle_mmio(vcpu);
			break;
		...
		}
	}
```

该线程同Guest的vcpu紧密相连. 如果我们把线程的执行看作Guest vcpu的一部分, 那么从Host的角度来看, 该vcpu在三种不同的上下文中运行: Host user/Host kernel/Guest, 将运行于一个更大的循环当中. 该vcpu的运行逻辑如下图: 

```
      Host user   |  Host kernel  | Guest mode   |
                  |               |              |
                  |               |              |
 +->kvm_run(vcpu)-------+         |              | 
 |                |     v         |              |
 |                | +->vm entry----------+       |     
 |                | |             |      v       |
 |                | |             |   Execute    |
 |                | |             |   Natively   |
 |                | |             |      |       |
 |                | |  vm exit<----------+       |  
 |                | |    |        |              |  
 |                | |    |        |              |
 |            Yes | |    v        |              |   
 |     +----------------I/O ?     |              |    
 |     |          | |    | No     |              |
 |     |          | |    |        |              |      
 |     |          | |    v        |              |
 |     v      Yes | |  Signal     |              |
 +--Handle IO<---------Pending?   |              |
                  | |    | No     |              |
                  | +----+        |              |
```

实际上, 在host上通过ps命令看到的关于vcpu这个线程的运行时间正是这三种上下文的总和. 

## 3.4. QEMU/KVM工作原理

KVM的工作原理如图: 

![工作原理](images/5.png)

![](./images/2019-06-04-15-30-02.png)

上图是一个执行过程图, 首先启动一个**虚拟化管理软件qemu**, 开始启动一个虚拟机, 通过**ioctl**等**系统调用**向**内核**中**申请指定的资源**, 搭建好虚拟环境, 启动虚拟机内的OS, 执行 **VMLAUCH** 指令, 即**进入了guest代码**执行过程. 

如果 Guest OS 发生**外部中断**或者**影子页表缺页**之类的事件, 暂停 Guest OS 的执行, **guest VM\-exit**, 进行一些必要的处理, 然后重新进入客户模式, 执行guest代码; 这个时候如果是io请求, 则提交给用户态下的qemu处理, qemu处理后再次通过IOCTL反馈给KVM驱动. 

KVM的大致工作原理: 

1. 用户模式的Qemu利用接口libkvm通过**ioctl系统调用**进入**内核模式**. KVMDriver为虚拟机创建虚拟CPU和虚拟内存, 然后执行VMLAUNCH指令进入**客户模式**, 装载Guest OS并运行. 
2. Guest OS运行过程中如果发生中断或者影子缺页等异常, 将暂停Guest OS的运行并保存当前上下文退出到**内核模式**来处理这些异常. 
3. 内核模式处理这些异常时如果不需要I/O则处理完成后重新进入客户模式, 如果需要I/O则进入到**用户模式**, 由Qemu来处理I/O, 处理完成后进入内核模式, 再进入**客户模式**. 

## 3.5. 相关进程/线程

每一个虚拟机(guest)在Host上都被模拟为一个QEMU进程, 即emulation进程, 它有自己的pid, 也可以被kill系统调用直接杀死(在这种情况下, 虚拟机的行为表现为"突然断电"). 在一个Linux系统中, 有多少个VM, 就有多少个进程. 

并且有四个线程, 线程数量不是固定的, 但是**至少会有三个**(**vCPU**, **IO**, **Signal**). 其中几个vCPU对应几个线程, 有一个**IO线程**还有一个**信号处理线程**. 

```cpp
➜  ~ pstree -p 20308
qemu-system-x86(20308)-+-{qemu-system-x86}(20353)
                       |-{qemu-system-x86}(20408)
                       |-{qemu-system-x86}(20409)
                       |-{qemu-system-x86}(20412)
```

## 3.6. KVM运行时的三种模式

kvm 模块让Linux主机成为一个虚拟机监视器(VMM), 并且在原有的Linux两种执行模式基础上, 新增加了客户模式, 客户模式拥有自己的内核模式和用户模式. 

所以在**虚拟机运行**时, 有三种模式, 分别是: 

- **客户模式**: 我们可以简单理解成客户机操作系统运行在的模式, 它本身又分为自己的内核模式和用户模式. 执行非I/O的客户代码, 虚拟机运行在这个模式下. 

- **用户模式**: 为用户提供虚拟机管理的用户空间工具以及代表用户执行I/O, Qemu运行在这个模式之下. 

- **内核模式**: 模拟CPU以及内存, 实现客户模式的切换, 处理从客户模式的退出. KVM内核模块运行在这个模式下. 

三种模式的层次关系我们可以用图简单描述一下: 

![三种模式的关系](images/4.png)

![](./images/2019-06-04-15-01-11.png)

在kvm的模型中, **每一个Gust OS**都是作为一个**标准的linux进程**, 都可以使用linux进程管理命令管理. 

这里假如**qemu**通过**ioctl**发出**KVM\_CREATE\_VM** 指令, **创建了一个VM后**, qemu需要需要发送一些命令给VM, 如**KVM\_CREATE\_VCPU**. 这些命令当然也是通过**ioctl**发送的, 用户程序中用ioctl发送**KVM\_CREATE_VM**得到的**返回值**就是**新创建的VM对应的fd(kvm\_vm**), fd是创建的**指向特定虚拟机实例**的**文件描述符**, 之后利用这个fd发送命令给VM进行访问控制. kvm解析这些命令的函数是kvm\_vm\_ioctl. 

## 3.7. 理解KVM与Qemu的关系

我们都知道开源虚拟机KVM, 并且知道它总是跟Qemu结合出现, 那这两者之间有什么关系呢？

首先, **Qemu**本身并不是KVM的一部分, 而是一整套**完整的虚拟化解决方案**, 它是纯软件实现的, 包括处理器虚拟化、内存虚拟化以及各种虚拟设备的模拟, 但因为是纯软件模拟, 所以性能相对比较低, 它被KVM进行改造后, 作为**KVM的前端**存在, 用来进行[**创建进程**](http://www.oenhan.com/cpu-load-balance)或者**IO交互**等; 

而广义的KVM实际上包含两部分, 一部分是基于Linux内核支持的KVM内核模块, 另一部分就是经过简化和修改Qemu. 

KVM内核模块模拟处理器和内存以支持虚拟机的运行, Qemu主要处理I/O以及为用户提供一个用户空间工具来进行虚拟机的管理. 两者相互结合, 相辅相成, 构成了一个完整的虚拟化平台. 

# 4. CPU虚拟化

X86体系结构**CPU虚拟化**技术的称为 **Intel VT\-x** 技术, 引入了VMX, 提供了两种处理器的工作环境.  VMCS 结构实现两种环境之间的切换.  VM Entry 使虚拟机进去guest模式, VM Exit 使虚拟机退出guest模式. 

VMM调度guest执行时, qemu 通过 ioctl [系统调用](http://oenhan.com/kernel-program-exec)进入内核模式, 在 KVM Driver中获得当前物理 CPU的引用. 之后将guest状态从VMCS中读出, 并装入物理CPU中. 执行 VMLAUCH 指令使得物理处理器进入非根操作环境, 运行guest OS代码. 

当 guest OS 执行一些特权指令或者外部事件时, 比如I/O访问, 对控制寄存器的操作, MSR的读写等,  都会导致物理CPU发生 VMExit,  停止运行 Guest OS, 将 Guest OS保存到VMCS中, Host 状态装入物理处理器中,  处理器进入根操作环境, KVM取得控制权, 通过读取 VMCS 中 VM_EXIT_REASON 字段得到引起 VM Exit 的原因.  从而调用kvm_exit_handler 处理函数.  如果由于 I/O 获得信号到达, 则退出到userspace模式的 Qemu 处理. 处理完毕后, 重新进入guest模式运行虚拟 CPU. 

guest的所有用户级别(user)的指令集, 都会直接由宿主机线程执行, 此线程会调用KVM的ioctl方式提供的接口加载guest的指令并在特殊的CPU模式下运行, 不需要经过CPU指令集的软件模拟转换, 大大的减少了虚拟化成本, 这也是KVM优于其他虚拟化方式的点之一. 

KVM向外提供了一个虚拟设备/dev/kvm, 通过ioctl(IO设备带外管理接口)来对KVM进行操作, 包括虚拟机的初始化, 分配内存, 指令加载等等. 

# 5. Mem虚拟化

OS对于物理内存主要有两点认识: 1.物理地址从0开始; 2.[内存地址](http://www.oenhan.com/kernel-program-exec)是连续的. 

VMM接管了所有内存, 但guest OS的对内存的使用就存在这两点冲突了, 除此之外, 一个guest对内存的操作很有可能影响到另外一个guest乃至host的运行. VMM的内存虚拟化就要解决这些问题. 

在OS代码中, 应用也是占用所有的逻辑地址, 同时不影响其他应用的关键点在于有线性地址这个中间层; 解决方法则是添加了一个中间层: **guest物理地址空间**; guest看到是从0开始的guest物理地址空间(类比从0开始的线性地址), 而且是连续的, 虽然有些地址没有映射; 同时guest物理地址映射到不同的host逻辑地址, 如此保证了VM之间的安全性要求. 

这样MEM虚拟化就是GVA\->GPA\->HPA的寻址过程, 传统软件方法有**影子页表**, 硬件虚拟化提供了EPT支持. 

guest的内存在host上由emulator提供, 对emulator来说, guest访问的内存就是他的虚拟地址空间, guest上需要经过一次虚拟地址到物理地址的转换, 转换到guest的物理地址其实也就是emulator的虚拟地址, emulator再次经过一次转换, 转换为host的物理地址. 后面会有介绍各种虚拟化的优化手段, 这里只是做一个overview. 

# 6. IO虚拟化

## 6.1. IO的虚拟化

**传统系统**中, **设备**都**直接或间接**的**挂在PCI总线**上. **PCI设备**通过**PCI配置空间**以及**设备地址空间**接收操作系统的**驱动请求和命令**, 通过**中断机制通知反馈操作系统**. **配置空间**和**设备地址空间**都将映射到**处理器Port空间**或者**操作系统内存空间**中, 所以设备的软件模拟需要VMM将相关的**Guest PIO**和**MMIO**请求截获, 通过硬件虚拟化提供的机制将其传送给软件. 模拟软件处理完后再通过VMM提供的**虚拟中断机制**反馈Guest. 如下图所示: 

```
        +-----------------------------------+
        | +--------------+                  |   
        | | PCI config   |     +----------+ |
        | +--------------+<--->| driver   | | 
        | +--------------+<--->|          | | 
        | | Device memory|     +----------+ |
        | +--------------+           ^      |   
        |       ^                    |      |   
        +-------|--------------------|------+  
                |                    | vINTR via VMM    
PIO/MMIO via VMM|         +----------+           
                v         |                       
         +------------------------+    
         | +--------+  +--------+ |    
         | |  PCI   |  | Device | |    
         | | config |  | memory | |  Virtual Device    
         | +--------+  +--------+ |              
         +------------------------+    
                      |   
                      v   
                +------------+
                |host driver |
                +------------+
```

**虚拟设备**的**软件逻辑**放在**用户层**也可以放在**内核**中. **完全的虚拟设备模拟**, 可以处理在Guest中不加修改的驱动请求. 通常这将消耗大量的处理器cycle去模拟设备. 如果可以修改或者重写Guest的驱动代码, 那么虚拟设备和驱动之间的IO接口可以根据虚拟化的特性重新定义为更高层更加高效的接口, 如下图所示: 

```
        +----------------+
        |                |
        | +-----------+  |
        | |para-driver|  |
        | +-----------+  |
        +-------^--------+
                |         
                | new I/O interface via VMM
                v
            +---------+                      
            |Virtual  |                       
            |device   |                       
            +---------+                       
                |                             
                v
           +------------+
           |host driver |
           +------------+
```

KVM的virtio正是通过这种方式提供了高速IO通道. 

除了软件模拟, 现有的硬件虚拟化技术还可以将一些支持虚拟化技术的新兴硬件直接分配给Guest. 除了需要支持虚拟化技术的硬件(可以发起remmappable的MSI中断请求), 设备的直接分配一般还需要主板上的芯片以及CPU支持, 比如英特尔的VT-d技术. 支持虚拟化技术的硬件平台主要做两件事, 一个是DMA Remapping, 将DMA请求中的Guest的物理地址映射到Host的物理地址, 另一个是中断Remapping, 将能remappable的中断请求根据由VMM设置, 位于内存的IRT(Interrupt Remapping Table)发送到指定的vcpu上. 

PC平台上, 通常北桥(或者类似结构的root-complex)连接着CPU、内存以及外设. 用于DMA Remapping和中断Remapping的硬件逻辑位于北桥中. 如下所示: 

```
      +-------------+
      |cpu0, cpu1...|              
      +-------------+    
            ^    
            |        <-- System Bus  
            |                |   
            v                v   
   +---------------------+  
   |  North Bridge       |   
   |                     |       +--------+
   |    +--------+       |<----->| Memory |
   |    |  vt-d  |       |       +--------+
   |    +--------+       |   
   +---------------------+   
         ^            ^          
         |            |   
         v            v    
    +--------+    +--------+
    | PCI-e  |    | South  |<-----> PCI legacy devices...
    | device |    | Bridge |
    +--------+    +--------+
```

目前, 只有支持MSI的PCI/PCI-e设备才能直接分配给Guest. 其中PCI-e设备可以直接与北桥相连或者桥连, 然后单独分配给一个Guest. 在一个桥后的所有的桥连PCI设备只能作为一个整体分配给一个Guest. KVM在硬件虚拟化的平台上支持PCI-e/PCI设备的直接分配. 




guest作为一个进程存在, 当然他的内核的所有驱动等都存在, 只是硬件被QEMU所模拟(后面介绍virtio的时候特殊). guest的所有硬件操作都会有QEMU来接管, QEMU负责与真实的宿主机硬件打交道. 

## 6.2. VirtIO

VirtIO为Guest和Qemu提供了高速的IO通道. Guest的磁盘和网络都是通过VirtIO来实现数据传输的. 由于Guest的地址空间mmap到Qemu的进程空间中, VirtIO以共享内存的数据传输方式以及半虚拟化(para-virtualized)接口为Guest提供了高效的硬盘以及网络IO性能. 其中, KVM为VirtIO设备与Guest的VirtIO驱动提供消息通知机制, 如下图所示: 

```
     +---------------+
     |  Qemu         |
     |    +--------+ |        +-------------------+
     |    | VirtIO | |        | +---------+       |
     |    | Device | |        | | VirtIO  | Guest |
     |    +--------+ |        | | Driver  |       |  
     +------|--^-----+        | +---------+       |  
            |  |              +---|---^-----------+  
      irqfd |  |              PIO |   |               
      fd_vm |  |ioeventfd         |   |vInterrupt             
   ---------|--|------------------|---|------------
            v  |                  v   |
        +----------+         +--------------+ Host
        | eventfd  |<------->|  KVM.ko      | kernel
        | core     |         |              |
        +----------+         +--------------+
```

如图所示, Guest VirtIO驱动通过访问port空间向Qemu的VirtIO设备发送IO发起消息. 而设备通过读写irqfd或者IOCTL fd_vm通知Guest驱动IO完成情况. irqfd和ioeventfd是KVM为用户程序基于内核eventfd机制提供的通知机制, 以实现异步的IO处理(这样发起IO请求的vcpu将不会阻塞). 之所以使用PIO而不是MMIO, 是因为KVM处理PIO的速度快于MMIO. 


# 7. KVM IO可能优化地方

## 7.1. Virt-IO的硬盘优化

从图1中可以看到, Guest的IO请求需要经过Qemu处理后通过系统调用才会转换成Host的IO请求发送给Host的驱动. 虽然共享内存以及半虚拟化接口的通信协议减轻了IO虚拟化的开销, 但是Qemu与内核之间的系统模式切换带来的开销是避免不了的. 

目前Linux内核社区中的vhost就是将用户态的Virt-IO网络设备放在了内核中, 避免系统模式切换以及简化算法逻辑最终达到IO减少延迟以及增大吞吐量的目的. 如下图所示: 

```cpp
                             +-------------------+
                             | +---------+       |
                             | | VirtIO  | Guest |
                             | | Driver  |       |  
                             | +-----+---+       |  
                             +---|---^-----------+  
                             PIO |   |               
                                 |   | vInterrupt             
   ------------------------------|---|--------------
                                 v   |
        +----------+         +--------------+  Host
        | Vhost    |<------->|  KVM.ko      |  kernel
        | net      |         |              |
        +----^-----+         +--------------+
             |               
             |               
         +---v----+          
         | NIC    |          
         | Driver |          
         +--------+ 
```

目前KVM的磁盘虚拟化还是在用户层通过Qemu模拟设备. 我们可以通过vhost框架将磁盘的设备模拟放到内核中达到优化的效果. 

## 7.2. 普通设备的直接分配(Direct Assign)

如前文所述, 目前只有特殊的PCI设备才能直接分配给相应的Guest, 即VMM-bypass, 避免额外的软件开销. 我们可以在KVM中软实现DMA以及中断的remapping功能, 然后将现有的普通设备直接分配给Guest. 如下图所示: 

```
               +----------------+
               |  Guest         |
               |  +---------+   |
     +-------->|  | Driver  |   |
     |         |  +---------+   |
     |         +------------^---+
   D |              |       |    
   M |      DMA Req.|       | vINTR
   A |              |       |
     |      +-------|-------|----------+   
   O |      |       v KVM   |          |  
   p |      |   +------------------+   |  
   e |      |   | DMA remmapping   |   |              
   r |      |   |                  |   |  
   a |      |   | INTR remmapping  |   |              
   t |      |   +-----------^------+   |  
   i |      +-------|-------|----------+  
   o |              |       | INTR        
   n |              v       |             
     |              +---------+           
     +------------->| Deivce  |           
                    +---------+
```

这将大大减少Guest驱动同物理设备之间的路径(省去了KVM的涉入), 去掉了虚拟设备的模拟逻辑, 不过IO性能的提高是以增加KVM的逻辑复杂度的代价换来的. 此时, IO的性能瓶颈从Qemu/KVM转移到物理设备, 但是IO的稳定性、安全性将会更加依赖于KVM的remapping逻辑实现. 

## 7.3. 普通设备的复用

在普通设备的直接分配的基础上, 我们甚至可以在多个Guest之间复用设备, 好比m个进程跑在n个处理器上一样(n < m). 比如将一个硬盘分成多个区, 每一个分区作为一个块设备直接分配给Guest;或者直接将n个网卡分配给m个Guest(n < m). 其中磁盘的复用, 只需在KVM中添加分区管理的逻辑, 而网卡的复用则要复杂一些: KVM需要为设备提供多个设备上下文(每一个设备上下文对应着一个Guest), 同时还需要提供算法逻辑对设备上下文进行切换和调度. 如下图所示: 

```
                        |                  KVM   |
                        |  Device context        |
                        |  queue                 |
           +------+     |     +-+                |
           |Guest |---------->| |                |
           -------+     |     +-+                |
                        |      |                 |
           +------+     |     +-+                |
           |Guest |---------->| |   +----------+ |
           +------+     |     +-+   | Device   | |
                        |      |    | Scheduler| |
           +------+     |     +-+   +----------+ |
           |Guest |---------->| |-----+          |
           +------+     |     +-+     |          |
                        |          +--v--------+ |
                        | Current--->+--+  DM  | |     +-----+
                        | Context  | +--+------------->| NIC |
                        |          +-----------+ |     +-----+
                        |                        |
```

其中, Device Modle(DM)实现前文提到的remapping逻辑, Device Scheduler用于选择和切换设备上下文实现物理设备的复用. 在普通设备直接分配的基础上, 通过对现有普通设备的复用, 将会带来廉价、灵活、高效的IO性能. 与之相对的是, 目前已经有支持SR-IOV的网卡, 从硬件上实现复用功能, 支持多个(静态, 即最大数目是内置的)虚拟的PCI网卡设备, 价格昂贵, 且受到一个网口总带宽有限的限制(软件复用技术, 可以复用多个网卡, 进而提高系统总的带宽). 


# 8. 参考

http://oenhan.com/kvm-src-1

https://www.linuxidc.com/Linux/2015-01/112328.htm

https://www.cnblogs.com/jiayy/p/3762853.html (none)