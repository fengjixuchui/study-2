
# 概述

Linux 内核设备驱动充分利用了 "一切皆文件" 的思想, VFIO 驱动也不例外, VFIO 中为了方便操作 **device**, **group**, **container** 等对象, 将它们和对应的设备文件进行绑定.

VFIO 提供了**两个字符设备文件**作为提供给用户程序的**入口点**, 分别是 `/dev/vfio/vfio` 和 `/dev/vfio/$GROUP`; 此外还在 **sysfs** 中添加了**一些文件**.

首先看 `/dev/vfio/vfio`, 它是一个 misc device, 在 **vfio 模块的初始化**函数 vfio_init 中注册:

```
# ll /dev/vfio/vfio
crw-rw-rw- 1 root root 10, 196 9月   8 02: 37 /dev/vfio/vfio
```

```cpp
// drivers/vfio/vfio_main.c
static struct miscdevice vfio_dev = {
	.minor = VFIO_MINOR,
	.name = "vfio",
	.fops = &vfio_fops,
	.nodename = "vfio/vfio",
	.mode = S_IRUGO | S_IWUGO,
};

static int __init vfio_init(void)
{
    int ret;
    /* ... */
    ret = misc_register(&vfio_dev);
    /* ... */
}
```

每次打开 `/dev/vfio/vfio` 文件, 都会**新创建**一个对应的 container 即 `struct vfio_container`

```cpp
// qemu
s->container = open("/dev/vfio/vfio", O_RDWR);

// drivers/vfio/vfio_main.c
struct vfio_container {
	struct kref			kref;
    // vfio group 链表
	struct list_head		group_list;
	struct rw_semaphore		group_lock;
    // 关联的 iommu_driver, 通过 container 可以进行 IOMMU 相关的操作
	struct vfio_iommu_driver	*iommu_driver;
    // iommu driver 的相关数据
	void				*iommu_data;
	bool				noiommu;
};
```

我们可以将 vfio group 加入到 container 中, container 维护了一个vfio group(`struct vfio_group`)的链表 `group_list`. **container 的作用!!!**就是通过其 `iommu_driver` **为加入的所有 group 提供 IOMMU 的服务**:

```cpp
struct vfio_iommu_driver {
	const struct vfio_iommu_driver_ops	*ops;
	struct list_head			vfio_next;
};
```

> noiommu 用于表示该 container 是否用于存放 no-iommu 的 Group(一个 container 不能同时存放到 no-iommu Group 和普通 Group). no-iommu Group 即背后没有 IOMMU 但仍然强行建立的 vfio group, 这个高级特性(`CONFIG_VFIO_NOIOMMU`)通常不建议开启, 我们忽略相关的代码即可.

`/dev/vfio/$GROUP` 文件显然对应着 vfio group,它的由来要更复杂一些,我们看vfio_init的一段代码来理解:

```cpp
#define MINORBITS	20
#define MINORMASK	((1U << MINORBITS) - 1)

// drivers/vfio/vfio_main.c
static int __init vfio_init(void)
{
    ......
    /* /dev/vfio/$GROUP */
	vfio.class = class_create(THIS_MODULE, "vfio");
	if (IS_ERR(vfio.class)) {
		ret = PTR_ERR(vfio.class);
		goto err_class;
	}

	vfio.class->devnode = vfio_devnode;

	ret = alloc_chrdev_region(&vfio.group_devt, 0, MINORMASK + 1, "vfio");
	if (ret)
		goto err_alloc_chrdev;

    cdev_init(&vfio.group_cdev, &vfio_group_fops);
    ret = cdev_add(&vfio.group_cdev, vfio.group_devt, MINORMASK + 1);
    if (ret)
        goto err_cdev_add;
    ......
}
```

其中 vfio_devnode 函数的定义如下:

```cpp
/*
 * Module/class support
 */
static char *vfio_devnode(struct device *dev, umode_t *mode)
{
	return kasprintf(GFP_KERNEL, "vfio/%s", dev_name(dev));
}
```

这里为 vfio group 字符设备动态分配了一整个 Major(即包含该Major下的所有 Minor)的设备号并注册了cdev, 一旦创建一个带devt的Device, vfio class(`/sys/class/vfio`)下, 就会创建一个 `/dev/vfio/$GROUP` 字符设备文件.

VFIO 分为 VFIO 核心模块和 VFIO 驱动模块, VFIO Group 是由 VFIO 驱动模块创建的, 最常用的是 vfio-pci 驱动. VFIO 驱动是以设备驱动的形式实现, 它们会注册一个 Driver, 并在其 probe 函数中调用 vfio_add_group_dev, 并最终会调用 device_create 为 VFIO Group 创建一个 Device(从而也创建了 `/dev/vfio/$GROUP` 设备文件):

```cpp
    /* vfio_add_group_dev --> vfio_create_group */
    dev = device_create(vfio.class, NULL,
            MKDEV(MAJOR(vfio.group_devt), minor),
            group, "%s%d", group->noiommu ? "noiommu-" : "",
            iommu_group_id(iommu_group));
```

至于上面说的sysfs文件,也是由VFIO驱动创建的,因为它本身就是一个(虚拟)设备驱动,自然可以创建sysfs目录与属性.

# 1. vfio_container

> **container 的作用!!!**就是通过其 `iommu_driver` **为加入的所有 group 提供 IOMMU 的服务** 

vfio container 和 vfio group 不同.

* **vfio container** 只有一个入口点即 `/dev/vfio/vfio`, 每次打开该设备文件, 都将获得一个新的 vfio container 实例.

* **vfio group** 和 `/dev/vfio/$GROUP` **设备文件**绑定, 每个设备文件唯一对应**一个vfio group**, 且**只能打开一次**, 试图第二次打开会返回 `-EBUSY`.

container fd 对应的 ioctl 接口如下:

```cpp
static long vfio_fops_unl_ioctl(struct file *filep,
				unsigned int cmd, unsigned long arg)
{
	struct vfio_container *container = filep->private_data;
	struct vfio_iommu_driver *driver;
	void *data;
	long ret = -EINVAL;

	if (!container)
		return ret;

	switch (cmd) {
	case VFIO_GET_API_VERSION:
		ret = VFIO_API_VERSION;
		break;
	case VFIO_CHECK_EXTENSION:
		ret = vfio_ioctl_check_extension(container, arg);
		break;
	case VFIO_SET_IOMMU:
		ret = vfio_ioctl_set_iommu(container, arg);
		break;
	default:
		driver = container->iommu_driver;
		data = container->iommu_data;
        // 其他 ioctl 都是 iommu_driver 的
		if (driver) /* passthrough all unrecognized ioctls */
			ret = driver->ops->ioctl(data, cmd, arg);
	}

	return ret;
}
```

## container的API

vfio container 本身具备的功能很小, 只有三个 ioctl:

* `VFIO_GET_API_VERSION`, 返回VFIO_API_VERSION(目前版本号为0)

* `VFIO_CHECK_EXTENSION`, 用来检测是否支持特定的扩展, 如支持哪种类型的 IOMMU

* `VFIO_SET_IOMMU`, 设置 **IOMMU Driver** 的类型, 指定的IOMMU必须是通过 `VFIO_CHECK_EXTENSION` 确认驱动支持的. 在调用该 ioctl 前**必须至少挂载一个 vfio group**

    * IOMMU driver 本质上只有**两种类型**, 即 **Type1 IOMMU** 和 **sPAPR IOMMU**, 前者代表 x86,ARM 等架构上的 IOMMU, 后者代表 POWER 架构上的 IOMMU

    * **Type1 IOMMU**, 又细分为 `VFIO_TYPE1_IOMMU`, `VFIO_TYPE1v2_IOMMU` 和 `VFIO_TYPE1_NESTING_IOMMU`, 一般来说用 `VFIO_TYPE1v2_IOMMU` 即可

    * 所有的 type 都可以作为 `VFIO_CHECK_EXTENSION` 的**参数**, 检查内核是否支持该类型, 用户应该**先检查是否支持该类型**再设置 IOMMU Driver

## iommu driver的API

上一节也说过(上面结构体也说明), container 会与 iommu_driver 联系起来, 这样通过 container 可以进行 IOMMU 相关的操作. 以 type1 iommu driver 说明

### 外部接口

vfio container 上的其余操作都会代理给其 IOMMU Driver 执行, 包括read,write,mmap 和上述三个 ioctl 以外的 ioctl.

```cpp
/* vfio_fops_read */
driver = container->iommu_driver;
if (likely(driver && driver->ops->read))
    ret = driver->ops->read(container->iommu_data,
                buf, count, ppos);

/* vfio_fops_write */
driver = container->iommu_driver;
if (likely(driver && driver->ops->write))
    ret = driver->ops->write(container->iommu_data,
                 buf, count, ppos);

/* vfio_fops_mmap */
driver = container->iommu_driver;
if (likely(driver && driver->ops->mmap))
    ret = driver->ops->mmap(container->iommu_data, vma);

/* vfio_fops_unl_ioctl */
default:
    driver = container->iommu_driver;
    data = container->iommu_data;

    if (driver) /* passthrough all unrecognized ioctls */
        ret = driver->ops->ioctl(data, cmd, arg);
```

另外, `VFIO_CHECK_EXTENSION` 实际上也是代理给 IOMMU Driver 执行的, 当 container 尚未指定 Driver 时, 是遍历系统中的 IOMMU Driver 依次调用 `VFIO_CHECK_EXTENSION`, 至少有一个返回 1 则最终返回 1, 否则返回 0, 当 container 指定了 Driver 时, 则对该 Driver 调用 `FIO_CHECK_EXTENSION`.

对于我们关心的Type 1 IOMMU Driver,其提供的重要的 ioctl 实际上有

* `VFIO_IOMMU_GET_INFO`: 用来得到 IOMMU 的一些信息, 这个 ioctl 只针对 Type1 的 IOMMU.

* `VFIO_CHECK_EXTENSION`, 当前 iommu_driver 的检查

* `VFIO_IOMMU_MAP_DMA`: 用来指定**设备端**看到的 **IO 地址**到**进程的虚拟地址**之间的映射, 类似于KVM中的 `KVM_SET_USER_MEMORY_REGION` 指定虚拟机物理地址到进程虚拟地址之间的映射.

会传入一个 `struct vfio_iommu_type1_dma_map`:

```cpp
struct vfio_iommu_type1_dma_map {
    __u32   argsz;
    __u32   flags;
#define VFIO_DMA_MAP_FLAG_READ (1 << 0)     /* readable from device */
#define VFIO_DMA_MAP_FLAG_WRITE (1 << 1)    /* writable from device */
    __u64   vaddr;                          /* Process virtual address */
    __u64   iova;                           /* IO virtual address */
    __u64   size;                           /* Size of mapping (bytes) */
};
```

* `VFIO_IOMMU_UNMAP_DMA`: 传入一个 `struct vfio_iommu_type1_dma_unmap`, 成功 unmap 的内存的 size 会在 size 中返回(可能比传入的 size 小):

```cpp
struct vfio_iommu_type1_dma_unmap {
    __u32   argsz;
    __u32   flags;
    __u64   iova;               /* IO virtual address */
    __u64   size;               /* Size of mapping (bytes) */
};
```

这里设置的 DMA Remapping 是针对整个 container, 即针对其中的所有 Group 的, 下面我们将详细讨论这一点.

### 内部接口

IOMMU Driver 实际上只是一个接口, 用于提供若干回调, 与具体的实现解耦:

```cpp
struct vfio_iommu_driver {
    const struct vfio_iommu_driver_ops  *ops;
    struct list_head                    vfio_next;
};

/**
 * struct vfio_iommu_driver_ops - VFIO IOMMU driver callbacks
 */
struct vfio_iommu_driver_ops {
    char            *name;
    struct module   *owner;
    void            *(*open)(unsigned long arg);
    void            (*release)(void *iommu_data);
    ssize_t         (*read)(void *iommu_data, char __user *buf,
                            size_t count, loff_t *ppos);
    ssize_t         (*write)(void *iommu_data, const char __user *buf,
                             size_t count, loff_t *size);
    long            (*ioctl)(void *iommu_data, unsigned int cmd,
                             unsigned long arg);
    int             (*mmap)(void *iommu_data, struct vm_area_struct *vma);
    int	            (*attach_group)(void *iommu_data,
                                    struct iommu_group *group);
    void            (*detach_group)(void *iommu_data,
                                    struct iommu_group *group);
    int             (*pin_pages)(void *iommu_data, unsigned long *user_pfn,
                                 int npage, int prot,
                                 unsigned long *phys_pfn);
    int             (*unpin_pages)(void *iommu_data,
                                   unsigned long *user_pfn, int npage);
    int             (*register_notifier)(void *iommu_data,
                                         unsigned long *events,
                                         struct notifier_block *nb);
    int             (*unregister_notifier)(void *iommu_data,
                                           struct notifier_block *nb);
};
```

> 目前 IOMMU Driver 均未实现 read, write, mmap回调, 因此对 VFIO container 实际上不能进行read, write 或 mmap 操作, 尽管不排除将来支持这些操作的可能.

在 Type 1 IOMMU Driver 中, 实现了以下接口:

```cpp
static const struct vfio_iommu_driver_ops vfio_iommu_driver_ops_type1 = {
    .name                   = "vfio-iommu-type1",
    .owner                  = THIS_MODULE,
    .open                   = vfio_iommu_type1_open,
    .release                = vfio_iommu_type1_release,
    .ioctl                  = vfio_iommu_type1_ioctl,
    .attach_group           = vfio_iommu_type1_attach_group,
    .detach_group           = vfio_iommu_type1_detach_group,
    .pin_pages              = vfio_iommu_type1_pin_pages,
    .unpin_pages            = vfio_iommu_type1_unpin_pages,
    .register_notifier      = vfio_iommu_type1_register_notifier,
    .unregister_notifier    = vfio_iommu_type1_unregister_notifier,
};
```

### Data Structures

在 vfio_iommu_type1_open 中, 创建了一个 struct vfio_iommu, 存放在 container 的 iommu_data 成员中:

```cpp
struct vfio_iommu {
    struct list_head    domain_list;
    struct vfio_domain  *external_domain; /* domain for external user */
    struct mutex        lock;
    struct rb_root      dma_list;
    struct blocking_notifier_head notifier;
    unsigned int        dma_avail;
    bool                v2;
    bool                nesting;
};
```

其中domain_list是struct vfio_domain构成的链表:

```cpp
struct vfio_domain {
    struct iommu_domain *domain;
    struct list_head    next;
    struct list_head    group_list;
    int                 prot;       /* IOMMU_CACHE */
    bool                fgsp;       /* Fine-grained super pages */
};
```

其中 group_list 又是 struct vfio_group 构成的链表(此VFIO Group非彼VFIO Group,前者定义在 `drivers/vfio/vfio_iommu_type1.c`, 后者定义在 `drivers/vfio/vfio.c`):

```cpp
struct vfio_group {
    struct iommu_group  *iommu_group;
    struct list_head    next;
};
```

这里,一个struct vfio_group和一个VFIO Group相对应,同时也对应于一个IOMMU Group.不同的IOMMU Group可以共享同一张IOMMU页表,我们说这些IOMMU Group属于同一个IOMMU Domain,在这里struct vfio_domain就对应着IOMMU Domain.最后,一个container中可以容纳若干IOMMU Domain,即可以同时管理多个IOMMU页表.external_domain是由VFIO驱动管理的外部IOMMU Domain,可以暂时忽略,分析vfio-mdev时会详细解释.

> 这里忽略同一个IOMMU Group在不同进程中可以对应不同IOMMU页表的情况(例如VT-d以及SMMU都可以根据PASID选取不同页表),这种场景在Linux 5.1 rc6尚未支持.Patchwork上可以找到尚未upstream的patch.

dma_list则是由struct vfio_dma构成的一棵红黑树,其索引是[iova, iova + size]区间(IOMMU Driver保证这些区间不重叠):

```cpp
struct vfio_dma {
    struct rb_node      node;
    dma_addr_t          iova;       /* Device address */
    unsigned long       vaddr;      /* Process virtual addr */
    size_t              size;       /* Map size (bytes) */
    int                 prot;       /* IOMMU_READ/WRITE */
    bool                iommu_mapped;
    bool                lock_cap;   /* capable(CAP_IPC_LOCK) */
    struct task_struct  *task;
    struct rb_root      pfn_list;   /* Ex-user pinned pfn list */
};
```

每个vfio_dma都代表一小段内存映射,而这些映射是作用于container下的所有IOMMU Domain,所有IOMMU Group的,也就是说container下不同IOMMU Domain的页表内容是相同的.不过这仍是有意义的,因为可能加入container的不同VFIO Group,分别被不同的IOMMU管辖,因此必须使用不同的IOMMU Domain.

### Operations

以下均不考虑vfio-mdev驱动的VFIO Group对IOMMU Driver造成的影响,对于vfio-mdev会在专门的文章讨论

我们首先考察vfio_iommu_type1_attach_group(vfio_iommu, iommu_group):

* 第一步,检查vfio_iommu下是否已经有IOMMU Group了,若已存在则立即返回-EINVAL.
* 第二步,从IOMMU Group可以得到其下面的Device(struct device),若它们所属的Bus不同则立即返回-EINVAL,否则记录下它们共同的Bus(记作bus).
* 第三步,调用iommu_domain_alloc(bus)创建一个IOMMU Domain,然后调用iommu_attach_group(iommu_domain, iommu_group)将IOMMU Group加入该Domain.
* 第四步,遍历vfio_iommu的domain_list链表,查找可以容纳IOMMU Group的Domain,若找到则将IOMMU Group从上一步的Domain中去除,加入到这一步的Domain中,并直接返回:

```cpp
/*
 * Try to match an existing compatible domain.  We don't want to
 * preclude an IOMMU driver supporting multiple bus_types and being
 * able to include different bus_types in the same IOMMU domain, so
 * we test whether the domains use the same iommu_ops rather than
 * testing if they're on the same bus_type.
 */
list_for_each_entry(d, &iommu->domain_list, next) {
    if (d->domain->ops == domain->domain->ops &&
        d->prot == domain->prot) {
        iommu_detach_group(domain->domain, iommu_group);
        if (!iommu_attach_group(d->domain, iommu_group)) {
            list_add(&group->next, &d->group_list);
            iommu_domain_free(domain->domain);
            kfree(domain);
            mutex_unlock(&iommu->lock);
            return 0;
        }

        ret = iommu_attach_group(domain->domain, iommu_group);
        if (ret)
            goto out_domain;
    }
}
```

* 否则,要在新建的IOMMU Domain上设置DMA Mapping,即调用vfio_iommu_replay(iommu, domain)重放所有DMA Mapping请求,最后将新Domain加入vfio_iommu的domain_list中.

我们再来考察vfio_iommu_type1_register_notifier和vfio_iommu_type1_unregister_notifier,它们的实现很简单:

https://tcbbd.moe/linux/qemu-kvm/vfio/


# 2. vfio_group

> 以下均以vfio-pci为例进行分析,对于其他VFIO驱动也有参考价值

## 创建

先从 VFIO Group 的创建开始,对于 `vfio-pci`,这是在 `vfio_pci_probe` 中完成的:

```cpp
static int vfio_pci_probe(struct pci_dev *pdev, const struct pci_device_id *id)
{
    struct vfio_pci_device *vdev;
    struct iommu_group *group;
    int ret;

    /* ... */

    ret = vfio_add_group_dev(&pdev->dev, &vfio_pci_ops, vdev);
    if (ret) {
        vfio_iommu_group_put(group, &pdev->dev);
        kfree(vdev);
        return ret;
    }

    /* ... */

    return ret;
}
```

这里创建了一个 `vfio_pci_device` 对象 vdev,并使用 VFIO Core 提供的 `vfio_add_group_dev` 创建了一个 VFIO Group.下面详细分析 `vfio_add_group_dev` 创建的数据结构.

首先, VFIO Core有一个全局变量 vfio:

```cpp
static struct vfio {
    struct class            *class;
    struct list_head        iommu_drivers_list;
    struct mutex            iommu_drivers_lock;
    struct list_head        group_list;
    struct idr              group_idr;
    struct mutex            group_lock;
    struct cdev             group_cdev;
    dev_t                   group_devt;
    wait_queue_head_t       release_q;
} vfio;
```

其中 `group_list` 是所有 VFIO Group 构成的链表, `group_idr` 是由 VFIO Group 的 Minor 号构成的一棵 Radix Tree.

再来看 VFIO Group, 每个 VFIO Group 都是和一个 IOMMU Group 相对应的:

```cpp
struct vfio_group {
    struct kref                     kref;
    int                             minor;
    atomic_t                        container_users;
    struct iommu_group              *iommu_group;
    struct vfio_container           *container;
    struct list_head                device_list;
    struct mutex                    device_lock;
    struct device                   *dev;
    struct notifier_block           nb;
    struct list_head                vfio_next;
    struct list_head                container_next;
    struct list_head                unbound_list;
    struct mutex                    unbound_lock;
    atomic_t                        opened;
    wait_queue_head_t               container_q;
    bool                            noiommu;
    struct kvm                      *kvm;
    struct blocking_notifier_head   notifier;
};
```

> 一个 IOMMU Group 代表一组设备, 在硬件上无法区分它们的 ID(例如它们都在 PCIe-PCI Bridge 后面), 因此只能共用一张 IOMMU 页表.

VFIO Group 的 dev会指向 `/dev/vfio/$GROUP` 对应的 Device, 和 `vfio_add_group_dev` 传入的 Device 无关. 由于 VFIO Group 和 IOMMU Group 是一一对应关系, 一个 Group 下可以有多个 VFIO Device, VFIO Group 通过 device_list 链表引用这些 VFIO Device. VFIO Device 的定义如下:

```cpp
struct vfio_device {
    struct kref                     kref;
    struct device                   *dev;
    const struct vfio_device_ops    *ops;
    struct vfio_group               *group;
    struct list_head                group_next;
    void                            *device_data;
};
```

我们向 `vfio_add_group_dev` 传入的 `pdev->dev` 被放入了 `vfio_device->dev`, `vfio_pci_ops` 被放入了 `vfio_device->ops`, vdev 则放入了 `vfio_device->device_data`.

下面分析 `vfio_add_group_dev(dev, ops, device_data)` 函数, 该函数的目的实际上是创建一个 VFIO Device, 并加入相应的 VFIO Group:

第一步,通过 dev(即 VFIO Device 背后的设备)获得 IOMMU Group

```cpp
iommu_group = iommu_group_get(dev);
if (!iommu_group)
    return -EINVAL;
```

第二步, 在全局变量 vfio 的 VFIO Group 链表中寻找匹配的 Group, 若找不到则创建一个新的, 并令其 iommu_group 指向上面获得的 IOMMU Group. 创建 VFIO Group 在 `vfio_create_group` 中完成, 其中这段代码值得注意:

```cpp
group->nb.notifier_call = vfio_iommu_group_notifier;

/*
 * blocking notifiers acquire a rwsem around registering and hold
 * it around callback.  Therefore, need to register outside of
 * vfio.group_lock to avoid A-B/B-A contention.  Our callback won't
 * do anything unless it can find the group in vfio.group_list, so
 * no harm in registering early.
 */
ret = iommu_group_register_notifier(iommu_group, &group->nb);
if (ret) {
    kfree(group);
    return ERR_PTR(ret);
}
```

这里向内核的 IOMMU 层注册了回调, 当 `IOMMU Group` 上发生一些事件时, 会通知 VFIO 层执行 `vfio_iommu_group_notifier`.

最后一步, 创建 VFIO Device. 我们首先调用 vfio_group_get_device(group, dev), 如果发现 VFIO Group 下已有对应的 VFIO Device 则返回 `-EBUSY`. 然后调用 `vfio_group_create_device(group, dev, ops, device_data)`:

```cpp
static
struct vfio_device *vfio_group_create_device(struct vfio_group *group,
                                             struct device *dev,
                                             const struct vfio_device_ops *ops,
                                             void *device_data)
{
    struct vfio_device *device;

    device = kzalloc(sizeof(*device), GFP_KERNEL);
    if (!device)
        return ERR_PTR(-ENOMEM);

    kref_init(&device->kref);
    device->dev = dev;
    device->group = group;
    device->ops = ops;
    device->device_data = device_data;
    dev_set_drvdata(dev, device);

    /* No need to get group_lock, caller has group reference */
    vfio_group_get(group);

    mutex_lock(&group->device_lock);
    list_add(&device->group_next, &group->device_list);
    mutex_unlock(&group->device_lock);

    return device;
}
```

## group的API

我们首先来看 `/dev/vfio/$GROUP` 提供的API, 该文件只支持 ioctl 操作:

```cpp
static const struct file_operations vfio_group_fops = {
    .owner              = THIS_MODULE,
    .unlocked_ioctl     = vfio_group_fops_unl_ioctl,
#ifdef CONFIG_COMPAT
    .compat_ioctl       = vfio_group_fops_compat_ioctl,
#endif
    .open               = vfio_group_fops_open,
    .release            = vfio_group_fops_release,
};
```

在 open 时, 会利用 Minor 号从 `vfio.group_idr` 中找到对应的 VIFO Group, 然后将文件的 `private_data` 设置为该 VFIO Group:

```cpp
group = vfio_group_get_from_minor(iminor(inode));
if (!group)
    return -ENODEV;

filep->private_data = group;
```

VFIO Group 只有 4 个 ioctl, 分别是:

* VFIO_GROUP_GET_STATUS, &status: 获取一个 struct vfio_group_status 表示 VFIO Group 的状态
* VFIO_GROUP_SET_CONTAINER, fd: 传入一个 fd 表示 VFIO container, 将 VFIO Group 加入该 container
* VFIO_GROUP_UNSET_CONTAINER: 将 VFIO Group 移出 container
* VFIO_GROUP_GET_DEVICE_FD, str: 传入一个字符串表示 VFIO Group 下的 Device, 获取该 Device 对应的 fd

实际上, vfio_group_status 只包含一个 flag, 为其定义了两个位 `VFIO_GROUP_FLAGS_VIABLE` 和 `VFIO_GROUP_FLAGS_CONTAINER_SET`, 后者显然表示 VFIO Group 是否绑定到了某个 container, Viable 的含义可参考 `vfio_dev_viable` 函数的注释:

```cpp
/*
 * A vfio group is viable for use by userspace if all devices are in
 * one of the following states:
 *  - driver-less
 *  - bound to a vfio driver
 *  - bound to a whitelisted driver
 *  - a PCI interconnect device
 *
 * We use two methods to determine whether a device is bound to a vfio
 * driver.  The first is to test whether the device exists in the vfio
 * group.  The second is to test if the device exists on the group
 * unbound_list, indicating it's in the middle of transitioning from
 * a vfio driver to driver-less.
 */
```

`VFIO_GROUP_SET_CONTAINER` 调用了 container 的 IOMMU Driver 的 attach_group 方法, 来将 Group 加入 container:

```cpp
driver = container->iommu_driver;
if (driver) {
    ret = driver->ops->attach_group(container->iommu_data,
                    group->iommu_group);
    if (ret)
        goto unlock_out;
}
```

类似地, VFIO_GROUP_UNSET_CONTAINER 调用了 IOMMU Driver 的 detach_group 方法:

```cpp
driver = container->iommu_driver;
if (driver)
    driver->ops->detach_group(container->iommu_data,
                  group->iommu_group);
```

VFIO_GROUP_GET_DEVICE_FD首先调用了VFIO Device的 open 方法:

```cpp
ret = device->ops->open(device->device_data);
if (ret) {
    vfio_device_put(device);
    return ret;
}
```

对于 vfio-pci 就是 vfio_pci_open, 该函数主要对传入的 vfio_pci_device 对象作了初始化, 初始化的过程依据了 vdev 背后的 pdev 的 Configuration Space.

随后, 为 VFIO Device 创建了一个 Anonymous Inode, 即不存在于任何目录下的游离于文件系统之外的孤儿 Inode, 并返回了其 fd:

```cpp
/*
 * We can't use anon_inode_getfd() because we need to modify
 * the f_mode flags directly to allow more than just ioctls
 */
ret = get_unused_fd_flags(O_CLOEXEC);
if (ret < 0) {
    device->ops->release(device->device_data);
    vfio_device_put(device);
    return ret;
}

filep = anon_inode_getfile("[vfio-device]", &vfio_device_fops,
               device, O_RDWR);
if (IS_ERR(filep)) {
    put_unused_fd(ret);
    ret = PTR_ERR(filep);
    device->ops->release(device->device_data);
    vfio_device_put(device);
    return ret;
}

/*
 * TODO: add an anon_inode interface to do this.
 * Appears to be missing by lack of need rather than
 * explicitly prevented.  Now there's need.
 */
filep->f_mode |= (FMODE_LSEEK | FMODE_PREAD | FMODE_PWRITE);

atomic_inc(&group->container_users);

fd_install(ret, filep);
```

## device的API

上一节中的vfio_device_fops实际上只是VFIO Device的ops的一个Wrapper,它将对VFIO Device fd的read,write, mmap 和 ioctl 代理给device->ops中的回调.

对于不同的 VFIO 驱动, read, write, mmap 的含义各有不同, 不过总的来说是将 VFIO 设备文件分为若干个 Region, 例如 PIO Region, MMIO Region, PCI Configuration Space 等, 每个 Region 位于 VFIO 设备文件的不同 offset 并分别可以读写和映射. 另外, 每个 VFIO 设备还可以有一个或多个 IRQ Space, 用于提供中断的模拟. 下面看一下相关的 ioctl:

`VFIO_DEVICE_GET_INFO`, `&info`, 获取一个 `struct vfio_device_info`, 表明 VFIO Device 的信息:

```cpp
struct vfio_device_info {
    __u32   argsz;
    __u32   flags;
#define VFIO_DEVICE_FLAGS_RESET (1 << 0)    /* Device supports reset */
#define VFIO_DEVICE_FLAGS_PCI   (1 << 1)    /* vfio-pci device */
#define VFIO_DEVICE_FLAGS_PLATFORM (1 << 2) /* vfio-platform device */
#define VFIO_DEVICE_FLAGS_AMBA  (1 << 3)    /* vfio-amba device */
#define VFIO_DEVICE_FLAGS_CCW   (1 << 4)    /* vfio-ccw device */
#define VFIO_DEVICE_FLAGS_AP    (1 << 5)    /* vfio-ap device */
    __u32   num_regions;    /* Max region index + 1 */
    __u32   num_irqs;       /* Max IRQ index + 1 */
};
```

提供的信息包括 VFIO Device 由哪种驱动提供(`vfio-mdev`设备则模拟其中一种), 有几个Region, 有几个 IRQ Space.

`VFIO_DEVICE_GET_REGION_INFO`, `&info`, 用于进一步查询 Region 的信息, 传入并返回一个 `struct vfio_region_info`(用户只填写index):

```cpp
struct vfio_region_info {
    __u32   argsz;
    __u32   flags;
#define VFIO_REGION_INFO_FLAG_READ  (1 << 0) /* Region supports read */
#define VFIO_REGION_INFO_FLAG_WRITE (1 << 1) /* Region supports write */
#define VFIO_REGION_INFO_FLAG_MMAP  (1 << 2) /* Region supports mmap */
#define VFIO_REGION_INFO_FLAG_CAPS  (1 << 3) /* Info supports caps */
    __u32   index;      /* Region index */
    __u32   cap_offset; /* Offset within info struct of first cap */
    __u64   size;       /* Region size (bytes) */
    __u64   offset;     /* Region offset from start of device fd */
};
```

`VFIO_DEVICE_GET_IRQ_INFO`, `&info`, 用于查询 IRQ Space 的信息, 传入并返回一个 `struct vfio_irq_info`(用户只填写 index):

```cpp
struct vfio_irq_info {
    __u32   argsz;
    __u32   flags;
#define VFIO_IRQ_INFO_EVENTFD       (1 << 0)
#define VFIO_IRQ_INFO_MASKABLE      (1 << 1)
#define VFIO_IRQ_INFO_AUTOMASKED    (1 << 2)
#define VFIO_IRQ_INFO_NORESIZE      (1 << 3)
    __u32   index;      /* IRQ index */
    __u32   count;      /* Number of IRQs within this index */
};
```

count 表示这个 IRQ Space 中的 IRQ 数量, 例如某个 IRQ Space 代表 MSI-X 中断, 那么它最多可以有 2048 个IRQ. EVENTFD flag 表示 IRQ Space 支持 eventfd 方式报告中断, MASKABLE flag 表示可以对其中的 IRQ 进行 mask 和 unmask 操作, AUTOMASKED 表示当 IRQ 上触发一次中断后, IRQ 会自动被 mask.

`VFIO_DEVICE_SET_IRQS`, `&irq_set`, 传入一个`struct vfio_irq_set` 用于配置中断:

```cpp
struct vfio_irq_set {
    __u32   argsz;
    __u32   flags;
#define VFIO_IRQ_SET_DATA_NONE      (1 << 0) /* Data not present */
#define VFIO_IRQ_SET_DATA_BOOL      (1 << 1) /* Data is bool (u8) */
#define VFIO_IRQ_SET_DATA_EVENTFD   (1 << 2) /* Data is eventfd (s32) */
#define VFIO_IRQ_SET_ACTION_MASK    (1 << 3) /* Mask interrupt */
#define VFIO_IRQ_SET_ACTION_UNMASK  (1 << 4) /* Unmask interrupt */
#define VFIO_IRQ_SET_ACTION_TRIGGER (1 << 5) /* Trigger interrupt */
    __u32   index;
    __u32   start;
    __u32   count;
    __u8    data[];
};
```

其中 index 表示选择第几个 IRQ Space, start 和 count 用于表示 subindex 的范围. 关于 flags 中 DATA 和 ACTION 的组合, 如下所示:

* `ACTION_MASK` 和 `ACTION_UNMASK` 分别表示屏蔽和启用选中的 IRQ

    * `DATA_NONE` 表示 `[start, start + count - 1]` 范围内的 IRQ 全部选中

    * `DATA_BOOL` 表示 `data[]` 为一个 bool 数组,其成员依次代表 start 到 `start + count - 1` 是否选中

* `ACTION_TRIGGER`

  * 首先需使用 `DATA_EVENTFD`, 通过 `data[]` 传入一个 eventfd 数组, 其成员注册为相应的 IRQ 的 Trigger(`-1` 代表相应的 IRQ 不设置 Trigger), 即当VFIO Device上产生一个中断时, 内核通过注册的 eventfd 通知用户程序.

    * 一旦注册过了 eventfd, 就可以用 `DATA_NONE` 或 `DATA_BOOL` 手动为选中的 IRQ 触发一个虚拟中断

VFIO_DEVICE_RESET, 重置 VFIO Device.




https://tcbbd.moe/linux/qemu-kvm/vfio/#VFIO-Group








当我们把一个设备直通给虚拟机时, 首先要做的就是将这个设备从 host 上进行解绑, 即**解除 host 上此设备的驱动**, 然后**将设备驱动绑定**为 "`vfio-pci`", 在完成绑定后会新增一个 `/dev/vfio/$groupid` 的文件, 其中 `$groupid` 为此 PCI 设备的 **iommu group id**, 这个 id 号是在操作系统加载 iommu driver 遍历扫描 host 上的 PCI 设备的时候就已经分配好的, 可以使用 `readlink -f /sys/bus/pci/devices/$bdf/iommu_group` 来查询.

类似的, `/dev/vfio/$groupid` 这个文件的句柄被关联到 **vfio_group** 上, **用户态进程**打开这个文件就可以**管理这个 iommu group 里的设备**.

```
# ll /dev/vfio/
total 0
drwxr-xr-x  2 root root       80 9月   8 02: 47 ./
drwxr-xr-x 18 root root     4580 9月   8 02: 47 ../
crw-------  1 root root 244,   0 9月   8 02: 47 21
crw-rw-rw-  1 root root  10, 196 9月   8 02: 37 vfio

# ll /sys/bus/pci/devices/0000\: 01\: 00.0/iommu_group
lrwxrwxrwx 1 root root 0 9月   8 02: 37 /sys/bus/pci/devices/0000: 01: 00.0/iommu_group -> ../. ./. ./. ./kernel/iommu_groups/21/
# readlink -f /sys/bus/pci/devices/0000\: 01\: 00.0/iommu_group
/sys/kernel/iommu_groups/21
```

# 3. device

然而 VFIO 中并**没有**为**每个 device** 单独创建一个文件, 而是通过 `VFIO_GROUP_GET_DEVICE_FD` 来调用这个 group ioctl 来**获取 device 的句柄**, 然后再通过这个句柄来管理设备.

> VFIO_GROUP_GET_DEVICE_FD: vfio 从 group->device_list 中查找device, 并通过匿名node和fd建立关系

# 4. vfio_iommu_driver

VFIO 框架中很重要的一部分是要完成 **DMA Remapping**, 即为 **Domain** 创建对应的 **IOMMU 页表**, 这个部分是由 `vfio_iommu_driver` 来完成的.

**vfio_container** 包含一个指针记录 **vfio_iommu_driver** 的信息, 在 x86 上 `vfio_iommu_driver` 的具体实现是由 **vfio_iommu_type1** 模块来完成的.

```cpp
struct vfio_container {
        struct kref                     kref;
        struct list_head                group_list;
        struct rw_semaphore             group_lock;
        struct vfio_iommu_driver        *iommu_driver;
        void                            *iommu_data;
        bool                            noiommu;
};
```

```
│ Symbol: VFIO_IOMMU_TYPE1 [=y]
│ Type  : tristate
│ Defined at drivers/vfio/Kconfig:2
│   Depends on: VFIO [=y]
│ Selected by [y]:
│   - VFIO [=y] && MMU [=y] && (X86 [=y] || S390 || ARM || ARM64)
```

**vfio_iommu_type1** 模块包含了 `vfio_iommu`, `vfio_domain`, `vfio_group`, `vfio_dma` 等关键数据结构.

```
// drivers/vfio/vfio_iommu_type1.c
vfio_iommu : struct
vfio_domain : struct
vfio_dma : struct
vfio_batch : struct
vfio_iommu_group : struct
vfio_iova : struct
vfio_pfn : struct
vfio_regions : struct
```

* **vfio_iommu** 可以认为是和 **container** 概念相对应的 iommu 数据结构, 在虚拟化场景下**每个虚拟机的物理地址空间**映射到一个 `vfio_iommu` 上.

* **vfio_group** 可以认为是和 **group** 概念对应的 iommu 数据结构, 它指向一个 `iommu_group` 对象, 记录了着 `iommu_group` 的信息.

* **vfio_domain** 这个概念尤其需要注意, 这里绝**不能**把它理解成**一个虚拟机 domain**, 它是一个与 **DRHD**(即 IOMMU 硬件)相关的概念, 它的出现就是**为了应对多 IOMMU 硬件的场景**, 我们知道在大规格服务器上可能会有**多个 IOMMU 硬件**, 不同的 IOMMU **硬件有可能存在差异**, 例如 IOMMU 0 支持 `IOMMU_CACHE`, 而 IOMMU 1 不支持 IOMMU_CACHE(当然这种情况少见, 大部分平台上硬件功能是具备一致性的), 这时候我们**不能**直接将分别**属于不同 IOMMU 硬件管理的设备**直接加入到**一个 container** 中, 因为它们的 IOMMU 页表 SNP bit 是不一致的. 因此, 一种合理的解决办法就是把**一个 container** 划分**多个 vfio_domain**, 当然在大多数情况下我们只需要一个 vfio_domain 就足够了. 处在**同一个 vfio_domain 中的设备共享 IOMMU 页表区域**, 不同的 vfio_domain 的页表属性又可以不一致, 这样我们就可以支持跨 IOMMU 硬件的设备直通的混合场景.

```
# ll /sys/class/iommu/
total 0
drwxr-xr-x  2 root root 0 Sep 19 09: 09 ./
drwxr-xr-x 76 root root 0 Sep 19 09: 09 ../
lrwxrwxrwx  1 root root 0 Sep 19 09: 09 dmar0 -> ../. ./devices/virtual/iommu/dmar0/
lrwxrwxrwx  1 root root 0 Sep 19 09: 09 dmar1 -> ../. ./devices/virtual/iommu/dmar1/
```

# 5. 小结

经过上面的介绍和分析, 我们可以把 VFIO 各个组件直接的关系用下图表示.

![2022-08-18-16-02-53.png](./images/2022-08-18-16-02-53.png)

![2022-08-16-22-08-04.png](./images/2022-08-16-22-08-04.png)

对这些的用户态代码可以看 qemu 或 kvmtool(更简单)


