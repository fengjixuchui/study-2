
<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [1. 架构](#1-架构)
- [2. IOMMU](#2-iommu)
- [相关配置](#相关配置)
  - [enable vt-d](#enable-vt-d)
  - [intel_iommu=on](#intel_iommuon)
  - [iommu=pt](#iommupt)
- [IOMMU初始化](#iommu初始化)
  - [BIOS](#bios)
- [DMAR ACPI Table结构](#dmar-acpi-table结构)
  - [DMA Remapping Reporting Structure](#dma-remapping-reporting-structure)
  - [Remapping Structure Types](#remapping-structure-types)
  - [DRHD（DMA Remapping Hardware Unit Definition）表](#drhddma-remapping-hardware-unit-definition表)
  - [](#)
- [3. IOMMU 驱动初始化](#3-iommu-驱动初始化)
- [4. 设备分组](#4-设备分组)
- [5. IOMMU 的作用和效果](#5-iommu-的作用和效果)
- [6. reference](#6-reference)

<!-- /code_chunk_output -->

# 1. 架构

IOMMU 驱动所处的位置位于 DMA 驱动之上, 其上又封装了一层 VFIO 驱动框架, 便于用户空间编写设备驱动直接调用底层的api操作设备.

```
                   +-------------+
                   | QEMU/CROSVM |
                   +-------------+
                          |
        .-----------------|--------------.
........|.................|..............|..........
        |                 |              |
 +---------------+  +-----------+  +-----------+
 | container API |  | group API |  | device API |
 +---------------+  +-----------+  +-----------+
        |                 |              |
        '-----------------|--------------'
                          |
                          v
                   +-------------+
                   | VFIO driver |
                   +-------------+
                          |
                          v
                .---------'---------.
                |                   |
                v                   v
         +-------------+     +--------------+
         | SMMU driver |     | IOMMU driver |
         +-------------+     +--------------+
	        |                   |
                '-------------------'
                          |
                          v
                    +------------+
                    | DMA driver |
                    +------------+
                          |
     .--------------------'--------------------.
     |             |             |             |
     v             v             v             v
+----------+  +----------+  +----------+  +----------+
|  Device  |  |  Device  |  |  Device  |  |  Device  |
+----------+  +----------+  +----------+  +----------+
```

![2022-08-14-00-59-42.png](./images/2022-08-14-00-59-42.png)

IOMMU 框架可分为如下层次:

1. DMA-MAP 层: 提供对其他驱动 DMA MAP/UNMAP 的 API 接口;
2. IOVA 层: 提供 IOVA 的分配与释放, 并提供 RCACHE 缓存机制;
3. IO 页表层: 提供对 IO 设备页表的操作;
4. SMMU 驱动层: 提供 SMMU 驱动初始化,提供IOMMU对应的回调函数;
5. 其它 API 接口: 可提供对 VFIO 等 API 接口;

# 2. IOMMU

类同于 MMU, 对 DMA 做地址翻译, 用来解决 DMA 的安全性问题. DMA API 同时还肩负起了**设置 IOMMU** 的职责.

```cpp
dma_ops = &intel_dma_ops;
i40e_alloc_mapped_page
  └─dma_map_page
      └─intel_map_page
          └─__intel_map_single
              ├─if(iommu_no_mapping) return paddr;
              ├─intel_alloc_iova
              └─domain_pfn_mapping
```

内核用 IOMMU 的好处, **限制了设备 DMA 可写的范围**, 设备不能随便从物理内存读写了.

其实 IOMMU 更大的用处在于**用户态驱动**, 如 DPDK 和 qemu, 用于 **qemu passthrough** 能更好的理解 IOMMU 的作用. **guest** 发起 **DMA** 时设置的地址是 `guest_phy_addr`, qemu 拿到 **guest DMA 的内存段开始地址** `guest_dma_phy_addr_start` 转换成自己的 `host_dma_virt_addr`, 然后把两个地址和 DMA 段的长度 len 通知 vfio 建立 map, vfio 找从 `host_dma_phy_addr` 开始的 len 长度的连续物理内存, host_dma_virt_addr 映射到 `host_dma_phy_addr`, 然后 pin 住, 让qemu的虚拟地址始终有物理内存对应并且对应到固定物理内存. vfio 再给 iommu 建立表项, 如果 DMA 要和 `guest_phy_addr` 交换数据, 就和 `host_dma_phy_addr` 交换数据, iommu 中有个 iova, 其实这个iova在这儿就是 `guest_phy_addr`. dpdk中有–iova, 相比较于 qemu 这儿就是小菜一碟.

kvm device passthrough 老方法

```cpp
kvm_iommu_map_pages
  └─iommu_map
      └─intel_iommu_map(domain->ops->map)
          └─domain_pfn_mapping
```

qemu 用 vfio 实现 device passthrough 新方法

```cpp
vfio_iommu_type1_ioctl
  └─vfio_dma_do_map
      └─vfio_pin_map_dma
          └─vfio_iommu_map
              └─iommu_map
                  └─intel_iommu_map(domain->ops->map)
                     └─domain_pfn_mapping
```

下面我们将对其原理进行分析, 主要参考资料是 Intel VT-d SPEC Chapter 3.


# 相关配置

BIOS 中 enable vt-d，内核参数配置 intel_iommu=on iommu=pt

## enable vt-d

意思很明确，BIOS 收集IOMMU硬件相关的信息以及它和PCI设备连接关系的信息，通过ACPI的表上报给操作系统.

## intel_iommu=on

用 intel_iommu 驱动来驱动 IOMMU 硬件单元，IOMMU 硬件有 intel/amd/arm 的等，我们一般用intel的硬件，当然用intel的iommu驱动了。

## iommu=pt

我们先看内核中关于这个配置的注释

```
/*
 * This variable becomes 1 if iommu=pt is passed on the kernel command line.
 * If this variable is 1, IOMMU implementations do no DMA translation for
 * devices and allow every device to access to whole physical memory. This is
 * useful if a user wants to use an IOMMU only for KVM device assignment to
 * guests and not for driver dma translation.
 */
```

光看注释理解还不太深刻，下面分析一下代码加深理解。

早期 kvm pci passthrough 的实现，qemu用到的参数是 `-device pci-assign,host=01:00.0`，kvm 最终调用了iommu 的代码 domain_pfn_mapping。

```cpp
kvm_iommu_map_pages
  └─iommu_map
      └─intel_iommu_map(domain->ops->map)
          └─domain_pfn_mapping
```

kvm 也用了 vfio-pci，qemu 参数 `-device vfio-pci,host=b1:00.0`，最终也调用了 iommu 的代码 domain_pfn_mapping。

```cpp
vfio_iommu_type1_ioctl
  └─vfio_dma_do_map
      └─vfio_pin_map_dma
          └─vfio_iommu_map
              └─iommu_map
                  └─intel_iommu_map(domain->ops->map)
                     └─domain_pfn_mapping
```

我们再看内核i40e的代码，它也调用到了domain_pfn_mapping。

```cpp
i40e_alloc_mapped_page
  └─dma_map_page
      └─intel_map_page
          └─__intel_map_single
              ├─if(iommu_no_mapping) return paddr;
              ├─intel_alloc_iova
              └─domain_pfn_mapping
```

发现 `kvm/vfio/i40e` 都调用到了函数 `domain_pfn_mapping`，相比于 kvm 和 vfio，i40e 多了一个if判断，条件是函数 `iommu_no_mapping` 的返回值。

```cpp
/* Check if the dev needs to go through non-identity map and unmap process.*/
static int iommu_no_mapping(struct device *dev)
{
	int found;

	if (iommu_dummy(dev))
		return 1;

	if (!iommu_identity_mapping)
		return 0;

	found = identity_mapping(dev);
	if (found) {
		if (iommu_should_identity_map(dev, 0))
			return 1;
		else {
			/*
			 * 32 bit DMA is removed from si_domain and fall back
			 * to non-identity mapping.
			 */
			dmar_remove_one_dev_info(si_domain, dev);
			pr_info("32bit %s uses non-identity mapping\n",
				dev_name(dev));
			return 0;
		}
	} else {
		/*
		 * In case of a detached 64 bit DMA device from vm, the device
		 * is put into si_domain for identity mapping.
		 */
		if (iommu_should_identity_map(dev, 0)) {
			int ret;
			ret = domain_add_dev_info(si_domain, dev);
			if (!ret) {
				pr_info("64bit %s uses identity mapping\n",
					dev_name(dev));
				return 1;
			}
		}
	}

	return 0;
}
```

再回头看 iommu 初始化的代码

```cpp
static int __init init_dmars(void)
{
	if (iommu_pass_through)
		iommu_identity_mapping |= IDENTMAP_ALL;
	if (iommu_identity_mapping) {
		ret = si_domain_init(hw_pass_through);
		if (ret)
			goto free_iommu;
	}

	if (iommu_identity_mapping) {
		ret = iommu_prepare_static_identity_mapping(hw_pass_through);
		if (ret) {
			pr_crit("Failed to setup IOMMU pass-through\n");
			goto free_iommu;
		}
	}
}
```

总结起来就是

配置了 `iommu=pt` 就 identity mapping

```cpp
if hw_pass_through==0
   hardware identity mapping
else
   software identity mapping
```

说明配置了 `iommu=pt` 上面的函数 `iommu_no_mapping` 返回 1，那么 i40e 驱动就直接 return paddr，并不会真正调用到 domain_pfn_mapping，直接用了物理地址少了一次映射性能当然会高一些。



# IOMMU初始化

## BIOS

BIOS 收集 IOMMU 相关的信息，通过 ACPI 中的特定表组织数据，放置在内存中，等操作系统接管硬件后，它会加载驱动，驱动再详细解析 ACPI 表中的信息。








```cpp
IOMMU_INIT_POST(detect_intel_iommu);

int __init detect_intel_iommu(void)
{
	struct dmar_res_callback validate_drhd_cb = {
		.cb[ACPI_DMAR_TYPE_HARDWARE_UNIT] = &dmar_validate_one_drhd,
		.ignore_unhandled = true,
	};
	......

    down_write(&dmar_global_lock);
    ret = dmar_table_detect();
	......

    if (!ret) {
        x86_init.iommu.iommu_init = intel_iommu_init;
        x86_platform.iommu_shutdown = intel_iommu_shutdown;
    }

    if (dmar_tbl) {
        acpi_put_table(dmar_tbl);
        dmar_tbl = NULL;
    }
    up_write(&dmar_global_lock);

    return ret ? ret : 1;
}
```

内核启动后从 **ACPI** 中获取 **DMAR table**, 解析 ACPI 表中两项: `DRHD, DMA Engine Reporting Structure` 和 `RMRR, Reserved memory Region Reporting Structure`. 调用 `detect_intel_iommu`, 它只检测了类型为 `ACPI_DMAR_TYPE_HARDWARE_UNIT` 的数据, 也就是 **IOMMU 硬件单元**, 还尝试读了 IOMMU 硬件的 capability 和 extended capability, 如果都成功给 `iommu_init` 赋值 `intel_iommu_init`.

```cpp
struct acpi_table_header * __initdata dmar_tbl;
static acpi_size dmar_tbl_size;
start_kernel
  └─mm_init
      └─mem_init
          └─detect_intel_iommu
              ├─dmar_table_detect
              ├─dmar_walk_dmar_table (解析ACPI表中两项DRHD和RMRR)
              ├─pci_request_acs
              └─x86_init.iommu.iommu_init = intel_iommu_init
```

`intel_iommu_init` 横空出世, 那就得看什么地方调用到它. `detect_intel_iommu` 执行时还没有 memory allocator, 所以干的活很简单, 但 `intel_iommu_init` 执行时 memory allocator已经形成, 所以 `intel_iommu_init` 就大量分配内存建立 iommu 的数据结构, 主要是 `struct dmar_drhd_unit` 和 `struct intel_iommu`.

```cpp
struct list_head dmar_drhd_units;
static LIST_HEAD(dmar_rmrr_units);
pci_iommu_init (arch/ia64/kernel/pci-dma.c or arch/x86/kernel/pci-dma.c)
  └─x86_init.iommu.iommu_init (drivers/iommu/intel/iommu.c)
      ├─iommu_init_mempool (drivers/iommu/intel/iommu.c)
      ├─dmar_table_init (drivers/iommu/intel/dmar.c) (dmesg打印类似信息：ACPI DMAR:DRHD (flags: 0x00000000)base: 0x00000000fed90000)
      |   └─parse_dmar_table (drivers/iommu/intel/dmar.c)
      |       ├─dmar_table_detect
      |       └─dmar_walk_dmar_table
      |           └─dmar_walk_remapping_entries
      |               ├─dmar_parse_one_drhd
      |               |   ├─dmar_find_dmaru
      |               |   ├─dmar_alloc_dev_scope//分配好多空内存
      |               |   ├─alloc_iommu
      |               |   ├─map_iommu
      |               |   └─dmar_register_drhd_unit
      |               ├─dmar_parse_one_rmrr (drivers/iommu/intel/iommu.c)
      |               ├─dmar_parse_one_atsr (drivers/iommu/intel/iommu.c)
      |               ├─dmar_parse_one_rhsa (drivers/iommu/intel/dmar.c)
      |               └─dmar_parse_one_andd (drivers/iommu/intel/dmar.c)
      ├─dmar_dev_scope_init (drivers/iommu/intel/dmar.c)
      |   ├─dmar_acpi_dev_scope_init (drivers/iommu/intel/dmar.c)
      |   |     └─for(dev_scope_num in acpi table)
      |   |           ├─acpi_bus_get_device (drivers/acpi/scan.c)
      |   |           └─dmar_acpi_insert_dev_scope  (drivers/iommu/intel/dmar.c)   //给上面注释中指的内存空间中写dev/bus
      |   └─for_each_pci_dev(dev) (drivers/iommu/intel/dmar.c) //把非acpi上报的dev上搞到iommu中来
      |       ├─dmar_alloc_pci_notify_info (drivers/iommu/intel/dmar.c)
      |       ├─dmar_pci_bus_add_dev (drivers/iommu/intel/dmar.c)
      |       |   ├─for_each_drhd_unit//找到dev的hrhd然后加入
      |       |   |    └─dmar_insert_dev_scope (drivers/iommu/intel/dmar.c)
      |       |   ├─dmar_iommu_notify_scope_dev (drivers/iommu/intel/iommu.c)
      |       |   └─intel_irq_remap_add_device (drivers/iommu/intel/irq_remapping.c)
      |       └─dmar_free_pci_notify_info (drivers/iommu/intel/dmar.c)
      ├─init_no_remapping_devices (drivers/iommu/intel/iommu.c) (Most GFX drivers don't call standard PCI DMA APIs to allocate DMA buffer,Such drivers will be broken with IOMMU enabled.)
      |   ├─for_each_drhd_unit (include/linux/dmar.h)
      |   |   └─for_each_active_dev_scope (include/linux/dmar.h)
      |   ├─for_each_active_drhd_unit (include/linux/dmar.h)
      |   |   └─for_each_active_dev_scope (include/linux/dmar.h)
      ├─init_dmars (drivers/iommu/intel/iommu.c)
      |   ├─for_each_iommu (include/linux/dmar.h)
      |   |   ├─intel_iommu_init_qi (mmu/intel/iommu.c) //软件给硬件通过这块内存提交任务, 硬件清自己的iotlb缓存
      |   |   |   └─dmar_enable_qi (drivers/iommu/intel/dmar.c)
      |   |   |       └─__dmar_enable_qi (drivers/iommu/intel/dmar.c) //写硬件寄存器
      |   |   ├─iommu_init_domains (iommu/intel/iommu.c) //分配了很多struct dmar_domain
      |   |   └─iommu_alloc_root_entry (iommu/intel/iommu.c)
      |   ├─for_each_active_iommu (include/linux/dmar.h)
      |   |   ├─iommu_flush_write_buffer (drivers/iommu/intel/iommu.c)
      |   |   └─iommu_set_root_entry (drivers/iommu/intel/iommu.c) //写硬件寄存器
      |   ├─si_domain_init (drivers/iommu/intel/iommu.c)
      |   |   ├─alloc_domain(DOMAIN_FLAG_STATIC_IDENTITY)  (drivers/iommu/intel/iommu.c) //额外创建一个domain
      |   |   ├─for_each_online_node (include/linux/nodemask.h) //在这个domain中dma地址和物理地址一一对应
      |   |   |   └─for_each_mem_pfn_range (include/linux/memblock.h)
      |   |   |       └─iommu_domain_identity_map (drivers/iommu/intel/iommu.c)
      |   |   |           └─__domain_mapping  (drivers/iommu/intel/iommu.c)
      |   |   └─for_each_rmrr_units (drivers/iommu/intel/iommu.c)//rmrr内存在这个domain中一一对应
      |   |      └─for_each_active_dev_scope (include/linux/dmar.h)
      |   |           └─iommu_domain_identity_map (drivers/iommu/intel/iommu.c)
      |   └─for_each_iommu (include/linux/dmar.h)
      |       ├─iommu_flush_write_buffer (drivers/iommu/intel/iommu.c)
      |       └─dmar_set_interrupt (drivers/iommu/intel/dmar.c) //iommu硬件自己的中断
      |           ├─dmar_alloc_hwirq (arch/x86/kernel/apic/msi.c or arch/ia64/kernel/msi_ia64.c)
      |           └─request_irq(irq, dmar_fault) (include/linux/interrupt.h)
      ├─bus_set_iommu (drivers/iommu/iommu.c)
      |   └─iommu_bus_init (drivers/iommu/iommu.c)
      |       ├─bus_iommu_probe (drivers/iommu/iommu.c) //处理bus上的设备
      |       |   ├─bus_for_each_dev (drivers/base/bus.c) //给dev分配group,
      |       |   |   └─probe_iommu_group (drivers/iommu/iommu.c)
      |       |   |        └─__iommu_probe_device (drivers/iommu/iommu.c)
      |       |   |             ├─intel_iommu_probe_device (drivers/iommu/intel/iommu.c)
      |       |   |             └─iommu_group_get_for_dev (drivers/iommu/iommu.c)
      |       |   |                   └─pci_device_group (drivers/iommu/iommu.c) //真正决定group的函数
      |       |   └─list_for_each_entry_safe (include/linux/list.h)
      |       |        ├─probe_alloc_default_domain (drivers/iommu/iommu.c) //给内核管理的dev分配default domain
      |       |        ├─iommu_group_create_direct_mappings (drivers/iommu/iommu.c) //对系统保留区分建立mapping, 如dev和ioapic的关系
      |       |        |   └─__iommu_group_for_each_dev (drivers/iommu/iommu.c)
      |       |        |       └─iommu_create_device_direct_mappings (drivers/iommu/iommu.c)
      |       |        |           ├─list_for_each_entry (include/linux/list.h)
      |       |        |           |   ├─iommu_iova_to_phys (drivers/iommu/iommu.c)
      |       |        |           |   └─iommu_map (drivers/iommu/iommu.c)
      |       |        |           └─iommu_flush_iotlb_all (include/linux/iommu.h)
      |       |        ├─__iommu_group_dma_attach (drivers/iommu/iommu.c)
      |       |        |   └─__iommu_group_for_each_dev (drivers/iommu/iommu.c)
      |       |        |       └─iommu_group_do_dma_attach (drivers/iommu/iommu.c)
      |       |        |           └─__iommu_attach_device (domain->ops->attach_dev(domain, dev)) (drivers/iommu/iommu.c)
      |       |        |               └─intel_iommu_attach_device (intel_iommu_attach_device)
      |       |        |                   └─domain_add_dev_info ("drivers/iommu/intel/iommu.c")
      |       |        |                       └─dmar_insert_one_dev_info (drivers/iommu/intel/iommu.c)
      |       |        |                           └─dmar_insert_one_dev_info (drivers/iommu/intel/iommu.c)
      |       |        |                               ├─intel_pasid_alloc_table (drivers/iommu/intel/pasid.c)
      |       |        |                               ├─intel_pasid_setup_pass_through (drivers/iommu/intel/pasid.c)
      |       |        |                               ├─domain_setup_first_level (drivers/iommu/intel/iommu.c)
      |       |        |                               └─intel_pasid_setup_second_level (drivers/iommu/intel/pasid.c)
      |       |        └─__iommu_group_dma_finalize (drivers/iommu/iommu.c)
      |       |              └─__iommu_group_for_each_dev (drivers/iommu/iommu.c)
      |       |                  └─iommu_group_do_probe_finalize (drivers/iommu/iommu.c)
      |       |                      └─intel_iommu_probe_finalize (domain->ops->probe_finalize(dev)) (drivers/iommu/intel/iommu.c)
      |       |                          └─iommu_setup_dma_ops (drivers/iommu/intel/iommu.c)
      |       |                              └─iommu_dma_init_domain (drivers/iommu/dma-iommu.c) //给dev分配iova并且flush到硬件上
      |       └─bus_register_notifier(iommu_bus_notifier) (drivers/iommu/intel/dmar.c) //用于处理hotplug的设备
      ├─for_each_active_iommu (include/linux/dmar.h)
      |   ├─iommu_device_sysfs_add (drivers/iommu/iommu-sysfs.c)
      |   |   ├─device_initialize (drivers/base/core.c)
      |   |   ├─kobject_set_name_vargs (lib/kobject.c)
      |   |   ├─device_add (drivers/base/core.c)
      |   |   └─dev_set_drvdata (dev_set_drvdata)
      |   └─iommu_device_register (drivers/iommu/iommu.c)
      └─for_each_iommu  (include/linux/dmar.h)
              └─iommu_enable_translation (drivers/iommu/intel/iommu.c)
```

# DMAR ACPI Table结构

在系统上电的时候，BIOS/UEFI负责检测并初始化DMAR（即VT-d硬件），为其分配相应的物理地址，并且以ACPI表中的DMAR（DMA Remapping Reporting）表的形式告知VT-d硬件的存在。
DMAR的格式如下所示，先是标准的APCI表的表头，然后是Host Address Width表示该系统中支持的物理地址宽度；标志字节Flag表示VT-d硬件支持的一些功能，最后是Remapping Structures，即一堆有组织的结构体用来描述VT-d硬件的功能.

## DMA Remapping Reporting Structure



## Remapping Structure Types

每个Remapping Structure的开始部分包含type和length两个字段。其中，type表示DMA-remapping structure的类型，而length表示该structure的长度。下表定义了type的可能值：


## DRHD（DMA Remapping Hardware Unit Definition）表

一个DMAR结构体用于唯一定义系统中存在的一个VT-d重定向硬件。其结构体如下所示：


## 



# 3. IOMMU 驱动初始化

内核在启动的时候就会**初始化好设备的信息**, 给**设备**分配相应的 **iommu group**, 其过程如下.

系统启动的时候会执行 `iommu_init` 函数进行内核对象 iommu 的初始化. 主要是调用了 `kset_create_and_add` 函数创建了 `iommu_groups` 对象. 在 `/sys/kernel` 目录下会出现 `iommu_groups` 目录.

可以使用内核调试方法, 在 `iommu_init` 函数处打断点, 查看系统对iommu初始化的过程.

![2021-10-21-11-50-13.png](./images/2021-10-21-11-50-13.png)

注意需要内核打开相应的配置开关才能使得IOMMU功能生效.

<table style="width:100%">
<caption>内核配置选项</caption>
  <tr>
    <th>体系结构</th>
    <th>配置项</th>
  </tr>
  <tr>
    <td>armv1/v2</td>
    <td>CONFIG_ARM_SMMU</td>
  </tr>
  <tr>
    <td>armv3</td>
    <td>CONFIG_ARM_SMMU_V3</td>
  </tr>
  <tr>
    <td>intel x86</td>
    <td>CONFIG_INTEL_IOMMU</td>
  </tr>
  <tr>
    <td>amd x86</td>
    <td>CONFIG_AMD_IOMMU</td>
  </tr>
</table>

# 4. 设备分组



# 5. IOMMU 的作用和效果

iommu实现了类似MMU的功能, 主要是做地址翻译. 其主要通过底层的DMA api来实现设备地址空间的映射. 其最终效果类似于下图:

```
               CPU                  CPU                  Bus
             Virtual              Physical             Address
             Address              Address               Space
              Space                Space

            +-------+             +------+             +------+
            |       |             |MMIO  |   Offset    |      |
            |       |  Virtual    |Space |   applied   |      |
          C +-------+ --------> B +------+ ----------> +------+ A
            |       |  mapping    |      |   by host   |      |
  +-----+   |       |             |      |   bridge    |      |   +--------+
  |     |   |       |             +------+             |      |   |        |
  | CPU |   |       |             | RAM  |             |      |   | Device |
  |     |   |       |             |      |             |      |   |        |
  +-----+   +-------+             +------+             +------+   +--------+
            |       |  Virtual    |Buffer|   Mapping   |      |
          X +-------+ --------> Y +------+ <---------- +------+ Z
            |       |  mapping    | RAM  |   by IOMMU
            |       |             |      |
            |       |             |      |
            +-------+             +------+
```

这里存在三种地址, 虚拟地址空间, 物理地址空间, 总线地址空间. 从 CPU 的视角看到的是虚拟机地址空间, 如我们调用 `kmalloc`,`vmalloc` 分配的都是**虚拟地址空间**. 然后通过 **MMU** 转换成 ram 或寄存器中的**物理地址**. 如 `C->B` 的过程. 物理地址可以在 `/proc/iomem` 中查看.

IO 设备通常使用的是总线地址, 在一些系统上, 总线地址就等于物理地址. 但是通常有 iommu 和南桥的设备则会做总线地址到物理地址之间的映射.

如上图 A->C 未进行DMA的过程, 内核首先扫到 IO 设备和他们的 MMIO 空间以及将设备挂到系统上的 host bridge 信息. 假如设备有一个 BAR, 内核从 BAR 中读取总线地址A, 然后转换成物理地址B. 物理地址B可以在 `/proc/iomem` 中查看到. 当驱动挂载到设备上时, 调用 `ioremap( )` 将物理地址B转换成虚拟地址C. 之后就可以通过读写C的地址来访问到最终的设备寄存器A的地址.

如上图 `X->Y`, `Z->Y` 使用DMA的过程, 驱动首先使用 kmalloc 或类似接口分配一块缓存空间即地址X, 内存管理系统会将X映射到 ram 中的物理地址Y. 设备本身不能使用类似的方式映射到同一块物理地址空间, 因为 DMA 没有使用 MMU 内存管理系统. 在一些系统上, 设备可以直接 DMA 到物理地址Y, 但是通常有 IOMMU 硬件的系统会 DMA 设备到地址Z, 然后通过IOMMU转换到物理地址Y. 这是因为调用 DMA API 的时候, 驱动将**虚拟地址**X传给类似 `dma_map_single( )` 的接口后, 会建立起必须的IOMMU映射关系, 然后返回DMA需要映射的地址Z. 驱动然后通知设备做DMA映射到地址Z, 最后 IOMMU 映射地址Z到ram中的缓存地址Y.

从上面的两个例子中客户看出, IOMMU硬件的存在会加速地址的转换速度, 从X->Y是通过CPU完成的, 而Z->Y则是通过IOMMU硬件完成. 同时能够做到安全, IOMMU会校验Y->Z地址的合法性, 避免了直接DMA到Y可能带来的安全问题.








# 6. reference

https://www.kernel.org/doc/Documentation/DMA-API-HOWTO.txt

https://www.blogsebastian.cn/?p=116

https://www.owalle.com/2021/11/01/iommu-code/ (IOMMU 代码分析)

