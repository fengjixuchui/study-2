
<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [1. 架构](#1-架构)
- [2. IOMMU 驱动初始化](#2-iommu-驱动初始化)
- [3. 设备分组](#3-设备分组)
- [4. IOMMU 的作用和效果](#4-iommu-的作用和效果)
  - [1.2. 内核态DMA驱动和DMA API](#12-内核态dma驱动和dma-api)
  - [1.3. 用户态DMA驱动](#13-用户态dma驱动)
  - [1.4. IOMMU](#14-iommu)
- [5. reference](#5-reference)

<!-- /code_chunk_output -->

# 1. 架构

IOMMU 驱动所处的位置位于 DMA 驱动之上, 其上又封装了一层 VFIO 驱动框架, 便于用户空间编写设备驱动直接调用底层的api操作设备.

```
                   +-------------+
                   | QEMU/CROSVM |
                   +-------------+
                          |
        .-----------------|--------------.
........|.................|..............|..........
        |                 |              |
 +---------------+  +-----------+  +-----------+
 | container API |  | group API |  | device API |
 +---------------+  +-----------+  +-----------+
        |                 |              |
        '-----------------|--------------'
                          |
                          v
                   +-------------+
                   | VFIO driver |
                   +-------------+
                          |
                          v
                .---------'---------.
                |                   |
                v                   v
         +-------------+     +--------------+
         | SMMU driver |     | IOMMU driver |
         +-------------+     +--------------+
	        |                   |
                '-------------------'
                          |
                          v
                    +------------+
                    | DMA driver |
                    +------------+
                          |
     .--------------------'--------------------.
     |             |             |             |
     v             v             v             v
+----------+  +----------+  +----------+  +----------+
|  Device  |  |  Device  |  |  Device  |  |  Device  |
+----------+  +----------+  +----------+  +----------+
```

![2022-08-14-00-59-42.png](./images/2022-08-14-00-59-42.png)

IOMMU框架可分为如下层次:

1. DMA-MAP层:提供对其他驱动DMA MAP/UNMAP的API接口;
2. IOVA层:提供IOVA的分配与释放,并提供RCACHE缓存机制;
3. IO页表层:提供对IO设备页表的操作;
4. SMMU驱动层:提供SMMU驱动初始化,提供IOMMU对应的回调函数;
5. 其它API接口:可提供对VFIO等API接口; 

# 2. IOMMU 驱动初始化

内核在启动的时候就会**初始化好设备的信息**, 给**设备**分配相应的 **iommu group**, 其过程如下.

系统启动的时候会执行 `iommu_init` 函数进行内核对象 iommu 的初始化. 主要是调用了 `kset_create_and_add` 函数创建了 `iommu_groups` 对象. 在 `/sys/kernel` 目录下会出现 `iommu_groups` 目录.

可以使用内核调试方法, 在 `iommu_init` 函数处打断点, 查看系统对iommu初始化的过程.

![2021-10-21-11-50-13.png](./images/2021-10-21-11-50-13.png)

注意需要内核打开相应的配置开关才能使得IOMMU功能生效.

<table style="width:100%">
<caption>内核配置选项</caption>
  <tr>
    <th>体系结构</th>
    <th>配置项</th>
  </tr>
  <tr>
    <td>armv1/v2</td>
    <td>CONFIG_ARM_SMMU</td>
  </tr>
  <tr>
    <td>armv3</td>
    <td>CONFIG_ARM_SMMU_V3</td>
  </tr>
  <tr>
    <td>intel x86</td>
    <td>CONFIG_INTEL_IOMMU</td>
  </tr>
  <tr>
    <td>amd x86</td>
    <td>CONFIG_AMD_IOMMU</td>
  </tr>
</table>

# 3. 设备分组



# 4. IOMMU 的作用和效果

iommu实现了类似MMU的功能, 主要是做地址翻译. 其主要通过底层的DMA api来实现设备地址空间的映射. 其最终效果类似于下图:

```
               CPU                  CPU                  Bus
             Virtual              Physical             Address
             Address              Address               Space
              Space                Space

            +-------+             +------+             +------+
            |       |             |MMIO  |   Offset    |      |
            |       |  Virtual    |Space |   applied   |      |
          C +-------+ --------> B +------+ ----------> +------+ A
            |       |  mapping    |      |   by host   |      |
  +-----+   |       |             |      |   bridge    |      |   +--------+
  |     |   |       |             +------+             |      |   |        |
  | CPU |   |       |             | RAM  |             |      |   | Device |
  |     |   |       |             |      |             |      |   |        |
  +-----+   +-------+             +------+             +------+   +--------+
            |       |  Virtual    |Buffer|   Mapping   |      |
          X +-------+ --------> Y +------+ <---------- +------+ Z
            |       |  mapping    | RAM  |   by IOMMU
            |       |             |      |
            |       |             |      |
            +-------+             +------+
```

这里存在三种地址, 虚拟地址空间, 物理地址空间, 总线地址空间. 从 CPU 的视角看到的是虚拟机地址空间, 如我们调用 `kmalloc`,`vmalloc` 分配的都是**虚拟地址空间**. 然后通过 **MMU** 转换成 ram 或寄存器中的**物理地址**. 如 `C->B` 的过程. 物理地址可以在 `/proc/iomem` 中查看.

IO 设备通常使用的是总线地址, 在一些系统上, 总线地址就等于物理地址. 但是通常有 iommu 和南桥的设备则会做总线地址到物理地址之间的映射.

如上图 A->C 未进行DMA的过程, 内核首先扫到 IO 设备和他们的 MMIO 空间以及将设备挂到系统上的 host bridge 信息. 假如设备有一个 BAR, 内核从 BAR 中读取总线地址A, 然后转换成物理地址B. 物理地址B可以在 `/proc/iomem` 中查看到. 当驱动挂载到设备上时, 调用 `ioremap( )` 将物理地址B转换成虚拟地址C. 之后就可以通过读写C的地址来访问到最终的设备寄存器A的地址.

如上图 `X->Y`, `Z->Y` 使用DMA的过程, 驱动首先使用 kmalloc 或类似接口分配一块缓存空间即地址X, 内存管理系统会将X映射到 ram 中的物理地址Y. 设备本身不能使用类似的方式映射到同一块物理地址空间, 因为 DMA 没有使用 MMU 内存管理系统. 在一些系统上, 设备可以直接 DMA 到物理地址Y, 但是通常有 IOMMU 硬件的系统会 DMA 设备到地址Z, 然后通过IOMMU转换到物理地址Y. 这是因为调用 DMA API 的时候, 驱动将**虚拟地址**X传给类似 `dma_map_single( )` 的接口后, 会建立起必须的IOMMU映射关系, 然后返回DMA需要映射的地址Z. 驱动然后通知设备做DMA映射到地址Z, 最后 IOMMU 映射地址Z到ram中的缓存地址Y.

从上面的两个例子中客户看出, IOMMU硬件的存在会加速地址的转换速度, 从X->Y是通过CPU完成的, 而Z->Y则是通过IOMMU硬件完成. 同时能够做到安全, IOMMU会校验Y->Z地址的合法性, 避免了直接DMA到Y可能带来的安全问题.






## 1.2. 内核态DMA驱动和DMA API

一般外设都自带DMA功能, DMA只是外设的数据传输通道, 外设的功能各不一样, 但DMA传输数据通道功能都一样, 所以内核就有了DMA API, 其它外设驱动只要调用内核DMA API就可以搞定DMA相关的功能了, 内存映射/数据对齐/缓存一致性等都由内核DMA API搞定.

## 1.3. 用户态DMA驱动

外设通过DMA把数据放到内核, 用户态再系统调用把数据手动到用户态, 开销很大, 所以想着外设直接把数据手动到用户态, 可用户态用的都是虚拟地址, 第一个问题就是得把虚拟地址转换成物理地址, 用 `/proc/self/pagemap` 正好可以获取虚拟地址对应的物理地址. 第二个问题是怎么保证虚拟地址对应的物理地址一定存在于内存中并且固定在内存中的同一个物理地址, 虚拟地址一定有对应的物理地址好说, 可以直接把 page 的 ref 加 1, 并且强行给 page 写个 0 数据, 但虚拟地址固定对应到一个物理地址就难说了, 假如进程给一个虚拟地址找了一个 page 让设备给这个 page DMA 写数据, 同时 kernel 开始了 page migration 或者 same page merge, 把进程的虚拟地址对应的物理设置成其它 page, 但设备DMA写的 page 还是原来的 page, 这样导致进程访问的数据就不是设备定到内存中的数据, 但这种概率很小啊. 总之 hugepage 能满足大部分特性.


## 1.4. IOMMU

> VFIO 的功能之一

类同于 MMU, 对 DMA 做地址翻译, 用来解决 DMA 的安全性问题. DMA API 同时还肩负起了**设置 IOMMU** 的职责.

```cpp
dma_ops = &intel_dma_ops;
i40e_alloc_mapped_page
  └─dma_map_page
      └─intel_map_page
          └─__intel_map_single
              ├─if(iommu_no_mapping) return paddr;
              ├─intel_alloc_iova
              └─domain_pfn_mapping
```

内核用 IOMMU 的好处, **限制了设备 DMA 可写的范围**, 设备不能随便从物理内存读写了.

其实 IOMMU 更大的用处在于**用户态驱动**, 如 DPDK 和 qemu, 用于 **qemu passthrough** 能更好的理解 IOMMU 的作用. **guest** 发起 **DMA** 时设置的地址是 `guest_phy_addr`, qemu 拿到 **guest DMA 的内存段开始地址** `guest_dma_phy_addr_start` 转换成自己的 `host_dma_virt_addr`, 然后把两个地址和 DMA 段的长度 len 通知 vfio 建立 map, vfio 找从 `host_dma_phy_addr` 开始的 len 长度的连续物理内存, host_dma_virt_addr 映射到 `host_dma_phy_addr`, 然后 pin 住, 让qemu的虚拟地址始终有物理内存对应并且对应到固定物理内存. vfio 再给 iommu 建立表项, 如果 DMA 要和 `guest_phy_addr` 交换数据, 就和 `host_dma_phy_addr` 交换数据, iommu 中有个 iova, 其实这个iova在这儿就是 `guest_phy_addr`. dpdk中有–iova, 相比较于 qemu 这儿就是小菜一碟.

kvm device passthrough 老方法

```cpp
kvm_iommu_map_pages
  └─iommu_map
      └─intel_iommu_map(domain->ops->map)
          └─domain_pfn_mapping
```

qemu 用 vfio 实现 device passthrough 新方法

```cpp
vfio_iommu_type1_ioctl
  └─vfio_dma_do_map
      └─vfio_pin_map_dma
          └─vfio_iommu_map
              └─iommu_map
                  └─intel_iommu_map(domain->ops->map)
                     └─domain_pfn_mapping
```

下面我们将对其原理进行分析, 主要参考资料是 Intel VT-d SPEC Chapter 3.


# 5. reference

https://www.kernel.org/doc/Documentation/DMA-API-HOWTO.txt

https://www.blogsebastian.cn/?p=116

https://www.owalle.com/2021/11/01/iommu-code/ (IOMMU 代码分析)

