
# SQ和CQ

主机往 SQ 中写入命令, SSD 往 CQ 中写入命令完成结果. SQ 与 CQ 的关系, 可以是**一对一**的关系, 也可以是**多对一**的关系, 但不管怎样, 它们是成对的: 有因就有果, 有 SQ 就必然有 CQ.

有**两种类型**的 SQ 和 CQ, 一种是 Admin, 另外一种是 IO, 前者放 Admin 命令, 用以主机 管理控制 SSD, 后者放置 IO 命令, 用以主机与 SSD 之间传输数据. Admin SQ/CQ 和 IO SQ/CQ 各司其职, 你不能把Admin命令放到IO SQ中, 同样, 你也不能把 IO 命令放到Admin SQ里面. **IO SQ/CQ** 不是一生下来就有的, 它们是**通过 Admin 命令创建**的.

正如图所示, 系统中**只有 1 对 Admin SQ/CQ**, 它们是**一一对应**的关系; IO SQ/CQ 却可以有很多, 多达 **65535 对**(**64K**减去 1 对 Admin SQ/CQ).

SQ 和 CQ:

![2023-02-05-23-20-09.png](./images/2023-02-05-23-20-09.png)

>需要指出的是, 对 NVMe over Fabrics, SQ和CQ的关系只能是一对一; IO SQ/CQ 也不是通过 Admin 命令创建的.

主机端**每个 CPU 核**(Core)可以有**一个或者多个 SQ**, 但**只有一个CQ**. 给每个 CPU 核分配一对 SQ/CQ 好理解, 为什么**一个 CPU 核**中还要**多个 SQ** 呢? 一是**性能需求**, **一个 CPU 核**中有**多线程**, 可以做到**一个线程独享一个 SQ**; 二是 **QoS** 需求, 什么是 QoS? Quality of Service, 服务质量. 脑补一个场景, 蛋蛋一边看小电影, 同时在后台用迅雷下载小电影, 由于电脑配置差, 看个小电影都卡. 蛋蛋不要卡顿！怎么办? NVMe建议, 你设置两个 SQ, 一个赋予高优先级, 一个低优先级, 把看小电影所需的命令放到高优先级的 SQ, 迅雷下载所需的命令放到低优先级的 SQ, 这样, 你的电脑就能把有限的资源优先满足你看小电影了. 至于迅雷卡不卡, 下载慢不慢, 这个时候已经不重要了. 能让蛋蛋舒舒服服地看完一部小电影, 就是好的 QoS. 实际系统中用多少个 SQ, 取决于系统配置和性能需求, 可灵活设置 I/O SQ个数.

关于系统中 **IO SQ 的个数**, NVMe 白皮书给出如下建议(见下表).

NVMe 白皮书对 NVMe 的配置建议:

![2023-02-05-23-20-00.png](./images/2023-02-05-23-20-00.png)

作为**队列**, 每个 SQ 和 CQ 都有一定的深度: 对Admin SQ/CQ来说, 其深度可以是 `2～4096`(**4K**); 对 IO SQ/CQ, 深度可以是 2～65536(**64K**). 队列深度也是可以配置的.

SQ/CQ 的**个数**可以配置, 每个 SQ/CQ 的**深度**也可以配置, 因此 NVMe 的性能是可以通过配置队列个数和队列深度来灵活调节的.

> 百变星君NVMe: 想胖就胖, 想瘦就瘦; 想高就高, 想矮就矮.

我们已经知道, **AHCI 只有一个命令队列**, 且**队列深度**是固定的 **32**, 和 NVMe 相比, 无论是在命令队列广度还是深度上, 都是无法望其项背的; NVMe 命令队列的百般变化, 更是 AHCI 无法做到的. 说到百般变化, 我突然又想到一件残忍的事情: PCIe 也是可以的. 一个 PCIe 接口, 可以有 1、2、4、8、12、16、32 条 lane! SATA 都要哭了: 单挑都挑不过你, 你还来群殴我, 太欺负人了.

* 每个 **SQ** 放入的是**命令条目**, 无论是 Admin 还是 IO 命令, **每个命令条目**大小都是 **64 字节**;
* 每个 **CQ** 放入的是**命令完成状态信息条目**, 每个条目大小是 **16 字节**.

## 小结

在继续谈大宝(DB)之前, 先对 SQ 和 CQ 做个小结:

* SQ 用以主机发命令, CQ 用以 SSD 回命令完成状态;

* SQ/CQ 可以在**主机的内存**中, 也可以在 **SSD** 中, 但**一般在主机内存中**(本书中除非特殊说明, 不然都是基于 SQ/CQ 在主机内存中作介绍);

* 两种类型的 SQ/CQ: Admin 和 IO, 前者发送 Admin 命令, 后者发送 IO 命令;

NVMe Spec 对 Admin SQ/CQ 和 IO SQ/CQ 有不同的约定:

* 系统中**只能**有**一对 Admin SQ/CQ**, 但可以有 **(64K - 1) 对 IO SQ/CQ**;

* `IO SQ/CQ` 深度可达 **2K ~ 64K**(65535 个命令), `Admin SQ/CQ` 深度可达 **2K ~ 4K**;

* `IO SQ/CQ` 的**广度**和**深度**都可以**灵活配置**;

* 每条 Admin/IO command 大小是 64B, 每条命令完成状态(Completion)大小是 16B.

* IO SQ 与 CQ 可以是**一对一**的关系, 也可以是**多对一**的关系;. 多个 IO SQ 可以支持多线程工作, 不同 IO SQ 是可以赋予**不同优先级**的;

* Admin 和 IO 的 SQ/CQ 均放在 Host 端 Memory 中;

* SQ 由 Host 更新, CQ 由 NVMe Controller 更新.

# 队列

SQ/CQ 中的 "Q" 指 Queue, 队列的意思(见图), 无论SQ还是CQ, 都是**队列**, 并且是**环形队列**. 队列有几个要素, 除了**队列深度**、**队列内容**, 还有队列的**头部**(Head)和**尾部**(Tail). 其实, 队列可以是连续的物理空间, 也可以不连续.

队列(Queue)的概念:

![2023-02-05-23-19-49.png](./images/2023-02-05-23-19-49.png)

* Tail: 指向队列中的**下一个空位**；
* Head: 指向**下一个将要被执行的命令**所在的**位置**.
* 当**队列为空**时, `Head = Tail`;
* 当**队列为满**时, `Head = Tail + 1`;

队伍的**头部**决定谁会**被马上服务**, **尾巴**决定了**新来**的人站的**位置**.
* **DB** 就是用来**记录**了一个 SQ 或者 CQ 的**头和尾**.
* **每个** SQ 或者 CQ, 都有**两个**对应的 **DB 寄存器**: Head DB 和 Tail DB.
* DB 是在 **SSD Controller 的寄存器**, 记录 SQ 和 CQ 的头和尾巴的位置.

如下图所示是一个**队列生产者/消费者**(Producer/Consumer)**模型**: **生产者**往队列的**尾部写入**东西, **消费者**从队列的**头部取出**东西.

![2023-02-05-23-19-42.png](./images/2023-02-05-23-19-42.png)

* 对一个 **SQ** 来说, 它的**生产者**是**主机**, 因为它**向 SQ 的尾部写入命令**; **消费者**是 **SSD**, 因为它从 SQ 的**头部取出**指令执行;

* 对一个 **CQ** 来说, 刚好相反, **生产者**是 **SSD**, 因为它向 CQ 的**尾部写入**命令完成信息; **消费者**则是**主机**, 它从 CQ 的**头部取出**命令完成信息.

下面举个例子说明.

1. 开始假设 SQ1 和 CQ1 是空的, Head = Tail = 0, 如图所示.

SQ、CQ、DB 初始化状态:

![2023-02-05-23-19-28.png](./images/2023-02-05-23-19-28.png)

2. 这个时候, **主机**往 **SQ1** 中**写入了三个命令**, **SQ1** 的 **Tail** 则变成 **3**. **主机**往 SQ1 写入三个命令后, 然后漂洋过海去**更新 SSD 控制器端**的 **SQ1 Tail DB 寄存器**, 值为 **3**. 主机更新这个寄存器的同时, 也是在**告诉 SSD 控制器**: 有新命令了, 帮忙去我那里取一下, 如图所示.

> tail 是下一个空位, 从 0 开始计数, 所以是 3

主机往SQ中写入三个命令:

![2023-02-05-23-19-20.png](./images/2023-02-05-23-19-20.png)

3. **SSD 控制器**收到通知后, 于是派人去 **SQ1** 把 **3 个命令**都取回来执行. SSD 把 SQ1 的**三个命令都消费**了, **SQ1** 的 **Head** 从而也调整为 **3**, **SSD 控制器**会把这个 Head 值写入本地的 **SQ1 Head DB 寄存器**, 如图所示.

SSD取走三个命令:

![2023-02-05-23-19-14.png](./images/2023-02-05-23-19-14.png)

4. SSD **执行完**了**两个命令**, 于是往 **CQ1** 中**写入两个命令完成信息**, 更新 CQ1 对应的 **Tail DB 寄存器**, 值为 **2**. 同时发消息给主机: 有命令完成, 请注意查看, 如图所示.

SSD 完成两个命令后写 CQ:

![2023-02-05-23-19-06.png](./images/2023-02-05-23-19-06.png)

5. **主机**收到 SSD 的短信通知(**中断信息**), 于是从 **CQ1** 中**取出**那两条完成信息. 处理完毕, 主机又漂洋过海地往 **CQ1 Head DB 寄存器**中写入 CQ1 的 head, 值为2, 如图所示.

主机处理完CQ中的两个命令状态:

![2023-02-05-23-18-56.png](./images/2023-02-05-23-18-56.png)

通过这个例子, 我们又重温了一下命令处理流程. 之前也许只记住了命令处理需要8步(距离曹植一步之遥), 但现在我们应该对命令处理流程有了更深入具体的认识.

那么, DB 在命令处理流程中起了什么作用呢?

首先, 如前文提到的, 它记住了 SQ 和 CQ 的头和尾. 对 SQ 来说, **SSD** 是**消费者**, 它直接和**队列的头**打交道, 很清楚 SQ 的头在哪里, 所以SQ head DB 由 SSD 自己维护; 但它**不知道队伍有多长**, 尾巴在哪, **后面还有多少命令等待执行**, 相反, **主机**知道, 所以 SQ Tail DB 由主机来更新. SSD 结合 SQ 的头和尾, 就知道还有多少命令在SQ中等待执行了. 对 CQ 来说, SSD 是生产者, 它很清楚 CQ 的尾巴在哪里, 所以 CQ Tail DB 由自己更新, 但是 SSD 不知道主机处理了多少条命令完成信息, 需要主机告知, 因此 CQ Head DB 由主机更新. SSD 根据 CQ 的头和尾, 就知道CQ还能不能, 以及能接受多少命令完成信息.

DB 还起到了通知作用: **主机更新 SQ Tail DB** 的同时, 也是在告知SSD**有新的命令**需要处理; **主机更新 CQ Head DB** 的同时, 也是在告知 SSD, 你返回的命令完成状态信息我已经处理, 同时表示谢意.

这里有一个对主机不公平的地方, 主机对 DB 只能写(还**仅限于写 SQ Tail DB 和 CQ Head DB**), **不能读取 DB**.

在这个限制下, 我们看看**主机**是怎样维护 SQ 和 CQ 的. SQ 的尾巴没有问题, 主机是生产者, 对新命令来说, 它清楚自己应该站在队伍哪里. 但是Head呢? SSD 在取指的时候, 是偷偷进行的, 主机对此毫不知情. **主机**发了取指通知后, 它并**不清楚 SSD 什么时候去取命令**、取了多少命令. 怎么办? 机智如你, 如果是你, 你会怎么做? 山人自有妙计. 给个提示(见图).

**SQ** 的 **Head DB** 在**命令完成状态**里:

![2023-02-05-23-18-47.png](./images/2023-02-05-23-18-47.png)

这是什么? 这是 **SSD** 往 **CQ** 中写入的**命令完成状态信息**(16字节).

是的, **SSD** 往 **CQ** 中**写入命令状态信息**的**同时**, 还把 **SQ Head DB 的信息**告知了主机！这样, **主机**对 **SQ 队列**的**头部和尾部的信息**就都有了, 可以轻松玩转 SQ.

CQ 呢? **主机**知道它队列的头部, 不知道尾部. 那**怎么能知道尾部**呢? 思路很简单, 既然 SSD 知道, 那你告诉我呗！SSD 怎么告诉主机呢? 还是**通过 SSD 返回命令状态信息**获取. 看到上图中所示的 "**P**" 了吗? 干什么用? 做标记用.

具体是这样的: 一开始 **CQ** 中每条命令完成将条目中的 "P" 比特初始化为 0 的工作, SSD 在往 CQ 中写入命令完成条目时, 会把 "P" 写成 1(如果之前该位置为 1, 控制器写 CQ 的时候翻转该比特, 即写 0). 记住一点, CQ 是在主机端的内存中, 主机可以检查 CQ 中的所有内容, 当然包括 "P"了. 主机记住上次队列的尾部, 然后往下一个一个检查 "P", 就能得出新的队列尾部了. 就是这样, 如下图所示.

主机根据 PhaseTag 计算 CQ 队列的尾部:

![2023-02-05-23-18-39.png](./images/2023-02-05-23-18-39.png)

# 小结

最后, 给大宝做个小结:

* DB 在 SSD控制器端, 是寄存器;

* DB 记录着 SQ 和 CQ 队列的头部和尾部;

* 每个 SQ 或者 CQ 有两个 DB —— Head DB 和 Tail DB;

* 主机只能写 DB, 不能读 DB;

* 主机通过 SSD 往 CQ 中写入的命令完成状态获取其队列头部或者尾部.










PCIe TLP 的类型有很多, 如下图, 不过,  NVMe 只挑选了 Memory Read/Wirte 传递信息











