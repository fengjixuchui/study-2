
<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [1. NVMe位置](#1-nvme位置)
- [2. 历史](#2-历史)
- [3. NVMe工作原理](#3-nvme工作原理)
  - [3.1. 两种命令](#31-两种命令)
    - [3.1.1. Admin命令集](#311-admin命令集)
    - [3.1.2. I/O命令集](#312-io命令集)
  - [3.2. 主机发送命令给SSD](#32-主机发送命令给ssd)
    - [3.2.1. SQ、CQ和DB](#321-sq-cq和db)
    - [3.2.2. 命令处理流程](#322-命令处理流程)

<!-- /code_chunk_output -->

# 1. NVMe位置

NVMe 是一种**主机**(Host)与 **SSD** 之间**通信的协议**, 它在协议栈中隶属高层, 如图所示.

NVMe 处于**协议栈**的最高层:

![2023-02-04-23-47-06.png](./images/2023-02-04-23-47-06.png)

![2023-02-05-22-26-01.png](./images/2023-02-05-22-26-01.png)

> NVMe 作为**命令层**和**应用层**协议, 理论上可以**适配**在**任何接口协议上**. 但 NVMe 协议的原配是 **PCIe**, 因此如无特别说明, 后面章节都是基于 `NVMe+PCIe`.

NVMe 在**协议栈**中处于**应用层**或者**命令层**, 它是指挥官、军师, 相当于三国时期诸葛亮的角色. 军师设计好计谋, 就交由手下五虎大将去执行. NVMe 的手下大将就是 PCIe, 它所制定的**任何命令**, 都交由 **PCIe** 去**完成**. 虽然 NVMe 的命令也可以由别的接口完成, 但 NVMe 与 PCIe 合作形成的战斗力无疑是最强的.

# 2. 历史

NVMe 是为 SSD 所生的. NVMe 出现之前, **SSD** 绝大多数用的是 **AHCI** 加 **SATA** 的组合, 后者其实是为**传统 HDD** 服务的.

与 HDD 相比, SSD 具有更低的延时和更高的性能, AHCI 已经不能跟上 SSD 性能发展的步伐, 而是成为了制约 SSD 性能的瓶颈. 所有 SATA 接口的 SSD, 你去看性能参数, 会发现都不会超过 600MB/s(或者说都不超过560MB/s). 如果碰到有人跟你说他的 SATA SSD 读取性能可以超过 600MB/s, 直接拨打 12315 投诉. 不是底层闪存带宽不够, 是 **SATA 接口**速度**限制**了带宽, 因为 SATA 3.0 最高带宽就是 600MB/s, 而且不会再有 SATA 4.0 了, 如图所示.

SATA 和 PCIe 接口速度对比:

![2023-02-04-23-50-17.png](./images/2023-02-04-23-50-17.png)

既然 **SATA 接口**速度太慢, 那么用 **PCIe** 好了, 不过**上层协议**还是 **AHCI**. 五虎上将有了, 由刘备指挥, 让人不禁感叹暴殄天物呀. 刘备什么水平, 诸葛亮出现之前, 居无定所, 一会跟着曹操混, 一会又跟着吕布混, 谁肯收留就跟谁混, 惨呀！**AHCI** 和刘备一个德行, **只有一个命令队列**, 最多同时只能发 **32 条命令**, HDD 时代(群雄逐鹿)还能混混, SSD 时代(三足鼎立)就只有被灭的份. 刘备需要三顾茅庐, 请诸葛亮出山辅佐. 同样, SSD 需要 PCIe, 更需要 NVMe.

在这样的背景下, Intel 等巨头携天子以令诸侯, 集大家的智慧, 制定出了 NVMe 规范, 最初的名字叫 NVMHCI, 是在 2007 年的Intel开发者论坛(IDF)上被提出来的, 并在同年 Intel 牵头成立 NVMe 开发工作组. 目的就是释放 SSD 性能潜力, 解 SSD 倒悬之苦. 最初制定 NVMe规范的主要公司如图所示.

最初制定NVMe规范的主要公司:

![2023-02-05-22-25-31.png](./images/2023-02-05-22-25-31.png)

# 3. NVMe工作原理

NVMe 制定了主机与 SSD controller 之间通信的命令, 以及命令如何执行的.

## 3.1. 两种命令

NVMe 有**两种命令**, 一种叫 Admin 命令, 用以**主机 管理和控制SSD**; 另外一种就是 I/O 命令, 用以**主机和 SSD 之间数据的传输**.

> 其实还有一种 Fabrics Commands, 对于 NVMe over PCIe 不用关心.

如图所示.

NVMe 命令集:

![2023-02-04-23-52-12.png](./images/2023-02-04-23-52-12.png)

### 3.1.1. Admin命令集

NVMe 支持的 Admin 命令如图所示

NVMe Admin 命令集:

![2023-02-04-23-52-21.png](./images/2023-02-04-23-52-21.png)

### 3.1.2. I/O命令集

NVMe 支持的 I/O 命令如图所示

NVMe NVM 命令集:

![2023-02-04-23-52-33.png](./images/2023-02-04-23-52-33.png)

NVMe 定义的命令很简单, 只有两种: Admin Command 和 IO Command. 加起来总共只要求 13 个(10Admin Commands + 3 IO Commands), 就完全足够了.

跟 ATA 规范中定义的命令相比, NVMe 的命令个数少了很多, 完全是**为 SSD 量身定制**的. 在 **SATA** 时代, 即使只有 **HDD** 才**需要**的命令(SSD 上其实完全没有必要), 但**为了符合协议标准**, SSD 还是需要实现那些毫无意义(完全只是为了兼容性)的命令. 没有办法, 谁叫你 SSD 寄人篱下呢. NVMe 让 SSD 扬眉吐气了一把.

## 3.2. 主机发送命令给SSD

命令有了, 那么, 主机又是怎么把这些命令发送给 SSD 执行呢?

* 当 Host 要下发 Admin command 时, 需要一个放置 Admin command 的队列, 这个队列就叫做 Admin Submission Queue, 简称 Admin SQ.

* Device 执行完成 Admin command 时, 会生成一个对应的 Completion 回应, 此时也需要一个放置 Completion 的队列, 这个队列就叫做 Admin Completion Queue, 简称 Admin CQ.

同样, 执行 IO Command 时, 也会有对应的两个队列, 分别是 IO SQ 和 IO CQ.

### 3.2.1. SQ、CQ和DB

NVMe 有三宝: `Submission Queue`(**SQ**)、`Completion Queue`(**CQ**)和 `Doorbell Register`(**DB**).

**SQ** 和 **CQ** 位于主机的**内存**中, **DB** 则位于 SSD 的**控制器内部**, 如下图所示.

SQ、CQ 和 DB 在系统中的位置:

![2023-02-04-23-57-45.png](./images/2023-02-04-23-57-45.png)

图可以看到, **SQ 和 CQ 在主机的内存**(Memory)中以及 **DB 在 SSD 端**.

而且上图让我们对**一个 PCIe 系统**有一个直观的认识. 图中的 **NVMe 子系统**一般就是 **SSD**. 请看这张图几秒钟, 然后闭上眼, 脑补SSD所处的位置: **SSD** 作为一个 **PCIe Endpoint**(EP)通过 **PCIe** 连着 `Root Complex`(**RC**), 然后 RC 连接着 **CPU** 和**内存**. RC 是什么? 我们可以认为 RC 就是 CPU 的代言人或者助理. 作为系统中的最高层, CPU说: "我很忙的, 你SSD有什么事情先跟我助理说！"尽管如此, SSD的地位还是较过去提升了一级, **过去** SSD 别说直接接触霸道总裁, 就是连助理的面都见不到, SSD 和助理之间还隔着一座**南桥**.

> 所以 NVMe SSD 最好直接挂在 root port 上, 不要在 Switch 的 PCIe Endpoint, 因为 Switch 会对 message 进行解析然后路由, 从而增加了路径.

回到我们的"吉祥三宝"(SQ、CQ、DB). **SQ** 位于**主机内存**中, **主机**要**发送命令**时, 先把准备好的命令**放在 SQ 中**, 然后**通知 SSD** 来取; **CQ** 也是位于**主机内存**中, 一个命令执行完成, 成功或失败, SSD 总会往 CQ 中写入命令完成状态. DB(大宝)又是干什么用的呢? **主机发送命令时**, 不是直接往SSD中发送命令, 而是把命令准备好放在自己的内存中, 那怎么通知SSD来获取命令执行呢? 主机就是通过**写 SSD 端的大宝寄存器**来**告知 SSD** 的.

### 3.2.2. 命令处理流程

我们来看看 NVMe 是如何处理命令的, 如图所示.

NVMe 命令处理流程:

![2023-02-04-23-57-37.png](./images/2023-02-04-23-57-37.png)

![2023-02-12-21-33-10.png](./images/2023-02-12-21-33-10.png)

在 NVMe Spec 中 Command 执行的流程有八步,  Host 与 Controller 之间用 PCIe TLP 传递信息.

第一步, Host 提交新的 Command. Host 下发一个新 Command 时, 将其放入 Host 内存中 SQ;

第二步, Host 通知 Controller 提取 Command.  Host 把 Command 写入 SQ 之后, 此时 Device 并不知道这件事. 所以,  Host 此时需要给 Controller 发信息, 通知 NVMe Controller:"我提交了新的命令请求, 麻烦尽快帮忙处理！". 这个过程通过更新在 Controller 内部的寄存器 SQ Tail Doorbell register, 会填充进去 SQ Tail entry pointer 的值;

第三步, NVMe Controller 从 SQ 提取 Command. 取走 Command 之后, 需要在 Controller 内部的 SQ Head Pointer 寄存器中更新 Head 所在的位置.  NVMe 没有规定 Command 存入队列的 执行顺序,  Controller 可以一次取出**多个 Command** 进行**批量处理**;

第四步, NVMe Controller 执行从 SQ 提取的 Commands. 一个队列中的 Command 执行顺序是不固定的(可能导致先提交的请求后处理), 这涉及到 NVMe Spec 定义的命令仲裁机制, 在后续文章中介绍. 执行 Read/Wirte Command 时, 这个过程也会与 Host Memory 进行数据传递;

第五步, NVMe Controller 将 Commands 的完成状态写入 CQ. 此时, Controller 需要更新 CQ Tail Pointer 寄存器;

第六步, NVMe Controller 通知 Host 检查 Commands 的完成状态. Controller 通过发送一个中断信息告知 Host: "您提交的 Commands, 我已经执行完毕了, 请您检查结果！";

第七步, 收到中断, Host 检查 CQ 中的 Completion 信息;

第八步, Host 告知 Controller 已处理完成 Completion 信息. Host 更新 Controller 内部的 CQ Head Doorbell. 告知 Controller: "您发送回来的 Command 执行结果, 我已处理完毕, 非常感谢！".

